[
    {
        "Section_id": "SignFlips",
        "Description": "Flips the signs of all entries in one dataset, X along some of the dimensions. In particular, it does so in a way that brings this dataset to the same orthant as the second dataset, Y, according to some criterion, computed along each dimension. The two critera currently available are the median and the maximum (in magnitude) value along each dimension.\nThis module can also be used to bring the dataset to the first orthant (i.e. with all criteras being positive) by providing the identity matrix as the second dataset."
    },
    {
        "Field List > Parameters": {
            "criterion": "string, {'median' (default), 'max'}, optional\nString describing the criterion used to choose whether to flip signs. Two options are currently supported:\n\n'median' Uses the median along each dimension\n'max' Uses the maximum (in magintude) along each dimension"
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Attributes": {
            "Q_array_": "size (d, d)\nFinal orthogonal matrix, used to modify X."
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "criterion": "str"
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > set_criterion_function": {
            "Returns": {
                "None": ""
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Uses the two datasets to learn the matrix Q_ that aligns the first dataset with the second.\nIn sign flips, Q_ is an diagonal orthogonal matrices (i.e. a matrix with 1 or -1 in each entry on diagonal and 0 everywhere else) picked such that all dimensions of X @ Q_ and Y are in the same orthant using some critera (median or max magnitude).",
            "Parameters": {
                "X": "np.ndarray, shape (n, d)\nDataset to be mapped to Y, must have same number of dimensions (axis 1) as Y.",
                "Y": "np.ndarray, shape (m, d)\nTarget dataset, must have same number of dimensions (axis 1) as X."
            },
            "Returns": {
                "self": "returns an instance of self"
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Uses the two datasets to learn the matrix Q_ that aligns the first dataset with the second. Then, transforms the first dataset X using the learned matrix Q_.",
            "Parameters": {
                "X": "np.ndarray, shape (n, d)\nDataset to be mapped to Y, must have same number of dimensions (axis 1) as Y.",
                "Y": "np.ndarray, shape (m, d)\nTarget dataset, must have same number of dimensions (axis 1) as X."
            },
            "Returns": {
                "X_prime": "np.ndarray, shape (n, d)\nFirst dataset of vectors, aligned to second. Equal to X @ Q_."
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Field List > Methods > transform": {
            "Description": "Transforms the dataset X using the learned matrix Q_. This may be the same as the first dataset as in fit(), or a new dataset. For example, additional samples from the same dataset.",
            "Parameters": {
                "X": "np.ndarray, shape(m, d)\nDataset to be transformed, must have same number of dimensions (axis 1) as X and Y that were passed to fit."
            },
            "Returns": {
                "X_prime": "np.ndarray, shape (n, d)\nFirst dataset of vectors, aligned to second. Equal to X @ Q_."
            }
        },
        "Section_id": "SignFlips"
    },
    {
        "Section_id": "OrthogonalProcrustes",
        "Description": "Computes the matrix solution of the seedless Procrustes problem, which is that given two matrices X and Y of equal shape (n, d), find an orthogonal matrix that most closely maps X to Y. Subsequently, uses that matrix to transform either the original X, or a different dataset in the same space.\nNote that when used to match two datasets, this method only requires that the datasets have the same number of entries, but not that there is some correspondence between the entries. In graph embeddings, this usually corresponds to the assumption that the vertex i in graph X has the same latent position as the vertex i in graph Y."
    },
    {
        "Field List > Attributes": {
            "Q_array_": "size (d, d)\nFinal orthogonal matrix, used to modify X.",
            "score_": "float\nFinal value of the objective function: Lower means the datasets have been matched together better."
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > __init__": {
            "Returns": {
                "None": ""
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Uses the two datasets to learn the matrix Q_ that aligns the first dataset with the second.",
            "Parameters": {
                "X": "np.ndarray, shape (n, d)\nDataset to be mapped to Y, must have the same shape as Y.",
                "Y": "np.ndarray, shape (m, d)\nTarget dataset, must have the same shape as X."
            },
            "Returns": {
                "self": "returns an instance of self"
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Uses the two datasets to learn the matrix Q_ that aligns the first dataset with the second. Then, transforms the first dataset X using the learned matrix Q_.",
            "Parameters": {
                "X": "np.ndarray, shape (n, d)\nDataset to be mapped to Y, must have the same shape as Y.",
                "Y": "np.ndarray, shape (m, d)\nTarget dataset, must have the same shape as X."
            },
            "Returns": {
                "X_prime": "np.ndarray, shape (n, d)\nFirst dataset of vectors, aligned to second. Equal to X @ Q_."
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Field List > Methods > transform": {
            "Description": "Transforms the dataset X using the learned matrix Q_. This may be the same as the first dataset as in fit(), or a new dataset. For example, additional samples from the same dataset.",
            "Parameters": {
                "X": "np.ndarray, shape(m, d)\nDataset to be transformed, must have same number of dimensions (axis 1) as X and Y that were passed to fit."
            },
            "Returns": {
                "X_prime": "np.ndarray, shape (n, d)\nFirst dataset of vectors, aligned to second. Equal to X @ Q_."
            }
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Rubric": {
            "Note": "Formally, minimizes ||X @ Q_ - Y||_F^2, which has a closed form solution, whenever Q_ is constrained to be an orthogonal matrix, that is a matrix that satisfies Q_ @ Q_.T = I. For the more details, including the proof of the closed-form solution see [1].\nImplementation-wise, this class is a wrapper of the scipy.linalg.orthogonal_procrustes(), which itself uses an algorithm described in find the optimal solution algorithm [2].",
            "References": [
                "(1,2) https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem",
                "(2) Peter H. Schonemann, \"A generalized solution of the orthogonal Procrustes problem\", Psychometrica -- Vol. 31, No. 1, March, 1996."
            ]
        },
        "Section_id": "OrthogonalProcrustes"
    },
    {
        "Section_id": "SeedlessProcrustes",
        "Description": "Matches two datasets using an orthogonal matrix. Unlike OrthogonalProcrustes, this does not require a matching between entries. It can even be used in the settings where the two datasets do not have the same number of entries.\nIn graph setting, it is used to align the embeddings of two different graphs, when it requires some simultaneous inference task and no 1-1 matching between the vertices of the two graphs can be established, for example, inside of the test for the equivalence of the latent distributions (see: LatentDistributionTest)."
    },
    {
        "Field List > Parameters": {
            "optimal_transport_lambda": "float (default=0.1), optional\nRegularization term of the Sinkhorn optimal transport algorithm.",
            "optimal_transport_eps": "float (default=0.01), optional\nTolerance parameter for the each Sinkhorn optimal transport algorithm. I.e. tolerance for each \"E-step\".",
            "optimal_transport_num_reps": "int (default=1000), optional\nNumber of repetitions in each iteration of the iterative optimal transport problem. I.e. maximum number of repetitions in each \"E-step\".",
            "iterative_num_reps": "int (default=100), optional\nNumber of reps in each iteration of the iterative optimal transport problem. I.e. maxumum number of total iterations the whole \"EM\" algorithm.",
            "init": "string, {'2d' (default), 'sign_flips', 'custom'}, optional\n'2d'\nUses  different restarts, where  is the dimension of the datasets. In particular, tries all matrices that are simultaneously diagonal and orthogonal. In other words, these are diagonal matrices with all entries on the diagonal being either +1 or -1. This is motivated by the fact that spectral graph embeddings have two types of orthogonal non-identifiability, one of which is captured by the orthogonal diagonal matrices. The final result is picked based on the final values of the objective function. For more on this, see [2].\n'sign_flips'\nInitial alignment done by making the median value in each dimension have the same sign. The motivation is similar to that in '2d', except this is a heuristic that can save time, but can sometimes yield suboptimal results.\n'custom'\nExpects either an initial guess for Q_ or an initial guess for P_, but not both. See initial_Q and initial_P, respectively. If neither is provided, initializes initial_Q to an identity with an appropriate number of dimensions.",
            "initial_Q": "np.ndarray, shape (d, d) or None, optional (default=None)\nAn initial guess for the alignment matrix, Q_, if such exists. Only one of initial_Q, initial_P can be provided at the same time, and only if init argument is set to 'custom'. If None, and initial_P is also None - initializes initial_Q to identity matrix. Must be an orthogonal matrix, if provided.",
            "initial_P": "np.ndarray, shape (n, m) or None, optional (default=None)\nInitial guess for the optimal transport matrix, P_, if such exists. Only one of initial_Q, initial_P can be provided at the same time, and only if init argument is set to 'custom'. If None, and initial_Q is also None - initializes initial_Q to identity matrix. Must be a soft assignment matrix if provided (rows sum up to 1/n, cols sum up to 1/m.)"
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Attributes": {
            "Q_array_": "size (d, d)\nFinal orthogonal matrix, used to modify X.",
            "P_array_": "size (n, m) where n and m are the sizes of two datasets\nFinal matrix of optimal transports, represent soft matching weights from points in one dataset to the other, normalized such that all rows sum to 1/n and all columns sum to 1/m.",
            "score_": "float\nFinal value of the objective function: Lower means the datasets have been matched together better.",
            "selected_initial_Q_": "array, size (d, d)\nInitial orthogonal matrix which was used as the initialization. If init was set to '2d' or 'sign_flips', then it is the adaptively selected matrix. If init was set to 'custom', and initial_Q was provided, then equal to that. If it was not provided, but initial_P was, then it is the matrix after the first procrustes performed. If neither was provided, then it is the identity matrix."
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "optimal_transport_lambda": "float",
                "optimal_transport_eps": "float",
                "optimal_transport_num_reps": "int",
                "iterative_num_reps": "int",
                "init": "str",
                "initial_Q": "ndarray | None",
                "initial_P": "ndarray | None"
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Uses the two datasets to learn the matrix self.Q_ that aligns the first dataset with the second.",
            "Parameters": {
                "X": "np.ndarray, shape (n, d)\nDataset to be mapped to Y, must have same number of dimensions (axis 1) as Y.",
                "Y": "np.ndarray, shape (m, d)\nTarget dataset, must have same number of dimensions (axis 1) as X."
            },
            "Returns": {
                "self": "returns an instance of self"
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Uses the two datasets to learn the matrix Q_ that aligns the first dataset with the second. Then, transforms the first dataset X using the learned matrix Q_.",
            "Parameters": {
                "X": "np.ndarray, shape (n, d)\nDataset to be mapped to Y, must have same number of dimensions (axis 1) as Y.",
                "Y": "np.ndarray, shape (m, d)\nTarget dataset, must have same number of dimensions (axis 1) as X."
            },
            "Returns": {
                "X_prime": "np.ndarray, shape (n, d)\nFirst dataset of vectors, aligned to second. Equal to X @ Q_."
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Field List > Methods > transform": {
            "Description": "Transforms the dataset X using the learned matrix Q_. This may be the same as the first dataset as in fit(), or a new dataset. For example, additional samples from the same dataset.",
            "Parameters": {
                "X": "np.ndarray, shape(m, d)\nDataset to be transformed, must have same number of dimensions (axis 1) as X and Y that were passed to fit."
            },
            "Returns": {
                "X_prime": "np.ndarray, shape (n, d)\nFirst dataset of vectors, aligned to second. Equal to X @ Q_."
            }
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Rubric": {
            "Note": "In essence, the goal of this procedure is to simultaneously obtain a, not necessarily 1-to-1, correspondence between the vertices of the two data sets, and an orthogonal alignment between two datasets. If the two datasets are represented with matrices  and , then the correspondence is a matrix  that is soft assignment matrix (that is, its rows sum to , and columns sum to ) and the orthogonal alignment is an orthogonal matrix  (an orthogonal matrix is any matrix that satisfies ). The global objective function is .\n\nNote that both  and  are matrices in . Thus, if one knew , it would be simple to obtain an estimate for , using the regular orthogonal procrustes. On the other hand, if  was known, then  and  could be thought of distributions over a finite number of masses, each with weight  or , respectively. These distributions could be \"matched\" via solving an optimal transport problem.\n\nHowever, both  and  are simultaneously unknown here. So the algorithm performs a sequence of alternating steps, obtaining iteratively improving estimates of  and , similarly to an expectation-maximization (EM) procedure. It is not known whether this procedure is formally an EM, but the analogy can be drawn as follows: after obtaining an initial guess of of , obtaining an assignment matrix  (\"E-step\") is done by solving an optimal transport problem via Sinkhorn algorithm, whereas obtaining an orthogonal alignment matrix  (\"M-step\") is done via regular orthogonal procurstes. These alternating steps are performed until iterative_num_reps is reached.\n\nFor more on how the initial guess can be performed, see init.",
            "References": [
                "(1) Agterberg, J., Tang, M., Priebe., C. E. (2020). \"Nonparametric Two-Sample Hypothesis Testing for Random Graphs with Negative and Repeated Eigenvalues\" arXiv:2012.09828",
                "(2) Agterberg, J., Tang, M., Priebe., C. E. (2020). \"On Two Distinct Sources of Nonidentifiability in Latent Position Random Graph Models\" arXiv:2003.14250"
            ]
        },
        "Section_id": "SeedlessProcrustes"
    },
    {
        "Section_id": "KMeansCluster",
        "Description": "KMeans Cluster.\nIt computes all possible models from one component to max_clusters. When the true labels are known, the best model is given by the model with highest adjusted Rand index (ARI). Otherwise, the best model is given by the model with highest silhouette score."
    },
    {
        "Field List > Parameters": {
            "max_clusters": "int, default=2.\nThe maximum number of clusters to consider. Must be >=2.",
            "random_state": "int, RandomState instance or None, optional (default=None)\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Attributes": {
            "n_clusters_": "int\nOptimal number of clusters. If y is given, it is based on largest ARI. Otherwise, it is based on highest silhouette score.",
            "model_KMeans_": "object\nFitted KMeans object fitted with n_clusters_.",
            "silhouette_list_": "List of silhouette scores computed for all possible number of clusters given by range(2, max_clusters).",
            "ari_list_": "Only computed when y is given. List of ARI values computed for all possible number of clusters given by range(2, max_clusters)."
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "max_clusters": "int",
                "random_state": "int | RandomState | None"
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fits kmeans model to the data.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples,), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "self": "returns an instance of self"
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > fit_predict": {
            "Description": "Fit the models and predict clusters based on best model.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples, ), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "labels": "array, shape (n_samples,)\nComponent labels."
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > predict": {
            "Description": "Predict clusters based on best model.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples, ), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "labels": "array, shape (n_samples,)\nComponent labels."
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "KMeansCluster"
    },
    {
        "Section_id": "GaussianCluster",
        "Description": "Gaussian Mixture Model (GMM)\nRepresentation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution. It computes all possible models from one component to max_components. The best model is given by the lowest BIC score."
    },
    {
        "Field List > Parameters": {
            "min_components": "int, default=2.\nThe minimum number of mixture components to consider (unless max_components is None, in which case this is the maximum number of components to consider). If max_componens is not None, min_components must be less than or equal to max_components.",
            "max_components": "int or None, default=None.\nThe maximum number of mixture components to consider. Must be greater than or equal to min_components.",
            "covariance_type": "{'all' (default), 'full', 'tied', 'diag', 'spherical'}, optional\nString or list/array describing the type of covariance parameters to use. If a string, it must be one of:\n\n'all'\nconsiders all covariance structures in ['spherical', 'diag', 'tied', 'full']\n'full'\neach component has its own general covariance matrix\n'tied'\nall components share the same general covariance matrix\n'diag'\neach component has its own diagonal covariance matrix\n'spherical'\neach component has its own single variance\nIf a list/array, it must be a list/array of strings containing only\n'spherical', 'tied', 'diag', and/or 'full'.",
            "tol": "float, defaults to 1e-3.\nThe convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.",
            "reg_covar": "float, defaults to 1e-6.\nNon-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.",
            "max_iter": "int, defaults to 100.\nThe number of EM iterations to perform.",
            "n_init": "int, defaults to 1.\nThe number of initializations to perform. The best results are kept.",
            "init_params": "{'kmeans', 'random'}, defaults to 'kmeans'.\nThe method used to initialize the weights, the means and the precisions. Must be one of:\n\n'kmeans' : responsibilities are initialized using kmeans.\n'random' : responsibilities are initialized randomly.",
            "random_state": "int, RandomState instance or None, optional (default=None)\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Attributes": {
            "n_components_": "int\nOptimal number of components based on BIC.",
            "covariance_type_": "str\nOptimal covariance type based on BIC.",
            "model_GaussianMixture_": "object\nFitted GaussianMixture object fitted with optimal number of components and optimal covariance structure.",
            "bic_": "pandas.DataFrame\nA pandas DataFrame of BIC values computed for all possible number of clusters given by range(min_components, max_components + 1) and all covariance structures given by covariance_type.",
            "ari_": "pandas.DataFrame\nOnly computed when y is given. Pandas Dataframe containing ARI values computed for all possible number of clusters given by range(min_components, max_components) and all covariance structures given by covariance_type."
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "min_components": "int",
                "max_components": "int | None",
                "covariance_type": "str",
                "tol": "float",
                "reg_covar": "float",
                "max_iter": "int",
                "n_init": "int",
                "init_params": "str",
                "random_state": "int | RandomState | None"
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fits gaussian mixure model to the data. Estimate model parameters with the EM algorithm.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples,), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "self": "self"
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > fit_predict": {
            "Description": "Fit the models and predict clusters based on best model.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples,), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "labels": "array, shape (n_samples,)\nComponent labels."
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > predict": {
            "Description": "Predict clusters based on best model.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples,), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "labels": "array, shape (n_samples,)\nComponent labels."
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict"
            },
            "Returns": {
                "self": "estimator instance"
            }
        },
        "Section_id": "GaussianCluster"
    },
    {
        "Section_id": "AutoGMMCluster",
        "Description": "Automatic Gaussian Mixture Model (GMM) selection.\nClustering algorithm using a hierarchical agglomerative clustering then Gaussian mixtured model (GMM) fitting. Different combinations of agglomeration, GMM, and cluster numbers are used and the clustering with the best selection criterion (bic/aic) is chosen."
    },
    {
        "Field List > Parameters": {
            "min_components": "int, default=2.\nThe minimum number of mixture components to consider (unless max_components is None, in which case this is the maximum number of components to consider). If max_components is not None, min_components must be less than or equal to max_components. If label_init is given, min_components must match number of unique labels in label_init.",
            "max_components": "int or None, default=10.\nThe maximum number of mixture components to consider. Must be greater than or equal to min_components. If label_init is given, min_components must match number of unique labels in label_init.",
            "affinity": "{'euclidean','manhattan','cosine','none', 'all' (default)}, optional\nString or list/array describing the type of affinities to use in agglomeration. If a string, it must be one of:\n\n'euclidean'\nL2 norm\n'manhattan'\nL1 norm\n'cosine'\ncosine similarity\n'none'\nno agglomeration - GMM is initialized with k-means\n'all'\nconsiders all affinities in ['euclidean','manhattan','cosine','none']\nIf a list/array, it must be a list/array of strings containing only 'euclidean', 'manhattan', 'cosine', and/or 'none'.\n\nNote that cosine similarity can only work when all of the rows are not the zero vector. If the input matrix has a zero row, cosine similarity will be skipped and a warning will be thrown.",
            "linkage": "{'ward','complete','average','single', 'all' (default)}, optional\nString or list/array describing the type of linkages to use in agglomeration. If a string, it must be one of:\n\n'ward'\nward's clustering, can only be used with euclidean affinity\n'complete'\ncomplete linkage\n'average'\naverage linkage\n'single'\nsingle linkage\n'all'\nconsiders all linkages in ['ward','complete','average','single']\nIf a list/array, it must be a list/array of strings containing only 'ward', 'complete', 'average', and/or 'single'.",
            "covariance_type": "{'full', 'tied', 'diag', 'spherical', 'all' (default)} , optional\nString or list/array describing the type of covariance parameters to use. If a string, it must be one of:\n\n'full'\neach component has its own general covariance matrix\n'tied'\nall components share the same general covariance matrix\n'diag'\neach component has its own diagonal covariance matrix\n'spherical'\neach component has its own single variance\n'all'\nconsiders all covariance structures in ['spherical', 'diag', 'tied', 'full']\nIf a list/array, it must be a list/array of strings containing only 'spherical', 'tied', 'diag', and/or 'spherical'.",
            "random_state": "int, RandomState instance or None, optional (default=None)\nThere is randomness in k-means initialization of sklearn.mixture.GaussianMixture. This parameter is passed to GaussianMixture to control the random state. If int, random_state is used as the random number generator seed; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.",
            "label_init": "array-like, shape (n_samples,), optional (default=None)\nList of labels for samples if available. Used to initialize the model. If provided, min_components and max_components must match the number of unique labels given here.",
            "kmeans_n_init": "int, optional (default = 1)\nIf kmeans_n_init is larger than 1 and label_init is None, additional kmeans_n_init-1 runs of sklearn.mixture.GaussianMixture initialized with k-means will be performed for all covariance parameters in covariance_type.",
            "max_iter": "int, optional (default = 100).\nThe maximum number of EM iterations to perform.",
            "selection_criteria": "str {\"bic\" or \"aic\"}, optional, (default=\"bic\")\nselect the best model based on Bayesian Information Criterion (bic) or Aikake Information Criterion (aic)",
            "verbose": "int, optional (default = 0)\nEnable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step.",
            "max_agglom_size": "int or None, optional (default = 2000)\nThe maximum number of datapoints on which to do agglomerative clustering as the initialization to GMM. If the number of datapoints is larger than this value, a random subset of the data is used for agglomerative initialization. If None, all data is used for agglomerative clustering for initialization.",
            "n_jobs": "int or None, optional (default = None)\nThe number of jobs to use for the computation. This works by computing each of the initialization runs in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See https://scikit-learn.org/stable/glossary.html#term-n-jobs for more details."
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Attributes": {
            "results_pandas_": "pandas.DataFrame\nContains exhaustive information about all the clustering runs. Columns are:\n'model'GaussianMixture object\nGMM clustering fit to the data\n\n'bic/aic'float\nBayesian Information Criterion\n\n'ari'float or nan\nAdjusted Rand Index between GMM classification, and true classification, nan if y is not given\n\n'n_components'int\nnumber of clusters\n\n'affinity'{'euclidean','manhattan','cosine','none'}\naffinity used in Agglomerative Clustering\n\n'linkage'{'ward','complete','average','single'}\nlinkage used in Agglomerative Clustering\n\n'covariance_type'{'full', 'tied', 'diag', 'spherical'}\ncovariance type used in GMM\n\n'reg_covar'float\nregularization used in GMM",
            "criter_": "the best (lowest) Bayesian Information Criterion",
            "n_components_": "int\nnumber of clusters in the model with the best bic/aic",
            "covariance_type_": "str\ncovariance type in the model with the best bic/aic",
            "affinity_": "str\naffinity used in the model with the best bic/aic",
            "linkage_": "str\nlinkage used in the model with the best bic/aic",
            "reg_covar_": "float\nregularization used in the model with the best bic/aic",
            "ari_": "float\nARI from the model with the best bic/aic, nan if no y is given",
            "model_": "sklearn.mixture.GaussianMixture\nobject with the best bic/aic"
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "min_components": "int",
                "max_components": "int | None",
                "affinity": "str | ndarray | List[str]",
                "linkage": "str | ndarray | List[str]",
                "covariance_type": "str | ndarray | List[str]",
                "random_state": "int | RandomState | None",
                "label_init": "ndarray | List[int] | None",
                "kmeans_n_init": "int",
                "max_iter": "int",
                "verbose": "int",
                "selection_criteria": "str",
                "max_agglom_size": "int | None",
                "n_jobs": "int | None"
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fits gaussian mixture model to the data. Initialize with agglomerative clustering then estimate model parameters with EM algorithm.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples,), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > fit_predict": {
            "Description": "Fit the models and predict clusters based on best model.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples, ), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "labels": "array, shape (n_samples,)\nComponent labels."
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > predict": {
            "Description": "Predict clusters based on best model.",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\nList of n_features-dimensional data points. Each row corresponds to a single data point.",
                "y": "array-like, shape (n_samples, ), optional (default=None)\nList of labels for X if available. Used to compute ARI scores."
            },
            "Returns": {
                "labels": "array, shape (n_samples,)\nComponent labels."
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Rubric": {
            "References": [
                "Jeffrey D. Banfield and Adrian E. Raftery. Model-based gaussian and non-gaussian clustering. Biometrics, 49:803\u2013821, 1993.",
                "Abhijit Dasgupta and Adrian E. Raftery. Detecting features in spatial point processes with clutter via model-based clustering. Journal of the American Statistical Association, 93(441):294\u2013302, 1998."
            ]
        },
        "Section_id": "AutoGMMCluster"
    },
    {
        "Section_id": "DivisiveCluster",
        "Description": "Recursively clusters data based on a chosen clustering algorithm. This algorithm implements a 'divisive' or 'top-down' approach."
    },
    {
        "Field List > Parameters": {
            "cluster_method": "str {'gmm', 'kmeans'}, defaults to 'gmm'.\nThe underlying clustering method to apply. If 'gmm' will use AutoGMMCluster. If 'kmeans', will use KMeansCluster.",
            "min_components": "int, defaults to 1.\nThe minimum number of mixture components/clusters to consider for the first split if 'gmm' is selected as cluster_method; and is set to 1 for later splits. If cluster_method is 'kmeans', it is set to 2 for all splits.",
            "max_components": "int, defaults to 2.\nThe maximum number of mixture components/clusters to consider at each split.",
            "min_split": "int, defaults to 1.\nThe minimum size of a cluster for it to be considered to be split again.",
            "max_level": "int, defaults to 4.\nThe maximum number of times to recursively cluster the data.",
            "delta_criter": "float, non-negative, defaults to 0.\nThe smallest difference between selection criterion values of a new model and the current model that is required to accept the new model. Applicable only if cluster_method is 'gmm'.",
            "cluster_kws": "dict, defaults to {}\nKeyword arguments (except min_components and max_components) for chosen clustering method."
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Field List > Attributes": {
            "model_": "GaussianMixture or KMeans object\nFitted clustering object based on which cluster_method was used."
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "cluster_method": "str {'gmm', 'kmeans'}",
                "min_components": "int",
                "max_components": "int",
                "cluster_kws": "dict",
                "min_split": "int",
                "max_level": "int",
                "delta_criter": "float"
            }
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fits clustering models to the data as well as resulting clusters",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\n"
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Field List > Methods > fit_predict": {
            "Description": "Fits clustering models to the data as well as resulting clusters and using fitted models to predict a hierarchy of labels",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\n",
                "fcluster": "bool, default=False\nif True, returned labels will be re-numbered so that each column of labels represents a flat clustering at current level, and each label corresponds to a cluster indexed the same as the corresponding node in the overall clustering dendrogram",
                "level": "int, optional (default=None)\nthe level of a single flat clustering to generate only available if fcluster is True"
            },
            "Returns": {
                "labels": "array_label,shape (n_samples, n_levels)\nif no level specified; otherwise, shape (n_samples,)"
            }
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Field List > Methods > predict": {
            "Description": "Predicts a hierarchy of labels based on fitted models",
            "Parameters": {
                "X": "array-like, shape (n_samples, n_features)\n",
                "fcluster": "bool, default=False\nif True, returned labels will be re-numbered so that each column of labels represents a flat clustering at current level, and each label corresponds to a cluster indexed the same as the corresponding node in the overall clustering dendrogram",
                "level": "int, optional (default=None)\nthe level of a single flat clustering to generate only available if fcluster is True"
            },
            "Returns": {
                "labels": "array-like, shape (n_samples, n_levels)\nif no level specified; otherwise, shape (n_samples,)"
            }
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Field List > Methods > set_predict_request": {
            "Description": "Request metadata passed to the predict method.\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to the predict method.\nFalse: metadata is not requested, and not passed to the predict.\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\nStr: metadata should be passed to the meta-estimator with this given alias instead of the original name.",
            "Parameters": {
                "fcluster": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for fcluster parameter in predict.",
                "level": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for level parameter in predict."
            }
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Rubric": {
            "Note": "This class inherits from anytree.node.nodemixin.NodeMixin, a lightweight class for doing various simple operations on trees.\nThis algorithm was strongly inspired by maggotcluster, a divisive clustering algorithm in https://github.com/neurodata/maggot_models and the algorithm for estimating a hierarchical stochastic block model presented in [2].",
            "References": [
                "Athey, T. L., & Vogelstein, J. T. (2019). AutoGMM: Automatic Gaussian Mixture Modeling in Python. arXiv preprint arXiv:1909.02688.",
                "Lyzinski, V., Tang, M., Athreya, A., Park, Y., & Priebe, C. E (2016). Community detection and classification in hierarchical stochastic blockmodels. IEEE Transactions on Network Science and Engineering, 4(1), 13-26."
            ]
        },
        "Section_id": "DivisiveCluster"
    },
    {
        "Section_id": "load_drosophila_left",
        "Description": "Load the left Drosophila larva mushroom body connectome\nThe mushroom body is a learning and memory center in the fly brain which is involved in sensory integration and processing. This connectome was observed by electron microscopy and then individial neurons were reconstructed; synaptic partnerships between these neurons became the edges of the graph."
    },
    {
        "Field List > Parameters": {
            "return_labels": "bool, optional (default=False)\nwhether to have a second return value which is an array of cell type labels for each node in the adjacency matrix"
        },
        "Section_id": "load_drosophila_left"
    },
    {
        "Field List > Returns": {
            "graph": "np.ndarray\nAdjacency matrix of the connectome",
            "labels": "np.ndarray\nOnly returned if return_labels is true. Array of string labels for each cell (vertex)"
        },
        "Section_id": "load_drosophila_left"
    },
    {
        "Rubric": {
            "References": [
                "Eichler, K., Li, F., Litwin-Kumar, A., Park, Y., Andrade, I., Schneider-Mizell, C. M., ... & Fetter, R. D. (2017). The complete connectome of a learning and memory centre in an insect brain. Nature, 548(7666), 175."
            ]
        },
        "Section_id": "load_drosophila_left"
    },
    {
        "Section_id": "load_drosophila_right",
        "Description": "Load the right Drosophila larva mushroom body connectome\nThe mushroom body is a learning and memory center in the fly brain which is involved in sensory integration and processing. This connectome was observed by electron microscopy and then individial neurons were reconstructed; synaptic partnerships between these neurons became the edges of the graph."
    },
    {
        "Field List > Parameters": {
            "return_labels": "bool, optional (default=False)\nwhether to have a second return value which is an array of cell type labels for each node in the adjacency matrix"
        },
        "Section_id": "load_drosophila_right"
    },
    {
        "Field List > Returns": {
            "graph": "np.ndarray\nAdjacency matrix of the connectome",
            "labels": "np.ndarray\nOnly returned if return_labels is true. Array of string labels for each cell (vertex)"
        },
        "Section_id": "load_drosophila_right"
    },
    {
        "Rubric": {
            "References": [
                "Eichler, K., Li, F., Litwin-Kumar, A., Park, Y., Andrade, I., Schneider-Mizell, C. M., ... & Fetter, R. D. (2017). The complete connectome of a learning and memory centre in an insect brain. Nature, 548(7666), 175."
            ]
        },
        "Section_id": "load_drosophila_right"
    },
    {
        "Section_id": "load_mice",
        "Description": "Load connectomes of mice from distinct genotypes.\nDataset of 32 mouse connectomes derived from whole-brain diffusion magnetic resonance imaging of four distinct mouse genotypes: BTBR T+ Itpr3tf/J (BTBR), C57BL/6J(B6), CAST/EiJ (CAST), and DBA/2J (DBA2). For each strain, connectomes were generated from eight age-matched mice (N = 8 per strain), with a sex distribution of of four males and four females. Each connectome was parcellated using asymmetric Waxholm Space, yielding a vertex set with a total of 332 regions of interest (ROIs) symmetrically distributed across the left and right hemispheres. Within a given hemisphere, there are seven superstructures consisting up multiple ROIs, resulting in a total of 14 distinct communities in each connectome."
    },
    {
        "Field List > Returns": {
            "data": "Bunch\nDictionary-like object, with the following attributes.\ngraphs: list of np.ndarray\nList of adjacency matrices of the connectome\nlabels: np.ndarray\nArray of string labels for each mouse (subject)\natlas: pd.DataFrame\nDataFrame of information for each ROI\nblocks: pd.DataFrame\nDataFrame of block assignments for each ROI\nfeatures: pd.DataFrame\nDataFrame of anatomical features for each ROI in each connectome\nparticipants: pd.DataFrame\nDataFrame of subject IDs and genotypes for each connectome\nmeta: Dictionary\nDictionary with meta information about the dataset (n_subjects and n_vertices)"
        },
        "Section_id": "load_mice"
    },
    {
        "Section_id": "select_dimension",
        "Description": "Generates profile likelihood from array based on Zhu and Godsie method. Elbows correspond to the optimal embedding dimension."
    },
    {
        "Field List > Parameters": {
            "X": "1d or 2d array-like\nInput array generate profile likelihoods for. If 1d-array, it should be sorted in decreasing order. If 2d-array, shape should be (n_samples, n_features).",
            "n_components": " int, optional, default: None.\nNumber of components to embed. If None, n_components = floor(log2(min(n_samples, n_features))). Ignored if X is 1d-array.",
            "n_elbows": "int, optional, default: 2.\nNumber of likelihood elbows to return. Must be > 1.",
            "threshold": "float, int, optional, default: None\nIf given, only consider the singular values that are > threshold. Must be >= 0.",
            "return_likelihoods": "bool, optional, default: False\nIf True, returns the all likelihoods associated with each elbow."
        },
        "Section_id": "select_dimension"
    },
    {
        "Field List > Returns": {
            "elbows": "List\nElbows indicate subsequent optimal embedding dimensions. Number of elbows may be less than n_elbows if there are not enough singular values.",
            "sing_vals": "List\nThe singular values associated with each elbow.",
            "likelihoods": "List of array-like\nArray of likelihoods of the corresponding to each elbow. Only returned if return_likelihoods is True."
        },
        "Section_id": "select_dimension"
    },
    {
        "Rubric": {
            "References": [
                "Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2), pp.918-930."
            ]
        },
        "Section_id": "select_dimension"
    },
    {
        "Section_id": "select_svd",
        "Description": "Dimensionality reduction using SVD.\nPerforms linear dimensionality reduction by using either full singular value decomposition (SVD) or truncated SVD. Full SVD is performed using SciPy's wrapper for ARPACK, while truncated SVD is performed using either SciPy's wrapper for LAPACK or Sklearn's implementation of randomized SVD.\nIt also performs optimal dimensionality selection using Zhu & Godsie algorithm if number of target dimension is not specified."
    },
    {
        "Field List > Parameters": {
            "X": "array-like, shape (n_samples, n_features)\nThe data to perform svd on.",
            "n_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "algorithm": "{'randomized' (default), 'full', 'truncated'}, optional\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n'full'\nComputes full svd using scipy.linalg.svd() Does not support graph input of type scipy.sparse.csr_array\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()\n'eigsh'\nComputes svd of a real, symmetric square matrix using scipy.sparse.linalg.eigsh(). Extremely fast for these types of matrices.",
            "n_iter": "int, optional, default: 5\nNumber of iterations for randomized SVD solver. Ignored for 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "svd_seed": "int or None, optional, default: None\nOnly applicable for algorithm='randomized'; allows you to seed the randomized svd solver for deterministic, albeit pseudo-randomized behavior."
        },
        "Section_id": "select_svd"
    },
    {
        "Field List > Returns": {
            "U": "array-like, shape (n_samples, n_components)\nLeft singular vectors corresponding to singular values.",
            "D": "array-like, shape (n_components)\nSingular values in decreasing order, as a 1d array.",
            "V": "array-like, shape (n_components, n_samples)\nRight singular vectors corresponding to singular values."
        },
        "Section_id": "select_svd"
    },
    {
        "Rubric": {
            "References": [
                "Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2), pp.918-930."
            ]
        },
        "Section_id": "select_svd"
    },
    {
        "Section_id": "AdjacencySpectralEmbed",
        "Description": "Class for computing the adjacency spectral embedding of a graph.\nThe adjacency spectral embedding (ASE) is a k-dimensional Euclidean representation of the graph based on its adjacency matrix. It relies on an SVD to reduce the dimensionality to the specified k, or if k is unspecified, can find a number of dimensions automatically (see select_svd)."
    },
    {
        "Field List > Parameters": {
            "n_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "algorithm": "{'randomized' (default), 'full', 'truncated'}, optional\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n'full'\nComputes full svd using scipy.linalg.svd() Does not support graph input of type scipy.sparse.csr_array\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "n_iter": "int, optional (default = 5)\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "check_lcc": "bool , optional (default = True)\nWhether to check if input graph is connected. May result in non-optimal results if the graph is unconnected. If True and input is unconnected, a UserWarning is thrown. Not checking for connectedness may result in faster computation.",
            "diag_aug": "bool, optional (default = True)\nWhether to replace the main diagonal of the adjacency matrix with a vector corresponding to the degree (or sum of edge weights for a weighted network) before embedding. Empirically, this produces latent position estimates closer to the ground truth.",
            "concat": "bool, optional (default False)\nIf graph is directed, whether to concatenate left and right (out and in) latent positions along axis 1.",
            "svd_seed": "int or None (default None)\nOnly applicable for algorithm='randomized'; allows you to seed the randomized svd solver for deterministic, albeit pseudo-randomized behavior."
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Attributes": {
            "n_features_in_": "int\nNumber of features passed to the fit() method.",
            "latent_left_": "array, shape (n_samples, n_components)\nEstimated left latent positions of the graph.",
            "latent_right_": "array, shape (n_samples, n_components), or None\nOnly computed when the graph is directed, or adjacency matrix is assymetric. Estimated right latent positions of the graph. Otherwise, None.",
            "singular_values_": "array, shape (n_components)\nSingular values associated with the latent position matrices."
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "n_components": "int | None",
                "n_elbows": "int | None",
                "algorithm": "str",
                "n_iter": "int",
                "check_lcc": "bool",
                "diag_aug": "bool",
                "concat": "bool",
                "svd_seed": "int | None"
            }
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit ASE model to input graph",
            "Parameters": {
                "graph": "array-like, scipy.sparse.csr_array, or networkx.Graph\nInput graph to embed.",
                "y": "Ignored"
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Fit the model with graphs and apply the transformation.\nn_dimension is either automatically determined or based on user input.",
            "Parameters": {
                "graph": "array-like, scipy.sparse.csr_array, or networkx.Graph\nInput graph to embed."
            },
            "Returns": {
                "out": "np.ndarray OR length 2 tuple of np.ndarray.\nIf undirected then returns single np.ndarray of latent position, shape(n_vertices, n_components). If directed, concat is True then concatenate latent matrices on axis 1, shape(n_vertices, 2*n_components). If directed, concat is False then tuple of the latent matrices. Each of shape (n_vertices, n_components)."
            }
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\nStr: metadata should be passed to the meta-estimator with this given alias instead of the original name.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Field List > Rubric": {
            "Note": "The singular value decomposition is used to find an orthonormal basis for a matrix, which in our case is the adjacency matrix of the graph. These basis vectors (in the matrices U or V) are ordered according to the amount of variance they explain in the original matrix. By selecting a subset of these basis vectors (through our choice of dimensionality reduction) we can find a lower dimensional space in which to represent the graph.",
            "References": [
                "Sussman, D.L., Tang, M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012",
                "Levin, K., Roosta-Khorasani, F., Mahoney, M. W., & Priebe, C. E. (2018). Out-of-sample extension of graph adjacency spectral embedding. PMLR: Proceedings of Machine Learning Research, 80, 2975-2984."
            ]
        },
        "Section_id": "AdjacencySpectralEmbed"
    },
    {
        "Section_id": "LaplacianSpectralEmbed",
        "Description": "Class for computing the laplacian spectral embedding of a graph.\nThe laplacian spectral embedding (LSE) is a k-dimensional Euclidean representation of the graph based on its Laplacian matrix. It relies on an SVD to reduce the dimensionality to the specified n_components, or if n_components is unspecified, can find a number of dimensions automatically."
    },
    {
        "Field List > Parameters": {
            "form": "{'DAD' (default), 'I-DAD', 'R-DAD'}, optional\nSpecifies the type of Laplacian normalization to use. See to_laplacian() for more details regarding form.",
            "n_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "algorithm": "{'randomized' (default), 'full', 'truncated'}, optional\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n'full'\nComputes full svd using scipy.linalg.svd()\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "n_iter": "int, optional (default = 5)\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "check_lcc": "bool , optional (defult = True)\nWhether to check if input graph is connected. May result in non-optimal results if the graph is unconnected. If True and input is unconnected, a UserWarning is thrown. Not checking for connectedness may result in faster computation.",
            "regularizer": "int, float or None, optional (default=None)\nConstant to be added to the diagonal of degree matrix. If None, average node degree is added. If int or float, must be >= 0. Only used when form is 'R-DAD'.",
            "concat": "bool, optional (default False)\nIf graph is directed, whether to concatenate left and right (out and in) latent positions along axis 1."
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Attributes": {
            "n_features_in_": "int\nNumber of features passed to the fit() method.",
            "latent_left_": "array, shape (n_samples, n_components)\nEstimated left latent positions of the graph.",
            "latent_right_": "array, shape (n_samples, n_components), or None\nOnly computed when the graph is directed, or adjacency matrix is assymetric. Estimated right latent positions of the graph. Otherwise, None.",
            "singular_values_": "array, shape (n_components)\nSingular values associated with the latent position matrices.",
            "svd_seed_": "int or None\nOnly applicable for algorithm='randomized'; allows you to seed the randomized svd solver for deterministic, albeit pseudo-randomized behavior."
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "form": "str {'DAD' (default), 'I-DAD', 'R-DAD'}",
                "n_components": "int | None",
                "n_elbows": "int",
                "algorithm": "str",
                "n_iter": "int",
                "check_lcc": "bool",
                "regularizer": "int | float | None",
                "concat": "bool"
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit LSE model to input graph",
            "Parameters": {
                "graph": "array-like, scipy.sparse.csr_array, or networkx.Graph\nInput graph to embed."
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Fit the model with graphs and apply the transformation.\nn_dimension is either automatically determined or based on user input.",
            "Parameters": {
                "graph": "array-like, scipy.sparse.csr_array, or networkx.Graph\nInput graph to embed."
            },
            "Returns": {
                "out": "np.ndarray OR length 2 tuple of np.ndarray.\nIf undirected then returns single np.ndarray of latent position, shape(n_vertices, n_components). If directed, concat is True then concatenate latent matrices on axis 1, shape(n_vertices, 2*n_components). If directed, concat is False then tuple of the latent matrices. Each of shape (n_vertices, n_components)."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\nStr: metadata should be passed to the meta-estimator with this given alias instead of the original name.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Field List > Methods > transform": {
            "Description": "Obtain latent positions from an adjacency matrix or matrix of out-of-sample vertices. For more details on transforming out-of-sample vertices, see Out-of-Sample (OOS) Embedding\nFor mathematical background, see [2].",
            "Parameters": {
                "X": "array-like or tuple, original shape or (n_oos_vertices, n_vertices).\nThe original fitted matrix ('graph' in fit) or new out-of-sample data. If X is the original fitted matrix, returns a matrix close to self.fit_transform(X).\n\nIf X is an out-of-sample matrix, n_oos_vertices is the number of new vertices, and n_vertices is the number of vertices in the original graph. If tuple, graph is directed and X[0] contains edges from out-of-sample vertices to in-sample vertices."
            },
            "Returns": {
                "out": "np.ndarray OR length 2 tuple of np.ndarray\nArray of latent positions, shape (n_oos_vertices, n_components) or (n_vertices, n_components). Transforms the fitted matrix if it was passed in.\n\nIf X is an array or tuple containing adjacency vectors corresponding to new nodes, returns the estimated latent positions for the new out-of-sample adjacency vectors. If undirected, returns array. If directed, returns (X_out, X_in), where X_out contains latent positions corresponding to nodes with edges from out-of-sample vertices to in-sample vertices."
            }
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Rubric": {
            "Note": "The singular value decomposition:\n\nis used to find an orthonormal basis for a matrix, which in our case is the Laplacian matrix of the graph. These basis vectors (in the matrices U or V) are ordered according to the amount of variance they explain in the original matrix. By selecting a subset of these basis vectors (through our choice of dimensionality reduction) we can find a lower dimensional space in which to represent the graph.",
            "References": [
                "Sussman, D.L., Tang M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012.",
                "Von Luxburg, Ulrike. \"A tutorial on spectral clustering,\" Statistics and computing, Vol. 17(4), pp. 395-416, 2007.",
                "Rohe, Karl, Sourav Chatterjee, and Bin Yu. \"Spectral clustering and the high-dimensional stochastic blockmodel,\" The Annals of Statistics, Vol. 39(4), pp. 1878-1915, 2011."
            ]
        },
        "Section_id": "LaplacianSpectralEmbed"
    },
    {
        "Section_id": "node2vec_embed",
        "Description": "Generates a node2vec embedding from a given graph. Will follow the word2vec algorithm to create the embedding."
    },
    {
        "Field List > Parameters": {
            "graph": "Union[nx.Graph, nx.DiGraph]\nA networkx graph or digraph. A multigraph should be turned into a non-multigraph so that the calling user properly handles the multi-edges (i.e. aggregate weights or take last edge weight). If the graph is unweighted, the weight of each edge will default to 1.",
            "num_walks": "int\nNumber of walks per source. Default is 10.",
            "walk_length": "int\nLength of walk per source. Default is 40.",
            "return_hyperparameter": "float\nReturn hyperparameter (p). Default is 1.0",
            "inout_hyperparameter": "float\nInout hyperparameter (q). Default is 1.0",
            "dimensions": "int\nDimensionality of the word vectors. Default is 128.",
            "window_size": "int\nMaximum distance between the current and predicted word within a sentence. Default is 2.",
            "workers": "int\nUse these many worker threads to train the model. Default is 8.",
            "iterations": "int\nNumber of epochs in stochastic gradient descent (SGD). Default is 3.",
            "interpolate_walk_lengths_by_node_degree": "bool\nUse a dynamic walk length that corresponds to each nodes degree. If the node is in the bottom 20 percentile, default to a walk length of 1. If it is in the top 10 percentile, use walk_length. If it is in the 20-80 percentiles, linearly interpolate between 1 and walk_length. This will reduce lower degree nodes from biasing your resulting embedding. If a low degree node has the same number of walks as a high degree node (which it will if this setting is not on), then the lower degree nodes will take a smaller breadth of random walks when compared to the high degree nodes. This will result in your lower degree walks dominating your higher degree nodes.",
            "random_seed": "int\nSeed to be used for reproducible results. Default is None and will produce a random output. Note that for a fully deterministically-reproducible run, you must also limit to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. In addition the environment variable PYTHONHASHSEED must be set to control hash randomization."
        },
        "Section_id": "node2vec_embed"
    },
    {
        "Field List > Returns": {
            "Tuple[np.array, List[Any]]": "A tuple containing a matrix, with each row index corresponding to the embedding for each node. The tuple also contains a vector containing the corresponding vertex labels for each row in the matrix. The matrix and vector are positionally correlated."
        },
        "Section_id": "node2vec_embed"
    },
    {
        "Rubric": {
            "Note": "The original reference implementation of node2vec comes from Aditya Grover from\nhttps://github.com/aditya-grover/node2vec/.\nFurther details on the Alias Method used in this functionality can be found at\nhttps://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
            "References": [
                "Aditya Grover and Jure Leskovec \"node2vec: Scalable Feature Learning for Networks.\" Knowledge Discovery and Data Mining, 2016."
            ]
        },
        "Section_id": "node2vec_embed"
    },
    {
        "Section_id": "OmnibusEmbed",
        "Description": "Omnibus embedding of arbitrary number of input graphs with matched vertex sets.\nGiven A1, A2, ..., An, a collection of (possibly weighted) adjacency matrices of a collection undirected graphs with matched vertices. Then the (mn x mn) omnibus matrix, M, has the subgraph where Mij = 1/2(Ai + Aj). The omnibus matrix is then embedded using adjacency spectral embedding."
    },
    {
        "Field List > Parameters": {
            "n_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "algorithm": "{'randomized' (default), 'full', 'truncated'}, optional\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n'full'\nComputes full svd using scipy.linalg.svd()\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "n_iter": "int, optional (default = 5)\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "check_lcc": "bool , optional (defult = True)\nWhether to check if the average of all input graphs are connected. May result in non-optimal results if the average graph is unconnected. If True and average graph is unconnected, a UserWarning is thrown.",
            "diag_aug": "bool, optional (default = True)\nWhether to replace the main diagonal of each adjacency matrices with a vector corresponding to the degree (or sum of edge weights for a weighted network) before embedding.",
            "concat": "bool, optional (default = False)\nIf graph(s) are directed, whether to concatenate each graph's left and right (out and in) latent positions along axis 1.",
            "svd_seed": "int or None (default = None)\nOnly applicable for algorithm='randomized'; allows you to seed the randomized svd solver for deterministic, albeit pseudo-randomized behavior.",
            "lse": "bool, optional (default = False)\nWhether to construct the Omni matrix use the laplacian matrices of the graphs and embed the Omni matrix with LSE"
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Attributes": {
            "n_features_in_": "int\nNumber of features passed to the fit() method.",
            "latent_left_": "array, shape (n_samples, n_components)\nEstimated left latent positions of the graph.",
            "latent_right_": "array, shape (n_samples, n_components), or None\nOnly computed when the graph is directed, or adjacency matrix is assymetric. Estimated right latent positions of the graph. Otherwise, None.",
            "singular_values_": "array, shape (n_components)\nSingular values associated with the latent position matrices."
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "n_components": "int | None",
                "n_elbows": "int | None",
                "algorithm": "str",
                "n_iter": "int",
                "check_lcc": "bool",
                "diag_aug": "bool",
                "concat": "bool",
                "svd_seed": "int | None",
                "lse": "bool"
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit Omnibus model to input graphs",
            "Parameters": {
                "graphs": "list of array-like, scipy.sparse.csr_array, or networkx.Graph\nInput graphs to embed.",
                "y": "Ignored"
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Fit the model with graphs and apply the transformation.\nn_dimension is either automatically determined or based on user input.",
            "Parameters": {
                "graphs": "list of array-like, scipy.sparse.csr_array, or networkx.Graph\nInput graphs to embed."
            },
            "Returns": {
                "out": "np.ndarray OR length 2 tuple of np.ndarray.\nIf undirected then returns single np.ndarray of latent position, shape(n_vertices, n_components). If directed, concat is True then concatenate latent matrices on axis 1, shape(n_vertices, 2*n_components). If directed, concat is False then tuple of the latent matrices. Each of shape (n_vertices, n_components)."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\nStr: metadata should be passed to the meta-estimator with this given alias instead of the original name.",
            "Parameters": {
                "graphs": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graphs parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Field List > Methods > transform": {
            "Description": "Obtain latent positions from an adjacency matrix or matrix of out-of-sample vertices. For more details on transforming out-of-sample vertices, see Out-of-Sample (OOS) Embedding\nFor mathematical background, see [2].",
            "Parameters": {
                "X": "array-like or tuple, original shape or (n_oos_vertices, n_vertices).\nThe original fitted matrix ('graph' in fit) or new out-of-sample data."
            },
            "Returns": {
                "out": "np.ndarray OR length 2 tuple of np.ndarray\nArray of latent positions, shape (n_oos_vertices, n_components) or (n_vertices, n_components). Transforms the fitted matrix if it was passed in.\n\nIf X is an array or tuple containing adjacency vectors corresponding to new nodes, returns the estimated latent positions for the new out-of-sample adjacency vectors. If undirected, returns array. If directed, returns (X_out, X_in), where X_out contains latent positions corresponding to nodes with edges from out-of-sample vertices to in-sample vertices."
            }
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Rubric": {
            "Note": "The singular value decomposition is used to find an orthonormal basis for a matrix, which in our case is the adjacency matrix of the graph. These basis vectors (in the matrices U or V) are ordered according to the amount of variance they explain in the original matrix. By selecting a subset of these basis vectors (through our choice of dimensionality reduction) we can find a lower dimensional space in which to represent the graph.",
            "References": [
                "Sussman, D.L., Tang M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012.",
                "Levin, K., Roosta-Khorasani, F., Mahoney, M. W., & Priebe, C. E. (2018). Out-of-sample extension of graph adjacency spectral embedding. PMLR: Proceedings of Machine Learning Research, 80, 2975-2984."
            ]
        },
        "Section_id": "OmnibusEmbed"
    },
    {
        "Section_id": "MultipleASE",
        "Description": "Multiple Adjacency Spectral Embedding (MASE) embeds arbitrary number of input graphs with matched vertex sets.\nFor a population of undirected graphs, MASE assumes that the population of graphs is sampled from P(A) = \u2211_{k=1}^{K} \u03c0_k P(A|Z=k) where P(A|Z=k) = N(0, \u039b_k) and \u039b_k = \u03b1_k I + \u03b2_k J. Score matrices, \u039b_k, are allowed to vary for each graph, but are symmetric. All graphs share a common a latent position matrix X.\nFor a population of directed graphs, MASE assumes that the population is sampled from P(A) = \u2211_{k=1}^{K} \u03c0_k P(A|Z=k) where P(A|Z=k) = N(0, \u039b_k) and \u039b_k = \u03b1_k I + \u03b2_k J. In this case, score matrices \u039b_k can be assymetric and non-square, but all graphs still share a common latent position matrices X and Y."
    },
    {
        "Field List > Parameters": {
            "n_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "algorithm": "{'randomized' (default), 'full', 'truncated'}, optional\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n'full'\nComputes full svd using scipy.linalg.svd()\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "n_iter": "int, optional (default = 5)\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "scaled": "bool, optional (default=True)\nWhether to scale individual eigenvectors with eigenvalues in first embedding stage.",
            "diag_aug": "bool, optional (default = True)\nWhether to replace the main diagonal of each adjacency matrices with a vector corresponding to the degree (or sum of edge weights for a weighted network) before embedding.",
            "concat": "bool, optional (default False)\nIf graph(s) are directed, whether to concatenate each graph's left and right (out and in) latent positions along axis 1.",
            "svd_seed": "int or None (default None)\nOnly applicable for algorithm='randomized'; allows you to seed the randomized svd solver for deterministic, albeit pseudo-randomized behavior."
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Attributes": {
            "n_graphs_": "int\nNumber of graphs",
            "n_vertices_": "int\nNumber of vertices in each graph",
            "latent_left_": "array, shape (n_samples, n_components)\nEstimated left latent positions of the graph.",
            "latent_right_": "array, shape (n_samples, n_components), or None\nEstimated right latent positions of the graph. Only computed when the an input graph is directed, or adjacency matrix is assymetric. Otherwise, None.",
            "scores_": "array, shape (n_samples, n_components, n_components)\nEstimated \u039b matrices for each input graph.",
            "singular_values_": "array, shape (n_components) OR length 2 tuple of arrays\nIf input graph is undirected, equal to the singular values of the concatenated adjacency spectral embeddings. If input graph is directed, singular_values_ is a tuple of length 2, where singular_values_[0] corresponds to the singular values of the concatenated left adjacency spectral embeddings, and singular_values_[1] corresponds to the singular values of the concatenated right adjacency spectral embeddings."
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "n_components": "int | None",
                "n_elbows": "int",
                "algorithm": "str",
                "n_iter": "int",
                "scaled": "bool",
                "diag_aug": "bool",
                "concat": "bool",
                "svd_seed": "int | None"
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit MASE model to input graphs",
            "Parameters": {
                "graphs": "list of nx.Graph, ndarray or scipy.sparse.csr_array\nIf list of nx.Graph, each Graph must contain same number of nodes. If list of ndarray or csr_array, each array must have shape (n_vertices, n_vertices). If ndarray, then array must have shape (n_graphs, n_vertices, n_vertices).",
                "y": "Ignored"
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Fit the model with graphs and apply the embedding on graphs. n_components is either automatically determined or based on user input.",
            "Parameters": {
                "graphs": "list of nx.Graph, ndarray or scipy.sparse.csr_array\nIf list of nx.Graph, each Graph must contain same number of nodes. If list of ndarray or csr_array, each array must have shape (n_vertices, n_vertices). If ndarray, then array must have shape (n_graphs, n_vertices, n_vertices)."
            },
            "Returns": {
                "out": "np.ndarray or length 2 tuple of np.ndarray.\nIf input graphs were symmetric shape (n_vertices, n_components). If graphs were directed and concat is False, returns tuple of two arrays (same shape as above). The first corresponds to the left latent positions, and the second to the right latent positions. When concat is True left and right (out and in"
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\nStr: metadata should be passed to the meta-estimator with this given alias instead of the original name.",
            "Parameters": {
                "graphs": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graphs parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Field List > Methods > transform": {
            "Description": "Obtain latent positions from an adjacency matrix or matrix of out-of-sample vertices. For more details on transforming out-of-sample vertices, see Out-of-Sample (OOS) Embedding\nFor mathematical background, see [2].",
            "Parameters": {
                "X": "array-like or tuple, original shape or (n_oos_vertices, n_vertices).\nThe original fitted matrix ('graph' in fit) or new out-of-sample data. If X is the original fitted matrix, returns a matrix close to self.fit_transform(X).\n\nIf X is an out-of-sample matrix, n_oos_vertices is the number of new vertices, and n_vertices is the number of vertices in the original graph. If tuple, graph is directed and X[0] contains edges from out-of-sample vertices to in-sample vertices."
            },
            "Returns": {
                "out": "np.ndarray OR length 2 tuple of np.ndarray\nArray of latent positions, shape (n_oos_vertices, n_components) or (n_vertices, n_components). Transforms the fitted matrix if it was passed in.\n\nIf X is an array or tuple containing adjacency vectors corresponding to new nodes, returns the estimated latent positions for the new out-of-sample adjacency vectors. If undirected, returns array. If directed, returns (X_out, X_in), where X_out contains latent positions corresponding to nodes with edges from out-of-sample vertices to in-sample vertices."
            }
        },
        "Section_id": "MultipleASE"
    },
    {
        "Rubric": {
            "Note": "If the matrix was diagonally augmented (e.g., self.diag_aug was True), fit followed by transform will produce a slightly different matrix than fit_transform.\nTo get the original embedding, using fit_transform is recommended. In the directed case, if A is the original in-sample adjacency matrix, the tuple (A.T, A) will need to be passed to transform if you do not wish to use fit_transform.",
            "References": [
                "Sussman, D.L., Tang M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012.",
                "Levin, K., Roosta-Khorasani, F., Mahoney, M. W., & Priebe, C. E. (2018). Out-of-sample extension of graph adjacency spectral embedding. PMLR: Proceedings of Machine Learning Research, 80, 2975-2984"
            ]
        },
        "Section_id": "MultipleASE"
    },
    {
        "Section_id": "mug2vec",
        "Description": "Multigraphs-2-vectors (mug2vec).\n\nmug2vec is a sequence of three algorithms that learns a feature vector for each input graph.\n\nSteps:\n\n1. Pass to ranks - ranks all edge weights from smallest to largest valued edges then normalize by a constant.\n\n2. Omnibus embedding - jointly learns a low dimensional matrix representation for all graphs under the random dot product model (RDPG).\n\n3. Classical MDS (cMDS) - learns a feature vector for each graph by computing Euclidean distance between each pair of graph embeddings from omnibus embedding, followed by an eigen decomposition."
    },
    {
        "Field List > Parameters": {
            "pass_to_ranks": "{'simple-nonzero' (default), 'simple-all', 'zero-boost'} string, or None\n'simple-nonzero'\nassigns ranks to all non-zero edges, settling ties using the average. Ranks are then scaled by \n\n'simple-all'\nassigns ranks to all non-zero edges, settling ties using the average. Ranks are then scaled by \n\n where n is the number of nodes\n\n'zero-boost'\npreserves the edge weight for all 0s, but ranks the other edges as if the ranks of all 0 edges has been assigned. If there are 10 0-valued edges, the lowest non-zero edge gets weight 11 / (number of possible edges). Ties settled by the average of the weight that those edges would have received. Number of possible edges is determined by the type of graph (loopless or looped, directed or undirected).\n\nNone\nNo pass to ranks applied.",
            "omnibus_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "omnibus_n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "cmds_components": "int or None, default = None\nDesired dimensionality of output data. If 'full', n_components must be <= min(X.shape). Otherwise, n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension() using n_elbows argument.",
            "cmds_n_elbows": "int, optional, default: 2\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "svd_seed": "int or None (default None)\nAllows you to seed the randomized svd solver used in the Omnibus embedding for deterministic, albeit pseudo-randomized behavior."
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Attributes": {
            "omnibus_n_components_": "int\nEquals the parameter n_components. If input n_components was None, then equals the optimal embedding dimension.",
            "cmds_n_components_": "int\nEquals the parameter n_components. If input n_components was None, then equals the optimal embedding dimension.",
            "embeddings_": "array, shape (n_components, n_features)\nEmbeddings from the pipeline. Each graph is a point in n_features dimensions."
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "pass_to_ranks": "typing_extensions.Literal[simple-nonzero, simple-all, zero-boost]",
                "omnibus_components": "int | None",
                "omnibus_n_elbows": "int",
                "cmds_components": "int | None",
                "cmds_n_elbows": "int",
                "svd_seed": "int | None"
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Computes a vector for each graph.",
            "Parameters": {
                "graphs": "list of nx.Graph or ndarray, or ndarray\nIf list of nx.Graph, each Graph must contain same number of nodes. If list of ndarray, each array must have shape (n_vertices, n_vertices). If ndarray, then array must have shape (n_graphs, n_vertices, n_vertices).",
                "y": "Ignored"
            },
            "Returns": {
                "self": "mug2vec\nReturns an instance of self."
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Computes a vector for each graph.",
            "Parameters": {
                "graphs": "list of nx.Graph or ndarray, or ndarray\nIf list of nx.Graph, each Graph must contain same number of nodes. If list of ndarray, each array must have shape (n_vertices, n_vertices). If ndarray, then array must have shape (n_graphs, n_vertices, n_vertices).",
                "y": "Ignored"
            },
            "Returns": {
                "embedding": "ndarray\nembeddings generated by fit."
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\nThe options for each parameter are:\n\nTrue: metadata is requested",
            "Parameters": {
                "graphs": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graphs parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "mug2vec"
    },
    {
        "Section_id": "ClassicalMDS",
        "Description": "Classical multidimensional scaling (cMDS).\n\ncMDS seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space."
    },
    {
        "Field List > Parameters": {
            "n_components": "int, or None (default=None)\nNumber of components to keep. If None, then it will run select_dimension() to find the optimal embedding dimension.",
            "n_elbows": "int, or None (default=2)\nIf n_components is None, then compute the optimal embedding dimension using select_dimension(). Otherwise, ignored.",
            "dissimilarity": "{'euclidean', 'precomputed'}, optional, default: 'euclidean'\nDissimilarity measure to use:\n\n'euclidean'\nPairwise Euclidean distances between points in the dataset.\n\n'precomputed'\nPre-computed dissimilarities are passed directly to fit() and fit_transform()."
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Attributes": {
            "n_components": "int\nEquals the parameter n_components. If input n_components was None, then equals the optimal embedding dimension.",
            "n_features_in_": "int\nNumber of features passed to the fit() method.",
            "components_": "array, shape (n_components, n_features)\nPrincipal axes in feature space.",
            "singular_values_": "array, shape (n_components,)\nThe singular values corresponding to each of the selected components.",
            "dissimilarity_matrix_": "array, shape (n_features, n_features)\nDissimilarity matrix",
            "svd_seed_": "int or None (default None)\nOnly applicable for n_components!=1; allows you to seed the randomized svd solver for deterministic, albeit pseudo-randomized behavior."
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "n_components": "int | None",
                "n_elbows": "int | None",
                "dissimilarity": "typing_extensions.Literal[euclidean, precomputed]",
                "svd_seed": "int | None"
            }
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit the model with X.",
            "Parameters": {
                "X": "array_like\nIf dissimilarity=='precomputed', the input should be the dissimilarity matrix with shape (n_samples, n_samples). If dissimilarity=='euclidean', then the input should be 2d-array with shape (n_samples, n_features) or a 3d-array with shape (n_samples, n_features_1, n_features_2)."
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "Fit the data from X, and returns the embedded coordinates.",
            "Parameters": {
                "X": "nd-array\nIf dissimilarity=='precomputed', the input should be the dissimilarity matrix with shape (n_samples, n_samples). If dissimilarity=='euclidean', then the input should be array with shape (n_samples, n_features) or a nd-array with shape (n_samples, n_features_1, n_features_2, ..., n_features_d). First axis of nd-array must be n_samples."
            },
            "Returns": {
                "X_new": "array-like, shape (n_samples, n_components)\nEmbedded input."
            }
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Rubric": {
            "References": [
                "Wickelmaier, Florian. \"An introduction to MDS.\" Sound Quality Research Unit, Aalborg University, Denmark 46.5 (2003)."
            ]
        },
        "Section_id": "ClassicalMDS"
    },
    {
        "Section_id": "density_test",
        "Description": "Compares two networks by testing whether the global connection probabilities (densites) for the two networks are equal under an Erdos-Renyi model assumption."
    },
    {
        "Field List > Parameters": {
            "A1": "np.array, int\nThe adjacency matrix for network 1. Will be treated as a binary network, regardless of whether it was weighted.",
            "A2": "np.array, int\nAdjacency matrix for network 2. Will be treated as a binary network, regardless of whether it was weighted.",
            "method": "string, optional, default=\"fisher\"\nSpecifies the statistical test to be used. The default option is \"fisher\", which uses Fisher's exact test, but the user may also enter \"chi2\" to use a chi-squared test."
        },
        "Section_id": "density_test"
    },
    {
        "Field List > Returns": {
            "DensityTestResult": "namedtuple\nThis named tuple returns the following data:\n\nstat: float\nThe statistic for the test specified by method.\npvalue: float\nThe p-value for the test specified by method.\nmisc: dict\nDictionary containing a number of computed statistics for the network comparison performed:\n\n\"probability1\", float\nThe probability of an edge (density) in network 1 (\").\n\n\"probability2\", float\nThe probability of an edge (density) in network 2 (\").\n\n\"observed1\", pd.DataFrame\nThe total number of edge connections for network 1.\n\n\"observed2\", pd.DataFrame\nThe total number of edge connections for network 2.\n\n\"possible1\", pd.DataFrame\nThe total number of possible edges for network 1.\n\n\"possible2\", pd.DataFrame\nThe total number of possible edges for network 1."
        },
        "Section_id": "density_test"
    },
    {
        "Rubric": {
            "Note": "Under the Erdos-Renyi model, edges are generated independently with probability is also known as the network density. This function tests whether the probability of an edge in network 1 is significantly different from that in network 2, by assuming that both networks came from an Erdos-Renyi model. In other words, the null hypothesis is\n\nAnd the alternative hypothesis is\n\nThis test makes several assumptions about the data and test (which could easily be loosened in future versions):\n\nWe assume that the networks are directed. If the networks are undirected (and the adjacency matrices are thus symmetric), then edges would be counted twice, which would lead to an incorrect calculation of the edge probability. We believe passing in the upper or lower triangle of the adjacency matrix would solve this, but this has not been tested.\n\nWe assume that the networks are loopless, that is we do not consider the probability of an edge existing between a node and itself. This can be weakened and made an option in future versions.\n\nWe only implement the alternative hypothesis of \"not equals\" (two-sided); future versions could implement the one-sided alternative hypotheses.",
            "References": [
                "Pedigo, B.D., Powell, M., Bridgeford, E.W., Winding, M., Priebe, C.E., Vogelstein, J.T. \"Generative network modeling reveals quantitative definitions of bilateral symmetry exhibited by a whole insect brain connectome,\" eLife (2023): e83739."
            ]
        },
        "Section_id": "density_test"
    },
    {
        "Section_id": "group_connection_test",
        "Description": "Compares two networks by testing whether edge probabilities between groups are significantly different for the two networks under a stochastic block model assumption."
    },
    {
        "Field List > Parameters": {
            "A1": "np.array, shape(n1,n1)\nThe adjacency matrix for network 1. Will be treated as a binary network, regardless of whether it was weighted.",
            "A2": "np.array, shape(n2,n2)\nThe adjacency matrix for network 2. Will be treated as a binary network, regardless of whether it was weighted.",
            "labels1": "array-like, shape (n1,)\nThe group labels for each node in network 1.",
            "labels2": "array-like, shape (n2,)\nThe group labels for each node in network 2.",
            "density_adjustment": "boolean, optional\nWhether to perform a density adjustment procedure. If True, will test the null hypothesis that the group-to-group connection probabilities of one network are a constant multiple of those of the other network. Otherwise, no density adjustment will be performed.",
            "method": "str, optional\nSpecifies the statistical test to be performed to compare each of the group-to-group connection probabilities. By default, this performs the score test (essentially equivalent to chi-squared test when density_adjustment=False), but the user may also enter \"chi2\" to perform the chi-squared test, or \"fisher\" for Fisher's exact test.",
            "combine_method": "str, optional\nSpecifies the method for combining p-values (see Notes and [1] for more details). Default is \"tippett\" for Tippett's method (recommended), but the user can also enter any other method supported by scipy.stats.combine_pvalues().",
            "correct_method": "str, optional\nSpecifies the method for correcting for multiple comparisons. Default value is \"holm\" to use the Holm-Bonferroni correction method, but many others are possible (see statsmodels.stats.multitest.multipletests() for more details and options).",
            "alpha": "float, optional\nThe significance threshold. By default, this is the conventional value of 0.05 but any value on the interval [0,1] can be entered. This only affects the results in misc['rejections']."
        },
        "Section_id": "group_connection_test"
    },
    {
        "Field List > Returns": {
            "GroupTestResult": "namedtuple\nA tuple containing the following data:\n\nstat: float\nThe statistic computed by the method chosen for combining p-values (see combine_method).\npvalue: float\nThe p-value for the overall network-to-network comparison using under a stochastic block model assumption. Note that this is the p-value for the comparison of the entire group-to-group connection matrices (i.e., B1 and B2).\nmisc: dict\nA dictionary containing a number of statistics relating to the individual group-to-group connection comparisons.\n\n\"uncorrected_pvalues\", pd.DataFrame\nThe p-values for each group-to-group connection comparison, before correction for multiple comparisons.\n\n\"stats\", pd.DataFrame\nThe test statistics for each of the group-to-group comparisons, depending on method.\n\n\"probabilities1\", pd.DataFrame\nThis contains the B_hat values computed in fit_sbm above for network 1, i.e. the hypothesized group connection density for each group-to-group connection for network 1.\n\n\"probabilities2\", pd.DataFrame\nSame as above, but for network 2.\n\n\"observed1\", pd.DataFrame\nThe total number of observed group-to-group edge connections for network 1.\n\n\"observed2\", pd.DataFrame\nSame as above, but for network 2.\n\n\"possible1\", pd.DataFrame\nThe total number of possible edges for each group-to-group pair in network 1.\n\n\"possible2\", pd.DataFrame\nSame as above, but for network 2.\n\n\"group_counts1\", pd.Series\nContains total number of nodes corresponding to each group label for network 1.\n\n\"group_counts2\", pd.Series\nSame as above, for network 2\n\n\"null_ratio\", float\nIf the \"density adjustment\" parameter is set to \"true\", this variable contains the null hypothesis for the quotient of odds ratios for the group-to-group connection densities for the two networks. In other words, it contains the hypothesized factor by which network 1 is \"more dense\" or \"less dense\" than network 2. If \"density adjustment\" is set to \"false\", this simply returns a value of 1.0.\n\n\"n_tests\", int\nThis variable contains the number of group-to-group comparisons performed by the function.\n\n\"rejections\", pd.DataFrame\nContains a square matrix of boolean variables. The side length of the matrix is equal to the number of distinct group labels. An entry in the matrix is \"true\" if the null hypothesis, i.e. that the group-to-group connection density corresponding to the row and column of the matrix is equal for both networks (with or without a density adjustment factor), is rejected. In simpler terms, an entry is only \"true\" if the group-to-group density is statistically different between the two networks for the connection from the group corresponding to the row of the matrix to the"
        },
        "Section_id": "group_connection_test"
    },
    {
        "Rubric": {
            "Note": "This function requires the group labels in both networks to be known and to have the same categories; although the exact number of nodes belonging to each group does not need to be identical. Note that using group labels inferred from the data may yield an invalid test.\n\nThis function also permits the user to test whether one network's group connection probabilities are a constant multiple of the other's (see density_adjustment parameter).",
            "References": [
                "Pedigo, B.D., Powell, M., Bridgeford, E.W., Winding, M., Priebe, C.E., Vogelstein, J.T. \"Generative network modeling reveals quantitative definitions of bilateral symmetry exhibited by a whole insect brain connectome,\" eLife (2023): e83739."
            ]
        },
        "Section_id": "group_connection_test"
    },
    {
        "Section_id": "latent_position_test",
        "Description": "Two-sample hypothesis test for the problem of determining whether two random dot product graphs have the same latent positions.\n\nThis test assumes that the two input graphs are vertex aligned, that is, there is a known mapping between vertices in the two graphs and the input graphs have their vertices sorted in the same order. Currently, the function only supports undirected graphs. \n\nRead more in the Latent Position Two-Graph Testing Tutorial"
    },
    {
        "Field List > Parameters": {
            "A1": "ndarray | csr_array | Graph\nThe first graph to run a hypothesis test on.",
            "A2": "ndarray | csr_array | Graph\nThe second graph to run a hypothesis test on.",
            "embedding": "typing_extensions.Literal[ase, omnibus]\nString describing the embedding method to use:\n\n'ase'\nEmbed each graph separately using adjacency spectral embedding and use Procrustes to align the embeddings.\n\n'omnibus'\nEmbed all graphs simultaneously using omnibus embedding.",
            "n_components": "int | None\nNumber of embedding dimensions. If None, the optimal embedding dimensions are found by the Zhu and Godsi algorithm.",
            "test_case": "typing_extensions.Literal[rotation, scalar-rotation, diagonal-rotation]\ndescribes the exact form of the hypothesis to test when using 'ase' or 'lse' as an embedding method. Ignored if using 'omnibus'. Given two latent positions, \n and \n, and an orthogonal rotation matrix \n that minimizes \n:\n\n'rotation'\n'scalar-rotation'\nwhere \n is a scalar, \n\n'diagonal-rotation'\nwhere \n is an arbitrary diagonal matrix",
            "n_bootstraps": "int\nNumber of bootstrap simulations to run to generate the null distribution",
            "workers": "int\nNumber of workers to use. If more than 1, parallelizes the bootstrap simulations. Supply -1 to use all cores available."
        },
        "Section_id": "latent_position_test"
    },
    {
        "Field List > Returns": {
            "stat": "float\nThe observed difference between the embedded positions of the two input graphs after an alignment (the type of alignment depends on test_case)",
            "pvalue": "float\nThe overall p value from the test; this is the max of 'p_value_1' and 'p_value_2'",
            "misc_dict": "dictionary\nA collection of other statistics obtained from the latent position test\n\n'p_value_1', 'p_value_2'float\nThe p value estimate from the null distributions from sample 1 and sample 2\n\n'null_distribution_1', 'null_distribution_2'np.ndarray (n_bootstraps,)\nThe distribution of T statistics generated under the null, using the first and and second input graph, respectively. The latent positions of each sample graph are used independently to sample random dot product graphs, so two null distributions are generated"
        },
        "Section_id": "latent_position_test"
    },
    {
        "Rubric": {
            "References": [
                "Tang, M., A. Athreya, D. Sussman, V. Lyzinski, Y. Park, Priebe, C.E. \"A Semiparametric Two-Sample Hypothesis Testing Problem for Random Graphs\" Journal of Computational and Graphical Statistics, Vol. 26(2), 2017"
            ]
        },
        "Section_id": "latent_position_test"
    },
    {
        "Section_id": "latent_distribution_test",
        "Description": "Two-sample hypothesis test for the problem of determining whether two random dot product graphs have the same distributions of latent positions.\n\nThis test can operate on two graphs where there is no known matching between the vertices of the two graphs, or even when the number of vertices is different. Currently, testing is only supported for undirected graphs.\n\nRead more in the Latent Distribution Two-Graph Testing Tutorial"
    },
    {
        "Field List > Parameters": {
            "A1": "variable (see description of 'input_graph')\nThe two graphs, or their embeddings to run a hypothesis test on. Expected variable type and shape depends on input_graph attribute",
            "A2": "variable (see description of 'input_graph')\nThe two graphs, or their embeddings to run a hypothesis test on. Expected variable type and shape depends on input_graph attribute",
            "test": "str (default=\"dcorr\")\nBackend hypothesis test to use, one of [\"cca\", \"dcorr\", \"hhg\", \"rv\", \"hsic\", \"mgc\"]. These tests are typically used for independence testing, but here they are used for a two-sample hypothesis test on the latent positions of two graphs. See hyppo.ksample.KSample for more information.",
            "metric": "str or function (default=\"euclidean\")\nDistance or a kernel metric to use, either a callable or a valid string. Kernel metrics (e.g. \"gaussian\") must be used with kernel-based HSIC test and distances (e.g. \"euclidean\") with all other tests. If a callable, then it should behave similarly to either sklearn.metrics.pairwise_distances() or to sklearn.metrics.pairwise.pairwise_kernels().\n\nValid strings for distance metric are, as defined in sklearn.metrics.pairwise_distances(),\n\nFrom scikit-learn: [\"euclidean\", \"cityblock\", \"cosine\", \"l1\", \"l2\", \"manhattan\"].\n\nFrom scipy.spatial.distance: [\"braycurtis\", \"canberra\", \"chebyshev\", \"correlation\", \"dice\", \"hamming\", \"jaccard\", \"kulsinski\", \"mahalanobis\", \"minkowski\", \"rogerstanimoto\", \"russellrao\", \"seuclidean\", \"sokalmichener\", \"sokalsneath\", \"sqeuclidean\", \"yule\"] See the documentation for scipy.spatial.distance for details on these metrics.\n\nValid strings for kernel metric are, as defined in sklearn.metrics.pairwise.pairwise_kernels(),\n\n[\"additive_chi2\", \"chi2\", \"linear\", \"poly\", \"polynomial\", \"rbf\", \"laplacian\", \"sigmoid\", \"cosine\"]\n\nNote \"rbf\" and \"gaussian\" are the same metric, which will use an adaptively selected bandwidth.",
            "n_components": "int or None (default=None)\nNumber of embedding dimensions. If None, the optimal embedding dimensions are found by the Zhu and Godsi algorithm. See select_svd() for more information. This argument is ignored if input_graph is False.",
            "n_bootstraps": "int (default=200)\nNumber of bootstrap iterations for the backend hypothesis test. See hyppo.ksample.KSample for more information.",
            "random_state": "{None, int, ~np.random.RandomState, ~np.random.Generator}\nThis parameter defines the object to use for drawing random variates. If random_state is None the ~np.random.RandomState singleton is used. If random_state is an int, a new RandomState instance is used, seeded with random_state. If random_state is already a RandomState or Generator instance, then that object is used. Default is None.",
            "workers": "int or None (default=None)\nNumber of workers to use. If more than 1, parallelizes the code. Supply -1 to use all cores available. None is a marker for 'unset' that will be interpreted as workers=1 (sequential execution) unless the call is performed under a Joblib parallel_backend context manager that sets another value for workers. See :class:joblib.Parallel for more details.",
            "size_correction": "bool (default=True)\nIgnored when the two graphs have the same number of vertices. The test degrades in validity as the number of vertices of the two graphs diverge from each other, unless a correction is performed.\n\nTrue\nWhenever the two graphs have different numbers of vertices, estimates the plug-in estimator for the variance and uses it to correct the embedding of the larger graph.\n\nFalse\nDoes not perform any modifications (not recommended).",
            "pooled": "bool (default=False)\nIgnored whenever the two graphs have the same number of vertices or size_correction is set to False. In order to correct the adjacency spectral embedding used in the test, it is needed to estimate the variance for each of the latent position estimates in the larger graph, which requires to compute different sample moments. These moments can be computed either over the larger graph (False), or over both graphs (True). Setting it to True should not affect the behavior of the test under the null hypothesis, but it is not clear whether it has more power or less power under which alternatives. Generally not recomended, as it is untested and included for experimental purposes.",
            "align_type": "str, {'sign_flips' (default), 'seedless_procrustes'} or None\nRandom dot product graphs have an inherent non-identifiability, associated with their latent positions. Thus, two embeddings of different graphs may not be orthogonally aligned. Without this accounted for, two embeddings of different graphs may appear",
            "align_kws": "dict\nKeyword arguments for the aligner of choice, either graspologic.align.SignFlips or graspologic.align.SeedlessProcrustes, depending on the align_type. See respective classes for more information.",
            "input_graph": "bool (default=True)\nFlag whether to expect two full graphs, or the embeddings.\n\nTrue\nThis function expects graphs, either as NetworkX graph objects or as adjacency matrices, provided as ndarrays of size (n, n) and (m, m). They will be embedded using adjacency spectral embeddings.\n\nFalse\nThis function expects adjacency spectral embeddings of the graphs, they must be ndarrays of size (n, d) and (m, d), where d must be same. n_components attribute is ignored in this case."
        },
        "Section_id": "latent_distribution_test"
    },
    {
        "Rubric": {
            "References": [
                "Tang, M., Athreya, A., Sussman, D. L., Lyzinski, V., & Priebe, C. E. (2017). \"A nonparametric two-sample hypothesis testing problem for random graphs.\" Bernoulli, 23(3), 1599-1630.",
                "Panda, S., Palaniappan, S., Xiong, J., Bridgeford, E., Mehta, R., Shen, C., & Vogelstein, J. (2019). \"hyppo: A Comprehensive Multivariate Hypothesis Testing Python Package.\" arXiv:1907.02088.",
                "Alyakin, A. A., Agterberg, J., Helm, H. S., Priebe, C. E. (2020). \"Correcting a Nonparametric Two-sample Graph Hypothesis Test for Graphs with Different Numbers of Vertices\" arXiv:2008.09434"
            ]
        },
        "Section_id": "latent_distribution_test"
    },
    {
        "Section_id": "NodePosition",
        "Description": "Contains the node id, 2d coordinates, size, and community id for a node.\n\nCreate new instance of NodePosition(node_id, x, y, size, community)"
    },
    {
        "Field List > Parameters": {
            "node_id": "str",
            "x": "float",
            "y": "float",
            "size": "float",
            "community": "int"
        },
        "Section_id": "NodePosition"
    },
    {
        "Field List > Methods > static__new__": {
            "Description": "Create new instance of NodePosition(node_id, x, y, size, community)",
            "Parameters": {
                "node_id": "str",
                "x": "float",
                "y": "float",
                "size": "float",
                "community": "int"
            }
        },
        "Section_id": "NodePosition"
    },
    {
        "Field List > Methods > count": {
            "Description": "Return number of occurrences of value."
        },
        "Section_id": "NodePosition"
    },
    {
        "Field List > Methods > index": {
            "Description": "Return first index of value."
        },
        "Section_id": "NodePosition"
    },
    {
        "Section_id": "layout_tsne",
        "Description": "Automatic graph layout generation by creating a generalized node2vec embedding, then using t-SNE for dimensionality reduction to 2d space.\n\nBy default, this function automatically attempts to prune each graph to a maximum of 10,000,000 edges by removing the lowest weight edges. This pruning is approximate and will leave your graph with at most max_edges, but is not guaranteed to be precisely max_edges.\n\nIn addition to pruning edges by weight, this function also only operates over the largest connected component in the graph.\n\nAfter dimensionality reduction, sizes are generated for each node based upon their degree centrality, and these sizes and positions are further refined by an overlap removal phase. Lastly, a global partitioning algorithm (graspologic.partition.leiden()) is executed for the largest connected component and the partition ID is included with each node position."
    },
    {
        "Field List > Parameters": {
            "graph": "networkx.Graph\nThe graph to generate a layout for. This graph may have edges pruned if the count is too high and only the largest connected component will be used to automatically generate a layout.",
            "perplexity": "int\nThe perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 4 and 100. Different values can result in significantly different results.",
            "n_iter": "int\nMaximum number of iterations for the optimization. We have found in practice that larger graphs require more iterations. We hope to eventually have more guidance on the number of iterations based on the size of the graph and the density of the edge connections.",
            "max_edges": "int\nThe maximum number of edges to use when generating the embedding. Default is 10000000. The edges with the lowest weights will be pruned until at most max_edges exist. Warning: this pruning is approximate and more edges than are necessary may be pruned. Running in 32 bit environment you will most likely need to reduce this number or you will out of memory.",
            "weight_attribute": "str\nThe edge dictionary data attribute that holds the weight. Default is weight. Note that the graph must be fully weighted or unweighted.",
            "random_seed": "int | None\nSeed to be used for reproducible results. Default is None and will produce a new random state. Specifying a random state will provide consistent results between runs. In addition the environment variable PYTHONHASHSEED must be set to control hash randomization.",
            "adjust_overlaps": "bool\nMake room for overlapping nodes while maintaining some semblance of the 2d spatial characteristics of each node. Default is True"
        },
        "Section_id": "layout_tsne"
    },
    {
        "Field List > Returns": {
            "Tuple[nx.Graph, List[NodePosition]]": "The largest connected component and a list of NodePositions for each node in the largest connected component. The NodePosition object contains: - node_id - x coordinate - y coordinate - size - community"
        },
        "Section_id": "layout_tsne"
    },
    {
        "Rubric": {
            "References": [
                "van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008."
            ]
        },
        "Section_id": "layout_tsne"
    },
    {
        "Section_id": "layout_umap",
        "Description": "Automatic graph layout generation by creating a generalized node2vec embedding, then using UMAP for dimensionality reduction to 2d space.\n\nBy default, this function automatically attempts to prune each graph to a maximum of 10,000,000 edges by removing the lowest weight edges. This pruning is approximate and will leave your graph with at most max_edges, but is not guaranteed to be precisely max_edges.\n\nIn addition to pruning edges by weight, this function also only operates over the largest connected component in the graph.\n\nAfter dimensionality reduction, sizes are generated for each node based upon their degree centrality, and these sizes and positions are further refined by an overlap removal phase. Lastly, a global partitioning algorithm (graspologic.partition.leiden()) is executed for the largest connected component and the partition ID is included with each node position."
    },
    {
        "Field List > Parameters": {
            "graph": "Graph\nThe graph to generate a layout for. This graph may have edges pruned if the count is too high and only the largest connected component will be used to automatically generate a layout.",
            "min_dist": "float\nThe effective minimum distance between embedded points. Default is 0.75. Smaller values will result in a more clustered/clumped embedding where nearby points on the manifold are drawn closer together, while larger values will result on a more even dispersal of points. The value should be set relative to the spread value, which determines the scale at which embedded points will be spread out.",
            "n_neighbors": "int\nThe size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation. Default is 25. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved.",
            "max_edges": "int\nThe maximum number of edges to use when generating the embedding. Default is 10000000. The edges with the lowest weights will be pruned until at most max_edges exist. Warning: this pruning is approximate and more edges than are necessary may be pruned. Running in 32 bit environment you will most likely need to reduce this number or you will out of memory.",
            "weight_attribute": "str\nThe edge dictionary data attribute that holds the weight. Default is weight. Note that the graph must be fully weighted or unweighted.",
            "random_seed": "int | None\nSeed to be used for reproducible results. Default is None and will produce random results.",
            "adjust_overlaps": "bool\nMake room for overlapping nodes while maintaining some semblance of the 2d spatial characteristics of each node. Default is True"
        },
        "Section_id": "layout_umap"
    },
    {
        "Field List > Returns": {
            "Tuple[nx.Graph, List[NodePosition]]": "The largest connected component and a list of NodePositions for each node in the largest connected component. The NodePosition object contains: - node_id - x coordinate - y coordinate - size - community"
        },
        "Section_id": "layout_umap"
    },
    {
        "Rubric": {
            "References": [
                "McInnes, L, Healy, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, ArXiv e-prints 1802.03426, 2018",
                "B\u00f6hm, Jan Niklas; Berens, Philipp; Kobak, Dmitry. A Unifying Perspective on Neighbor Embeddings along the Attraction-Repulsion Spectrum. ArXiv e-prints 2007.08902v1, 17 Jul 2020."
            ]
        },
        "Section_id": "layout_umap"
    },
    {
        "Section_id": "categorical_colors",
        "Description": "Generates a node -> color mapping based on the partitions provided.\n\nThe partitions are ordered by population descending, and a series of perceptually balanced, complementary colors are chosen in sequence.\n\nIf a theme_path is provided, it must contain a path to a json file generated by Thematic, otherwise it will use the theme packaged with this library.\n\nColors will be different when selecting for a light background vs. a dark background, using the principles defined by Thematic.\n\nIf more partitions than colors available (100) are selected, the colors will be cycled through again."
    },
    {
        "Field List > Parameters": {
            "partitions": "Dict[Any, int]\nA dictionary of node ids to partition ids.",
            "light_background": "bool\nDefault is True. Colors selected for a light background will be slightly different in hue and saturation to complement a light or dark background.",
            "theme_path": "Optional[str]\nA color scheme is provided with graspologic, but if you wish to use your own you can generate one with Thematic and provide the path to it to override the bundled theme."
        },
        "Section_id": "categorical_colors"
    },
    {
        "Field List > Returns": {
            "Dict[Any, str]": "Returns a dictionary of node id -> color based on the partitions provided."
        },
        "Section_id": "categorical_colors"
    },
    {
        "Section_id": "sequential_colors",
        "Description": "Generates a node -> color mapping where a color is chosen for the value as it maps the value range into the sequential color space.\n\nIf a theme_path is provided, it must contain a path to a json file generated by Thematic, otherwise it will use the theme packaged with this library.\n\nColors will be different when selecting for a light background vs. a dark background, using the principles defined by Thematic.\n\nIf more partitions than colors available (100) are selected, the colors will be cycled through again."
    },
    {
        "Field List > Parameters": {
            "node_and_value": "Dict[Any, float]\nA node to value mapping. The value is a single entry in a continuous range, which is then mapped into the sequential color space.",
            "light_background": "bool\nDefault is True. Colors selected for a light background will be slightly different in hue and saturation to complement a light or dark background.",
            "use_log_scale": "bool\nDefault is False.",
            "theme_path": "Optional[str]\nA color scheme is provided with graspologic, but if you wish to use your own you can generate one with Thematic and provide the path to it to override the bundled theme."
        },
        "Section_id": "sequential_colors"
    },
    {
        "Field List > Returns": {
            "Dict[Any, str]": "Returns a dictionary of node id -> color based on the original value provided for the node as it relates to the total range of all values."
        },
        "Section_id": "sequential_colors"
    },
    {
        "Section_id": "save_graph",
        "Description": "Renders a graph to file.\n\nEdges will be displayed with the same color as the source node."
    },
    {
        "Field List > Parameters": {
            "output_path": "str\nThe output path to write the rendered graph to. Suggested file extension is .png.",
            "graph": "nx.Graph\nThe graph to be displayed. If the networkx Graph contains only nodes, no edges will be displayed.",
            "positions": "List[graspologic.layouts.NodePosition]\nThe positionsfor every node in the graph.",
            "node_colors": "Dict[Any, str]\nA mapping of node id to colors. Must contain an entry for every node in the graph.",
            "vertex_line_width": "float\nLine width of vertex outline. Default is 0.01.",
            "vertex_alpha": "float\nAlpha (transparency) of vertices in visualization. Default is 0.55.",
            "edge_line_width": "float\nLine width of edge. Default is 0.5.",
            "edge_alpha": "float\nAlpha (transparency) of edges in visualization. Default is 0.02.",
            "figure_width": "float\nWidth of figure. Default is 15.0.",
            "figure_height": "float\nHeight of figure. Default is 15.0.",
            "light_background": "bool\nLight background or dark background. Default is True.",
            "vertex_shape": "str\nMatplotlib Marker for the vertex shape. See https://matplotlib.org/api/markers_api.html for a list of allowed values . Default is o (i.e: a circle)",
            "arrows": "bool\nFor directed graphs, if True, draw arrow heads. Default is False",
            "dpi": "int\nDots per inch of the figure. Default is 100."
        },
        "Section_id": "save_graph"
    },
    {
        "Field List > Returns": {
            "None": "None"
        },
        "Section_id": "save_graph"
    },
    {
        "Section_id": "show_graph",
        "Description": "Renders and displays a graph.\n\nAttempts to display it via the platform-specific display library such as TkInter\n\nEdges will be displayed with the same color as the source node."
    },
    {
        "Field List > Parameters": {
            "graph": "nx.Graph\nThe graph to be displayed. If the networkx Graph contains only nodes, no edges will be displayed.",
            "positions": "List[graspologic.layouts.NodePosition]\nThe positionsfor every node in the graph.",
            "node_colors": "Dict[Any, str]\nA mapping of node id to colors. Must contain an entry for every node in the graph.",
            "vertex_line_width": "float\nLine width of vertex outline. Default is 0.01.",
            "vertex_alpha": "float\nAlpha (transparency) of vertices in visualization. Default is 0.55.",
            "edge_line_width": "float\nLine width of edge. Default is 0.5.",
            "edge_alpha": "float\nAlpha (transparency) of edges in visualization. Default is 0.02.",
            "figure_width": "float\nWidth of figure. Default is 15.0.",
            "figure_height": "float\nHeight of figure. Default is 15.0.",
            "light_background": "bool\nLight background or dark background. Default is True.",
            "vertex_shape": "str\nMatplotlib Marker for the vertex shape. See https://matplotlib.org/api/markers_api.html for a list of allowed values . Default is o (i.e: a circle)",
            "arrows": "bool\nFor directed graphs, if True, draw arrow heads. Default is False",
            "dpi": "int\nDots per inch of the figure. Default is 500."
        },
        "Section_id": "show_graph"
    },
    {
        "Field List > Returns": {
            "None": "None"
        },
        "Section_id": "show_graph"
    },
    {
        "Section_id": "graph_match",
        "Description": "Attempts to solve the Graph Matching Problem or the Quadratic Assignment Problem (QAP) through an implementation of the Fast Approximate QAP (FAQ) Algorithm [1].\n\nThis algorithm can be thought of as finding an alignment of the vertices of two graphs which minimizes the number of induced edge disagreements, or, in the case of weighted graphs, the sum of squared differences of edge weight disagreements. Various extensions to the original FAQ algorithm are also included in this function ([2-5])."
    },
    {
        "Field List > Parameters": {
            "A": "ndarray, csr_array, csr_array of shape (n, n), or a list thereof\nThe first (potentially multilayer) adjacency matrix to be matched. Multiplex networks (e.g. a network with multiple edge types) can be used by inputting a list of the adjacency matrices for each edge type.",
            "B": "ndarray, csr_array, csr_array of shape (m, m), or a list thereof\nThe second (potentially multilayer) adjacency matrix to be matched. Must have the same number of layers as A, but need not have the same size (see padding).",
            "AB": "ndarray, csr_array, csr_array of shape (n, m), or a list thereof, default=None\nA (potentially multilayer) matrix representing connections from the objects indexed in A to those in B, used for bisected graph matching (see [2]).",
            "BA": "ndarray, csr_array, csr_array of shape (m, n), or a list thereof, default=None\nA (potentially multilayer) matrix representing connections from the objects indexed in B to those in A, used for bisected graph matching (see [2]).",
            "S": "ndarray, csr_array, csr_array of shape (n, m), default=None\nA matrix representing the similarity of objects indexed in A to each object indexed in B. Note that the scale (i.e. the norm) of this matrix will affect how strongly the similarity (linear) term is weighted relative to the adjacency (quadratic) terms.",
            "partial_match": "ndarray of shape (n_matches, 2), dtype=int, or tuple of two array-likes of shape (n_matches,), default=None\nIndices specifying known matches to include in the optimization. The first column represents indices of the objects in A, and the second column represents their corresponding matches in B.",
            "init": "ndarray of shape (n_unseed, n_unseed), default=None\nInitialization for the algorithm. Setting to None specifies the \"barycenter\", which is the most commonly used initialization and represents an uninformative (flat) initialization. If a ndarray, then this matrix must be square and have size equal to the number of unseeded (not already matched in partial_match) nodes.",
            "init_perturbation": "float, default=0.0\nWeight of the random perturbation from init that the initialization will undergo. Must be between 0 and 1.",
            "n_init": "int, default=1\nNumber of initializations/runs of the algorithm to repeat. The solution with the best objective function value over all initializations is kept. Increasing n_init can improve performance but will take longer.",
            "shuffle_input": "bool, default=True\nWhether to shuffle the order of the inputs internally during optimization. This option is recommended to be kept to True besides for testing purposes; it alleviates a dependence of the solution on the (arbitrary) ordering of the input rows/columns.",
            "maximize": "bool, default=True\nWhether to maximize the objective function (graph matching problem) or minimize it (quadratic assignment problem). maximize=True corresponds to trying to find a permutation wherein the input matrices are as similar as possible - for adjacency matrices, this corresponds to maximizing the overlap of the edges of the two networks. Conversely, maximize=False would attempt to make this overlap as small as possible.",
            "padding": "{'naive', 'adopted'}, default='naive'\nSpecification of a padding scheme if A and B are not of equal size. See the padded graph matching tutorial or [3] for more explanation. Adopted padding has not been tested for weighted networks; use with caution.",
            "n_jobs": "int, default=None\nThe number of jobs to run in parallel. Parallelization is over the initializations, so only relevant when n_init > 1. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See joblib.Parallel for more details.",
            "max_iter": "int, default=30\nMust be 1 or greater, specifying the max number of iterations for the algorithm. Setting this value higher may provide more precise solutions at the cost of longer computation time.",
            "tol": "float, default=0.01\nStopping tolerance for the FAQ algorithm. Setting this value smaller may provide more precise solutions at the cost of longer computation time.",
            "verbose": "int, default=0\nA positive number specifying the level of verbosity for status updates in the algorithm's progress. If n_jobs > 1, then this parameter behaves as the verbose parameter for joblib.Parallel.",
            "rng": "int, default=None\nThis parameter defines the object to use for drawing random variates. If rng is None the ~np.random.RandomState singleton is used. If rng is an int, a new RandomState instance is used, seeded with rng. If rng is already a RandomState or Generator instance, then that object is used. Default is None.",
            "transport": "bool, default=False\nWhether to use the transportation problem formulation of the QAP. If True, the transport_regularizer and transport_tol parameters must be set.",
            "transport_regularizer": "float, default=100\nRegularization parameter for the transportation problem. Larger values will result in a closer approximation to the true QAP, but will also be more computationally expensive.",
            "transport_tol": "float, default=0.05\nTolerance for the transportation problem. Smaller values will result in a closer approximation to the true QAP, but will also be more computationally expensive.",
            "transport_max_iter": "int, default=1000\nMaximum number of iterations for the transportation problem. Setting this value higher may provide more precise solutions at the cost of longer computation time.",
            "fast": "bool, default=True\nWhether to use the fast version of the FAQ algorithm. If False, the slower version will be used."
        },
        "Section_id": "graph_match"
    },
    {
        "Field List > Returns": {
            "res": "MatchResult\nMatchResult containing the following fields.\n\nindices_A: ndarray\nSorted indices in A which were matched.\n\nindices_B: ndarray\nIndices in B which were matched. Element indices_B[i] was matched to element indices_A[i]. indices_B can also be thought of as a permutation of the nodes of B with respect to A.\n\nscore: float\nObjective function value at the end of optimization.\n\nmisc: list of dict\nList of length n_init containing information about each run. Fields for each run are score, n_iter, convex_solution, and converged."
        },
        "Section_id": "graph_match"
    },
    {
        "Rubric": {
            "Note": "Many extensions [2-5] to the original FAQ algorithm are included in this function. The full objective function which this function aims to solve can be written as\n\nwhere \n is a permutation matrix we are trying to learn, \n is the adjacency matrix in network \n for the \n-th edge type (and likewise for B), \n (with a slight abuse of notation, but for consistency with the code) is an adjacency matrix representing a subgraph of any connections which go from objects in \n to those in \n (and defined likewise for \n), and \n is a similarity matrix indexing the similarity of each object in \n to each object in \n.\n\nIf partial_match is used, then the above will be maximized/minimized over the set of permutations which respect this partial matching of the two networks.\n\nIf maximize, this function will attempt to maximize \n (solve the graph matching problem); otherwise, it will be minimized.",
            "References": [
                "J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik, S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and C.E. Priebe, \u201cFast approximate quadratic programming for graph matching,\u201d PLOS one, vol. 10, no. 4, p. e0121002, 2015.",
                "B.D. Pedigo, M. Winding, C.E. Priebe, J.T. Vogelstein, \"Bisected graph matching improves automated pairing of bilaterally homologous neurons from connectomes,\" bioRxiv 2022.05.19.492713 (2022)",
                "D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski, C. Priebe, \"Seeded graph matching,\" Pattern Recognit. 87 (2019) 203\u2013215",
                "A. Saad-Eldin, B.D. Pedigo, C.E. Priebe, J.T. Vogelstein \"Graph Matching via Optimal Transport,\" arXiv 2111.05366 (2021)",
                "K. Pantazis, D.L. Sussman, Y. Park, Z. Li, C.E. Priebe, V. Lyzinski, \"Multiplex graph matching matched filters,\" Applied Network Science (2022)"
            ]
        },
        "Section_id": "graph_match"
    },
    {
        "Section_id": "EREstimator",
        "Description": "Erdos-Reyni Model\n\nThe Erdos-Reyni (ER) model is a simple random graph model in which the probability of any potential edge in the graph existing is the same for any two nodes i and j.\n\n for all i, j\n\nRead more in the Erdos-Renyi (ER) Model Tutorial"
    },
    {
        "Field List > Parameters": {
            "directed": "boolean, optional (default=True)\nWhether to treat the input graph as directed. Even if a directed graph is input, this determines whether to force symmetry upon the block probability matrix fit for the SBM. It will also determine whether graphs sampled from the model are directed.",
            "loops": "boolean, optional (default=False)\nWhether to allow entries on the diagonal of the adjacency matrix, i.e. loops in the graph where a node connects to itself."
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Attributes": {
            "p": "float\nValue between 0 and 1 (inclusive) representing the probability of any edge in the ER graph model",
            "p_mat": "np.ndarray, shape (n_verts, n_verts)\nProbability matrix for the fit model, from which graphs could be sampled."
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "directed": "bool"
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > fit": {
            "Parameters": {
                "graph": "array_like or networkx.Graph\nInput graph to fit",
                "y": "array_like, length graph.shape[0], optional\nCategorical labels for the block assignments of the graph"
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > bic": {
            "Description": "Bayesian information criterion for the current model on the input graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph"
            },
            "Returns": {
                "float": "The lower the better"
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > mse": {
            "Description": "Compute mean square error for the current model on the input graph\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph"
            },
            "Returns": {
                "float": "Mean square error for the model's fit P matrix"
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > sample": {
            "Description": "Sample graphs (realizations) from the fitted model\n\nCan only be called after the the model has been fit",
            "Parameters": {
                "n_samples": "int (default 1), optional\nThe number of graphs to sample"
            },
            "Returns": {
                "graphs": "np.array (n_samples, n_verts, n_verts)\nArray of sampled graphs, where the first dimension indexes each sample, and the other dimensions represent (n_verts x n_verts) adjacency matrices for the sampled graphs.\n\nNote that if only one sample is drawn, a (1, n_verts, n_verts) array will still be returned."
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > score": {
            "Description": "Compute the average log-likelihood over each potential edge of the given graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute"
            },
            "Returns": {
                "float": "sum of log-loglikelihoods for each potential edge in input graph"
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > score_samples": {
            "Description": "Compute the weighted log probabilities for each potential edge.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute",
                "clip": "scalar or None, optional (default=None)\nValues for which to clip probability matrix, entries less than c or more than 1 - c are set to c or 1 - c, respectively. If None, values will not be clipped in the likelihood calculation, which may result in poorly behaved likelihoods depending on the model."
            },
            "Returns": {
                "sample_scores": "np.ndarray (size of graph)\nlog-likelihood per potential edge in the graph"
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Field List > Methods > set_score_request": {
            "Description": "Request metadata passed to the score method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to score.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.\n\nNote\n\nThis method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in score."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "EREstimator"
    },
    {
        "Rubric": {
            "References": [
                "https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model"
            ]
        },
        "Section_id": "EREstimator"
    },
    {
        "Section_id": "DCEREstimator",
        "Description": "Degree-corrected Erdos-Reyni Model\n\nThe Degree-corrected Erdos-Reyni (DCER) model is an extension of the ER model in which each node has an additional \"promiscuity\" parameter that determines its expected degree in the graph.\n\nRead more in the Erdos-Renyi (ER) Model Tutorial"
    },
    {
        "Field List > Parameters": {
            "directed": "boolean, optional (default=True)\nWhether to treat the input graph as directed. Even if a directed graph is input, this determines whether to force symmetry upon the block probability matrix fit for the SBM. It will also determine whether graphs sampled from the model are directed.",
            "loops": "boolean, optional (default=False)\nWhether to allow entries on the diagonal of the adjacency matrix, i.e. loops in the graph where a node connects to itself.",
            "degree_directed": "boolean\nWhether to allow seperate degree correction parameters for the in and out degree of each node. Ignored if directed is False."
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Attributes": {
            "p_": "float\nThe p parameter as described in the above model, which weights the overall probability of connections between any two nodes.",
            "p_mat_": "np.ndarray, shape (n_verts, n_verts)\nProbability matrix for the fit model, from which graphs could be sampled.",
            "degree_corrections_": "np.ndarray, shape (n_verts, 1) or (n_verts, 2)\nDegree correction vector(s) . If degree_directed parameter was False, then will be of shape (n_verts, 1) and element i represents the degree correction for node . Otherwise, the first column contains out degree corrections and the second column contains in degree corrections."
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "directed": "bool",
                "loops": "bool",
                "degree_directed": "bool"
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit the DCSBM to a graph, optionally with known block labels.\nIf y is None, the block assignments for each vertex will first be estimated.",
            "Parameters": {
                "graph": "array-like or array_like or networkx.Graph\nInput graph to fit",
                "y": "array-like, length graph.shape[0], optional\nCategorical labels for the block assignments of the graph"
            },
            "Returns": {
                "self": "DCEREstimator\nFitted instance of self"
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > bic": {
            "Description": "Bayesian information criterion for the current model on the input graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph"
            },
            "Returns": {
                "float": "The lower the better"
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > mse": {
            "Description": "Compute mean square error for the current model on the input graph\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph"
            },
            "Returns": {
                "float": "Mean square error for the model's fit P matrix"
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > sample": {
            "Description": "Sample graphs (realizations) from the fitted model\n\nCan only be called after the the model has been fit",
            "Parameters": {
                "n_samples": "int\nThe number of graphs to sample"
            },
            "Returns": {
                "graphs": "np.array\nArray of sampled graphs, where the first dimension indexes each sample, and the other dimensions represent (n_verts x n_verts) adjacency matrices for the sampled graphs."
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > score": {
            "Description": "Compute the average log-likelihood over each potential edge of the given graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute"
            },
            "Returns": {
                "float": "sum of log-loglikelihoods for each potential edge in input graph"
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > score_samples": {
            "Description": "Compute the weighted log probabilities for each potential edge.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute",
                "clip": "float or None\nValues for which to clip probability matrix, entries less than c or more than 1 - c are set to c or 1 - c, respectively. If None, values will not be clipped in the likelihood calculation, which may result in poorly behaved likelihoods depending on the model."
            },
            "Returns": {
                "sample_scores": "np.ndarray\nlog-likelihood per potential edge in the graph"
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Field List > Methods > set_score_request": {
            "Description": "Request metadata passed to the score method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to score.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.\n\nNote\n\nThis method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in score."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Rubric": {
            "Note": "The DCER model is rarely mentioned in literature, though it is simply a special case of the DCSBM where there is only one community.",
            "References": [
                "https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model",
                "Karrer, B., & Newman, M. E. (2011). Stochastic blockmodels and community structure in networks. Physical review E, 83(1), 016107."
            ]
        },
        "Section_id": "DCEREstimator"
    },
    {
        "Section_id": "SBMEstimator",
        "Description": "Stochastic Block Model\n\nThe stochastic block model (SBM) represents each node as belonging to a block (or community). For a given potential edge between node i and j, the probability of an edge existing is specified by the block that nodes i and j belong to:\n\nP(A_ij = 1 | z_i = k, z_j = l) = B[k, l]\n\nwhere \ud835\udfd8\ud835\udfd9 is an n_nodes length vector specifying which block each node belongs to.\n\nRead more in the Stochastic Block Model (SBM) Tutorial"
    },
    {
        "Field List > Parameters": {
            "directed": "boolean, optional (default=True)\nWhether to treat the input graph as directed. Even if a directed graph is input, this determines whether to force symmetry upon the block probability matrix fit for the SBM. It will also determine whether graphs sampled from the model are directed.",
            "loops": "boolean, optional (default=False)\nWhether to allow entries on the diagonal of the adjacency matrix, i.e. loops in the graph where a node connects to itself.",
            "n_components": "int, optional (default=None)\nDesired dimensionality of embedding for clustering to find communities. n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension().",
            "min_comm": "int, optional (default=1)\nThe minimum number of communities (blocks) to consider.",
            "max_comm": "int, optional (default=10)\nThe maximum number of communities (blocks) to consider (inclusive).",
            "cluster_kws": "dict, optional (default={})\nAdditional kwargs passed down to GaussianCluster",
            "embed_kws": "dict, optional (default={})\nAdditional kwargs passed down to AdjacencySpectralEmbed"
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Attributes": {
            "block_p_": "ndarray,shape (n_blocks, n_blocks)\nThe block probability matrix B, where the element Bij represents the probability of an edge between block i and block j .",
            "p_mat_": "ndarray, shape (n_verts, n_verts)\nProbability matrix P for the fit model, from which graphs could be sampled.",
            "vertex_assignments_": "ndarray, shape (n_verts)\nA vector of integer labels corresponding to the predicted block that each node belongs to if y was not passed during the call to fit().",
            "block_weights_": "ndarray, shape (n_blocks)\nContains the proportion of nodes that belong to each block in the fit model."
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "directed": "bool",
                "loops": "bool",
                "n_components": "int",
                "min_comm": "int",
                "max_comm": "int",
                "cluster_kws": "Dict[str, Any]",
                "embed_kws": "Dict[str, Any]"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit the SBM to a graph, optionally with known block labels\n\nIf y is None, the block assignments for each vertex will first be estimated.",
            "Parameters": {
                "graph": "ndarray | csr_array | Graph",
                "y": "Any | None"
            },
            "Returns": {
                "SBMEstimator": "SBMEstimator"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > bic": {
            "Description": "Bayesian information criterion for the current model on the input graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "ndarray\nInput graph"
            },
            "Returns": {
                "float": "The lower the better"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > mse": {
            "Description": "Compute mean square error for the current model on the input graph\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "ndarray\nInput graph"
            },
            "Returns": {
                "float": "Mean square error for the model's fit P matrix"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > sample": {
            "Description": "Sample graphs (realizations) from the fitted model\n\nCan only be called after the the model has been fit",
            "Parameters": {
                "n_samples": "int (default 1), optional\nThe number of graphs to sample"
            },
            "Returns": {
                "graphs": "np.array (n_samples, n_verts, n_verts)\nArray of sampled graphs, where the first dimension indexes each sample, and the other dimensions represent (n_verts x n_verts)"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > score": {
            "Description": "Compute the average log-likelihood over each potential edge of the given graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute"
            },
            "Returns": {
                "float": "sum of log-loglikelihoods for each potential edge in input graph"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > score_samples": {
            "Description": "Compute the weighted log probabilities for each potential edge.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute",
                "clip": "scalar or None, optional (default=None)\nValues for which to clip probability matrix, entries less than c or more than 1 - c are set to c or 1 - c, respectively. If None, values will not be clipped in the likelihood calculation, which may result in poorly behaved likelihoods depending on the model."
            },
            "Returns": {
                "sample_scores": "np.ndarray (size of graph)\nlog-likelihood per potential edge in the graph"
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Field List > Methods > set_score_request": {
            "Description": "Request metadata passed to the score method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to score.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.\n\nNote\n\nThis method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in score."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Rubric": {
            "References": [
                "Holland, P. W., Laskey, K. B., & Leinhardt, S. (1983). Stochastic blockmodels: First steps. Social networks, 5(2), 109-137."
            ]
        },
        "Section_id": "SBMEstimator"
    },
    {
        "Section_id": "DCSBMEstimator",
        "Description": "Degree-corrected Stochastic Block Model\n\nThe degree-corrected stochastic block model (DCSBM) represents each node as belonging to a block (or community). For a given potential edge between node i and j, the probability of an edge existing is specified by the block that nodes i and j belong to as in the SBM. However, an additional \"promiscuity\" parameter is added for each node, allowing the vertices within a block to have heterogeneous expected degree distributions:\n\nP(A_ij = 1 | z_i = k, z_j = l) = B[k, l]\u03b8_i\u03b8_j\n\nwhere \ud835\udfd8\ud835\udfd9 is an n_nodes length vector specifying which block each node belongs to, and \u03b8 is an n_nodes length vector specifiying the degree correction for each node.\n\nThe degree_directed parameter of this model allows the degree correction parameter to be different for the in and out degree of each node:\n\nP(A_ij = 1 | z_i = k, z_j = l) = B[k, l]\u03b8_i\u03b8_j\n\nwhere \u03b8_i and \u03b8_j need not be the same.\n\nRead more in the Stochastic Block Model (SBM) Tutorial"
    },
    {
        "Field List > Parameters": {
            "directed": "boolean, optional (default=True)\nWhether to treat the input graph as directed. Even if a directed graph is input, this determines whether to force symmetry upon the block probability matrix fit for the SBM. It will also determine whether graphs sampled from the model are directed.",
            "degree_directed": "boolean, optional (default=False)\nWhether to fit an \"in\" and \"out\" degree correction for each node. In the degree_directed case, the fit model can have a different expected in and out degree for each node.",
            "loops": "boolean, optional (default=False)\nWhether to allow entries on the diagonal of the adjacency matrix, i.e. loops in the graph where a node connects to itself.",
            "n_components": "int, optional (default=None)\nDesired dimensionality of embedding for clustering to find communities. n_components must be < min(X.shape). If None, then optimal dimensions will be chosen by select_dimension().",
            "min_comm": "int, optional (default=1)\nThe minimum number of communities (blocks) to consider.",
            "max_comm": "int, optional (default=10)\nThe maximum number of communities (blocks) to consider (inclusive).",
            "cluster_kws": "dict, optional (default={})\nAdditional kwargs passed down to GaussianCluster",
            "embed_kws": "dict, optional (default={})\nAdditional kwargs passed down to LaplacianSpectralEmbed"
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Attributes": {
            "block_p_": "np.ndarray, shape (n_blocks, n_blocks)\nThe block probability matrix , where the element represents the expected number of edges between block and block .",
            "p_mat_": "np.ndarray, shape (n_verts, n_verts)\nProbability matrix  for the fit model, from which graphs could be sampled.",
            "degree_corrections_": "np.ndarray, shape (n_verts, 1) or (n_verts, 2)\nDegree correction vector(s) . If degree_directed parameter was False, then will be of shape (n_verts, 1) and element represents the degree correction for node . Otherwise, the first column contains out degree corrections and the second column contains in degree corrections.",
            "vertex_assignments_": "np.ndarray, shape (n_verts)\nA vector of integer labels corresponding to the predicted block that each node belongs to if y was not passed during the call to fit().",
            "block_weights_": "np.ndarray, shape (n_blocks)\nContains the proportion of nodes that belong to each block in the fit model."
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "degree_directed": "bool",
                "directed": "bool",
                "loops": "bool",
                "n_components": "int",
                "min_comm": "int",
                "max_comm": "int",
                "cluster_kws": "Dict[str, Any]",
                "embed_kws": "Dict[str, Any]"
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit the DCSBM to a graph, optionally with known block labels\n\nIf y is None, the block assignments for each vertex will first be estimated.",
            "Parameters": {
                "graph": "array_like or networkx.Graph\nInput graph to fit",
                "y": "array_like, length graph.shape[0], optional\nCategorical labels for the block assignments of the graph"
            },
            "Returns": {
                "self": "DCSBMEstimator object\nFitted instance of self"
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > bic": {
            "Description": "Bayesian information criterion for the current model on the input graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph"
            },
            "Returns": {
                "float": "The lower the better"
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > mse": {
            "Description": "Compute mean square error for the current model on the input graph\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph"
            },
            "Returns": {
                "float": "Mean square error for the model's fit P matrix"
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > sample": {
            "Description": "Sample graphs (realizations) from the fitted model\n\nCan only be called after the the model has been fit",
            "Parameters": {
                "n_samples": "int\nThe number of graphs to sample"
            },
            "Returns": {
                "graphs": "np.array\nArray of sampled graphs, where the first dimension indexes each sample, and the other dimensions represent (n_verts x n_verts) adjacency matrices for the sampled graphs."
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > score": {
            "Description": "Compute the average log-likelihood over each potential edge of the given graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute"
            },
            "Returns": {
                "float": "sum of log-loglikelihoods for each potential edge in input graph"
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > score_samples": {
            "Description": "Compute the weighted log probabilities for each potential edge.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute",
                "clip": "scalar or None, optional (default=None)\nValues for which to clip probability matrix, entries less than c or more than 1 - c are set to c or 1 - c, respectively. If None, values will not be clipped in the likelihood calculation, which may result in poorly behaved likelihoods depending on the model."
            },
            "Returns": {
                "sample_scores": "np.ndarray (size of graph)\nlog-likelihood per potential edge in the graph"
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Field List > Methods > set_score_request": {
            "Description": "Request metadata passed to the score method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to score.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.\n\nNote\n\nThis method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in score."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Rubric": {
            "Note": "Note that many examples in the literature describe the DCSBM as being sampled with a Poisson distribution. Here, we implement this model with a Bernoulli. When individual edge probabilities are relatively low these two distributions will yield similar results.",
            "References": [
                "Karrer, B., & Newman, M. E. (2011). Stochastic blockmodels and community structure in networks. Physical review E, 83(1), 016107."
            ]
        },
        "Section_id": "DCSBMEstimator"
    },
    {
        "Section_id": "RDPGEstimator",
        "Description": "Random Dot Product Graph\n\nUnder the random dot product graph model, each node is assumed to have a \"latent position\" in some -dimensional Euclidian space. This vector dictates that node's probability of connection to other nodes. For a given pair of nodes i and j, the probability of connection is the dot product between their latent positions:\n\nP(A_ij = 1) = <x_i, x_j>\n\nwhere x_i is the latent position of node i. If the graph being modeled is is undirected, then P(A_ij = 1) = <x_i, x_j> + <x_j, x_i>. Latent positions can be estimated via AdjacencySpectralEmbed.\n\nRead more in the Random Dot Product Graph (RDPG) Model Tutorial"
    },
    {
        "Field List > Parameters": {
            "loops": "boolean, optional (default=False)\nWhether to allow entries on the diagonal of the adjacency matrix, i.e. loops in the graph where a node connects to itself.",
            "n_components": "int, optional (default=None)\nThe dimensionality of the latent space used to model the graph. If None, the method of Zhu and Godsie will be used to select an embedding dimension.",
            "ase_kws": "dict, optional (default={})\nDictionary of keyword arguments passed down to AdjacencySpectralEmbed, which is used to fit the model.",
            "diag_aug_weight": "int or float, optional (default=1)\nWeighting used for diagonal augmentation, which is a form of regularization for fitting the RDPG model.",
            "plus_c_weight": "int or float, optional (default=1)\nWeighting used for a constant scalar added to the adjacency matrix before embedding as a form of regularization."
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Attributes": {
            "latent_tuple_": "length 2, or np.ndarray, shape (n_verts, n_components)\nThe fit latent positions for the RDPG model. If a tuple, then the graph that was input to fit was directed, and the first and second elements of the tuple are the left and right latent positions, respectively. The left and right latent positions will both be of shape (n_verts, n_components). If latent_ is an array, then the graph that was input to fit was undirected and the left and right latent positions are the same.",
            "p_mat_": "np.ndarray, shape (n_verts, n_verts)\nProbability matrix P for the fit model, from which graphs could be sampled."
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "loops": "bool",
                "n_components": "int",
                "ase_kws": "dict",
                "diag_aug_weight": "float",
                "plus_c_weight": "float"
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Calculate the parameters for the given graph model",
            "Parameters": {
                "graph": "ndarray | csr_array | Graph",
                "y": "Any | None"
            },
            "Returns": {
                "RDPGEstimator": "RDPGEstimator"
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > bic": {
            "Description": "Bayesian information criterion for the current model on the input graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "ndarray"
            },
            "Returns": {
                "bic": "float\nThe lower the better"
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > mse": {
            "Description": "Compute mean square error for the current model on the input graph\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "ndarray\nInput graph"
            },
            "Returns": {
                "mse": "float\nMean square error for the model's fit P matrix"
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > sample": {
            "Description": "Sample graphs (realizations) from the fitted model\n\nCan only be called after the the model has been fit",
            "Parameters": {
                "n_samples": "int\nThe number of graphs to sample"
            },
            "Returns": {
                "graphs": "np.array\nArray of sampled graphs, where the first dimension indexes each sample, and the other dimensions represent (n_verts x n_verts) adjacency matrices for the sampled graphs."
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > score": {
            "Description": "Compute the average log-likelihood over each potential edge of the given graph.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute"
            },
            "Returns": {
                "float": "sum of log-loglikelihoods for each potential edge in input graph"
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > score_samples": {
            "Description": "Compute the weighted log probabilities for each potential edge.\n\nNote that this implicitly assumes the input graph is indexed like the fit model.",
            "Parameters": {
                "graph": "np.ndarray\nInput graph. Must be same shape as model's p_mat_ attribute",
                "clip": "scalar or None, optional (default=None)\nValues for which to clip probability matrix, entries less than c or more than 1 - c are set to c or 1 - c, respectively. If None, values will not be clipped in the likelihood calculation, which may result in poorly behaved likelihoods depending on the model."
            },
            "Returns": {
                "sample_scores": "np.ndarray (size of graph)\nlog-likelihood per potential edge in the graph"
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Field List > Methods > set_score_request": {
            "Description": "Request metadata passed to the score method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to score.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.\n\nNote\n\nThis method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a Pipeline. Otherwise it has no effect.",
            "Parameters": {
                "graph": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for graph parameter in score."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Rubric": {
            "References": [
                "Athreya, A., Fishkind, D. E., Tang, M., Priebe, C. E., Park, Y., Vogelstein, J. T., ... & Sussman, D. L. (2018). Statistical inference on random dot product graphs: a survey. Journal of Machine Learning Research, 18(226), 1-92.",
                "Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2), pp.918-930."
            ]
        },
        "Section_id": "RDPGEstimator"
    },
    {
        "Section_id": "EdgeSwapper",
        "Description": "Degree Preserving Edge Swaps\n\nThis class allows for performing degree preserving edge swaps to generate new networks with the same degree sequence as the input network."
    },
    {
        "Field List > Attributes": {
            "adjacency_": "np.ndarray OR csr_array, shape (n_verts, n_verts)\nThe initial adjacency matrix to perform edge swaps on. Must be unweighted and undirected.",
            "edge_list_": "np.ndarray, shape (n_verts, 2)\nThe corresponding edgelist for the input network",
            "seed_": "int, optional\nRandom seed to make outputs reproducible, must be positive"
        },
        "Section_id": "EdgeSwapper"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "adjacency": "ndarray | csr_array",
                "seed": "int | None"
            }
        },
        "Section_id": "EdgeSwapper"
    },
    {
        "Field List > Methods > swap_edges": {
            "Description": "Performs a number of edge swaps on the graph",
            "Parameters": {
                "n_swaps": "int(default 1), optional\nThe number of edge swaps to be performed"
            },
            "Returns": {
                "adjacency": "np.ndarray OR csr.matrix, shape (n_verts, n_verts)\nThe adjancency matrix after a number of edge swaps are performed on the graph",
                "edge_list": "np.ndarray (n_verts, 2)\nThe edge_list after a number of edge swaps are perfomed on the graph"
            }
        },
        "Section_id": "EdgeSwapper"
    },
    {
        "Rubric": {
            "References": [
                "Fosdick, B. K., Larremore, D. B., Nishimura, J., & Ugander, J. (2018). Configuring random graph models with fixed degree sequences. Siam Review, 60(2), 315-355.",
                "Carstens, C. J., & Horadam, K. J. (2017). Switching edges to randomize networks: what goes wrong and how to fix it. Journal of Complex Networks, 5(3), 337-351.",
                "https://github.com/joelnish/double-edge-swap-mcmc/blob/master/dbl_edge_mcmc.py"
            ]
        },
        "Section_id": "EdgeSwapper"
    },
    {
        "Section_id": "SpectralVertexNomination",
        "Description": "Class for spectral vertex nomination on a single graph.\n\nGiven a graph  and a subset of  called  (the \"seed\"), Single Graph Vertex Nomination is the problem of ranking all  in order of relation to members of . Spectral Vertex Nomination solves this problem by embedding  into a low dimensional euclidean space (see: Adjacency Spectral Embed Tutorial ), and then generating a nomination list by some distance based algorithm. In the simple unattributed case, for each seed vertex , the other vertices are ranked in order of euclidean distance from ."
    },
    {
        "Field List > Parameters": {
            "input_graph": "bool, default = True\nFlag whether to expect two full graphs, or the embeddings.\n\nTrue\n.fit and .fit_predict() expect graphs as adjacency matrix, provided as ndarray of shape (n, n). They will be embedded using the specified embedder.\n\nFalse\n.fit() and .fit_predict() expect an embedding of the graph, i.e. a ndarray of size (n, d).",
            "embedder": "str or BaseSpectralEmbed, default = 'ASE'\nMay provide either a embed object or a string indicating which embedding method to use, which may be either: \"ASE\" for AdjacencySpectralEmbed or \"LSE\" for LaplacianSpectralEmbed.",
            "n_neighbors": "int, default=None\nThe number of vertices to nominate for each seed.",
            "metric": "str, default = 'euclidean'\nDistance metric to use when finding the nearest neighbors, all sklearn metrics available.",
            "metric_params": "dict, default = None\nArguments for the sklearn DistanceMetric specified via metric parameter."
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Attributes": {
            "nearest_neighbors_": "sklearn.neighbors.NearestNeighbors\nA fit sklearn NearestNeighbors classifier used to find closest vertices to each seed."
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "input_graph": "bool",
                "embedder": "str | BaseSpectralEmbed",
                "n_neighbors": "int | None",
                "metric": "str",
                "metric_params": "dict | None"
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Constructs the embedding if not provided, then calculates the pairwise distance from each seed to each vertex in graph",
            "Parameters": {
                "X": "ndarray\nIf input_graph is True\nExpects a graph as an adjacency matrix, i.e. an ndarray of shape (n, n). Will be embedded using the specified embedder.\n\nIf input_graph is False\nExpects an embedding of the graph, i.e. a ndarray of size (n, d).",
                "y": "NoneType\nIncluded by sklearn convention."
            },
            "Returns": {
                "self": "object\nReturns an instance of self."
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > predict": {
            "Description": "Nominates vertices for each seed vertex. Methodology is distance based ranking",
            "Parameters": {
                "y": "np.ndarray\nThe indices of the seed vertices. Should be a dim 1 array with length less than ."
            },
            "Returns": {
                "Tuple[ndarray, ndarray]": "Nomination Listnp.ndarray\nShape is (number_vertices, number_vertices_in_seed) . Each column is a seed vertex, and the rows of each column are a list of vertex indexes from the original adjacency matrix in order degree of match.\n\nDistance Matrixnp.ndarray\nThe matrix of distances associated with each element of the nomination list."
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > fit_predict": {
            "Description": "Calls this class' fit and then predict methods",
            "Parameters": {
                "X": "ndarray\nIf input_graph is True\nExpects a graph as an adjacency matrix, i.e. an ndarray of shape (n, n). Will be embedded using the specified embedder.\n\nIf input_graph is False\nExpects an embedding of the graph, i.e. a ndarray of size (n, d).",
                "y": "ndarray.\nList of unattributed seed vertex indices."
            },
            "Returns": {
                "Tuple[ndarray, ndarray]": "Nomination Listnp.ndarray\nShape is (number_vertices, number_vertices_in_seed) . Each column is a seed vertex, and the rows of each column are a list of vertex indexes from the original adjacency matrix in order degree of match.\n\nDistance Matrixnp.ndarray\nThe matrix of distances associated with each element of the nomination list."
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Field List > Methods > set_params": {
            "Description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.",
            "Parameters": {
                "**params": "dict\nEstimator parameters."
            },
            "Returns": {
                "self": "estimator instance\nEstimator instance."
            }
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Rubric": {
            "References": [
                "Fishkind, D. E.; Lyzinski, V.; Pao, H.; Chen, L.; Priebe, C. E. Vertex nomination schemes for membership prediction. Ann. Appl. Stat. 9 2015. https://projecteuclid.org/euclid.aoas/1446488749",
                "Jordan Yoder, Li Chen, Henry Pao, Eric Bridgeford, Keith Levin, Donniell E. Fishkind, Carey Priebe, Vince Lyzinski, Vertex nomination: The canonical sampling and the extended spectral nomination schemes, Computational Statistics & Data Analysis, Volume 145, 2020. http://www.sciencedirect.com/science/article/pii/S0167947320300074"
            ]
        },
        "Section_id": "SpectralVertexNomination"
    },
    {
        "Section_id": "VNviaSGM",
        "Description": "This class implements Vertex Nomination via Seeded Graph Matching (VNviaSGM) with the algorithm described in [1].\n\nRather than providing a 1-1 matching for the vertices of two graphs, as in GraphMatch, VNviaSGM ranks the potential matches for a vertex of interst (VOI) in one to graph to the vertices in another graph, based on probability of matching."
    },
    {
        "Field List > Parameters": {
            "order_voi_subgraph": "int, positive (default = 1)\nOrder used to create induced subgraph on A about VOI where the max distance between VOI and other nodes is order_voi_subgraph. This induced subgraph will be used to determine what seeds are used when the SGM algorithm is called. If no seeds are in this subgraph about VOI, then a UserWarning is thrown, and nomination_list_ is None.",
            "order_seeds_subgraph": "int, positive (default = 1)\nOrder used to create induced subgraphs on A and B. These subgraphs are centered about the seeds that were determined by the subgraph generated by order_voi_subgraph. These two subgraphs will be passed into the SGM algorithm.",
            "n_init": "int, positive (default = 100)\nNumber of random initializations of the seeded graph matching algorithm (SGM). Increasing the number of restarts will make the probabilities returned more precise.",
            "max_nominations": "int (default = None)\nMax number of nominations to include in the nomination list. If None is passed, then all nominations computed will be returned.",
            "graph_match_kwsdict": "default = {}\nGives users the option to pass custom arguments to the graph matching algorithm. Format should be {'arg_name': arg_value, ...}. See GraphMatch"
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Attributes": {
            "n_seeds_": "int\nNumber of seeds passed in seedsA that occured in the induced subgraph about VOI",
            "nomination_list_": "2d-array\nAn array containing vertex nominations in the form nomination list = [[j, p_val],...] where p_val is the probability that the VOI matches to node j in graph B (sorted by descending probability)"
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "order_voi_subgraph": "int",
                "order_seeds_subgraph": "int",
                "n_init": "int",
                "max_nominations": "int | None",
                "graph_match_kws": "Dict[str, Any]"
            }
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fits the model to two graphs.",
            "Parameters": {
                "A": "2d-array, square\nAdjacency matrix, the graph where voi is known",
                "B": "2d-array, square\nAdjacency matrix, the graph where voi is not known",
                "voi": "int\nVertex of interest",
                "seeds": "list, 2d-array\nList of length two, of form [seedsA, seedsB]. The elements of seedsA and seedsB are vertex indices from A and B, respectively, which are known to be matched; that is, vertex seedsA[i] is matched to vertex seedsB[i]. Note: len(seedsA)==len(seedsB)."
            },
            "Returns": {
                "self": "An instance of self"
            }
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Methods > fit_predict": {
            "Description": "Fits model to two adjacency matrices and returns",
            "Parameters": {
                "A": "2d-array, square\nAdjacency matrix, the graph where voi is known",
                "B": "2d-array, square\nAdjacency matrix, the graph where voi is not known",
                "voi": "int\nVertex of interest",
                "seeds": "list, 2d-array\nList of length two, of form [seedsA, seedsB]. The elements of seedsA and seedsB are vertex indices from A and B, respectively, which are known to be matched; that is, vertex seedsA[i] is matched to vertex seedsB[i]. Note: len(seedsA)==len(seedsB)."
            },
            "Returns": {
                "nomination_list_": "2d-array\nThe nomination list."
            }
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Methods > get_metadata_routing": {
            "Description": "Get metadata routing of this object.\n\nPlease check User Guide on how the routing mechanism works.",
            "Returns": {
                "routing": "MetadataRequest\nA MetadataRequest encapsulating routing information."
            }
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Methods > get_params": {
            "Description": "Get parameters for this estimator.",
            "Parameters": {
                "deep": "bool, default=True\nIf True, will return the parameters for this estimator and contained subobjects that are estimators."
            },
            "Returns": {
                "params": "dict\nParameter names mapped to their values."
            }
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Field List > Methods > set_fit_request": {
            "Description": "Request metadata passed to the fit method.\n\nNote that this method is only relevant if enable_metadata_routing=True (see sklearn.set_config()). Please see User Guide on how the routing mechanism works.\n\nThe options for each parameter are:\n\nTrue: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.\n\nFalse: metadata is not requested and the meta-estimator will not pass it to fit.\n\nNone: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\nstr: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.\n\nNew in version 1.3.",
            "Parameters": {
                "A": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for A parameter in fit.",
                "B": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for B parameter in fit.",
                "seeds": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for seeds parameter in fit.",
                "voi": "str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED\nMetadata routing for voi parameter in fit."
            },
            "Returns": {
                "self": "object\nThe updated object."
            }
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Rubric": {
            "Note": "VNviaSGM generates an initial induced subgraph about the VOI to determine which seeds are close enough to be used. If no seeds are close enough, then a warning is thrown and nomination_list_ is set to None.\n\nAll the seeds that are close enough are then used to generate subgraphs in both A and B. These subgraphs are matched using several random initializations of the seeded graph matching algorithm (SGM), and a nomination list is returned. See GraphMatch for SGM docs",
            "References": [
                "Patsolic, HG, Park, Y, Lyzinski, V, Priebe, CE. Vertex nomination via seeded graph matching. Stat Anal Data Min: The ASA Data Sci Journal. 2020; 13: 229\u2013 244. https://doi.org/10.1002/sam.11454"
            ]
        },
        "Section_id": "VNviaSGM"
    },
    {
        "Section_id": "modularity",
        "Description": "Given an undirected graph and a dictionary of vertices to community ids, calculate the modularity."
    },
    {
        "Field List > Parameters": {
            "graph": "Graph\nAn undirected graph",
            "partitions": "Dict[Any, int]\nA dictionary representing a community partitioning scheme with the keys being the vertex and the value being a community id.",
            "weight_attribute": "str\nThe edge data attribute on the graph that contains a float weight for the edge.",
            "resolution": "float\nThe resolution to use when calculating the modularity."
        },
        "Section_id": "modularity"
    },
    {
        "Field List > Returns": {
            "float": "The sum of the modularity of each of the communities."
        },
        "Section_id": "modularity"
    },
    {
        "Rubric": {
            "References": [
                "https://en.wikipedia.org/wiki/Modularity_(networks)"
            ]
        },
        "Section_id": "modularity"
    },
    {
        "Section_id": "modularity_components",
        "Description": "Given an undirected, weighted graph and a community partition dictionary, calculates a modularity quantum for each community ID. The sum of these quanta is the modularity of the graph and partitions provided."
    },
    {
        "Field List > Parameters": {
            "graph": "Graph\nAn undirected graph",
            "partitions": "Dict[Any, int]\nA dictionary representing a community partitioning scheme with the keys being the vertex and the value being a community id.",
            "weight_attribute": "str\nThe edge data attribute on the graph that contains a float weight for the edge.",
            "resolution": "float\nThe resolution to use when calculating the modularity."
        },
        "Section_id": "modularity_components"
    },
    {
        "Field List > Returns": {
            "Dict[int, float]": "A dictionary of the community id to the modularity component of that community"
        },
        "Section_id": "modularity_components"
    },
    {
        "Section_id": "leiden",
        "Description": "Leiden is a global network partitioning algorithm. Given a graph, it will iterate through the network node by node, and test for an improvement in our quality maximization function by speculatively joining partitions of each neighboring node.\n\nThis process continues until no moves are made that increases the partitioning quality."
    },
    {
        "Field List > Parameters": {
            "graph": "GraphRepresentation\nA graph representation, whether a weighted edge list referencing an undirected graph, an undirected networkx graph, or an undirected adjacency matrix in either numpy.ndarray or scipy.sparse.csr_array form. Please see the Notes section regarding node ids used.",
            "starting_communities": "Optional[Dict[Any, int]]\nDefault is None. An optional community mapping dictionary that contains a node id mapping to the community it belongs to. Please see the Notes section regarding node ids used.\n\nIf no community map is provided, the default behavior is to create a node community identity map, where every node is in their own community.",
            "extra_forced_iterations": "int\nDefault is 0. Leiden will run until a maximum quality score has been found for the node clustering and no nodes are moved to a new cluster in another iteration. As there is an element of randomness to the Leiden algorithm, it is sometimes useful to set extra_forced_iterations to a number larger than 0 where the process is forced to attempt further refinement.",
            "resolution": "Union[int, float]\nDefault is 1.0. Higher resolution values lead to more communities and lower resolution values leads to fewer communities. Must be greater than 0.",
            "randomness": "Union[int, float]\nDefault is 0.001. The larger the randomness value, the more exploration of the partition space is possible. This is a major difference from the Louvain algorithm, which is purely greedy in the partition exploration.",
            "use_modularity": "bool\nDefault is True. If False, will use a Constant Potts Model (CPM).",
            "random_seed": "Optional[int]\nDefault is None. Can provide an optional seed to the PRNG used in Leiden for deterministic output.",
            "weight_attribute": "str\nDefault is weight. Only used when creating a weighed edge list of tuples when the source graph is a networkx graph. This attribute corresponds to the edge data dict key.",
            "is_weighted": "Optional[bool]\nDefault is None. Only used when creating a weighted edge list of tuples when the source graph is an adjacency matrix. The graspologic.utils.is_unweighted() function will scan these matrices and attempt to determine whether it is weighted or not. This flag can short circuit this test and the values in the adjacency matrix will be treated as weights.",
            "weight_default": "Union[int, float]\nDefault is 1.0. If the graph is a networkx graph and the graph does not have a fully weighted sequence of edges, this default will be used. If the adjacency matrix is found or specified to be unweighted, this weight_default will be used for every edge.",
            "check_directed": "bool\nDefault is True. If the graph is an adjacency matrix, we will attempt to ascertain whether it is directed or undirected. As our leiden implementation is only known to work with an undirected graph, this function will raise an error if it is found to be a directed graph. If you know it is undirected and wish to avoid this scan, you can set this value to False and only the lower triangle of the adjacency matrix will be used to generate the weighted edge list.",
            "trials": "int\nDefault is 1. Runs leiden trials times, keeping the best partitioning as judged by the quality maximization function (default: modularity, see use_modularity parameter for details). This differs from extra_forced_iterations by starting over from scratch each for each trial, while extra_forced_iterations attempts to make microscopic adjustments from the \"final\" state."
        },
        "Section_id": "leiden"
    },
    {
        "Field List > Returns": {
            "Dict[Any, int]": "The results of running leiden over the provided graph, a dictionary containing mappings of node -> community id. Isolate nodes in the input graph are not returned in the result."
        },
        "Section_id": "leiden"
    },
    {
        "Rubric": {
            "Note": "No two different nodes are allowed to encode to the same str representation, e.g. node_a id of \"1\" and node_b id of 1 are different object types but str(node_a) == str(node_b). This collision will result in a ValueError",
            "References": [
                "Traag, V.A.; Waltman, L.; Van, Eck N.J. \"From Louvain to Leiden: guaranteeing well-connected communities\", Scientific Reports, Vol. 9, 2019",
                "https://github.com/microsoft/graspologic-native"
            ]
        },
        "Section_id": "leiden"
    },
    {
        "Section_id": "HierarchicalCluster",
        "Description": "Create new instance of HierarchicalCluster(node, cluster, parent_cluster, level, is_final_cluster)"
    },
    {
        "Field List > Attributes": {
            "node": "Any\nNode id",
            "cluster": "int\nLeiden cluster id",
            "parent_cluster": "int | None\nOnly used when level != 0, but will indicate the previous cluster id that this node was in",
            "level": "int\nEach time a community has a higher population than we would like, we create a subnetwork of that community and process it again to break it into smaller chunks. Each time we detect this, the level increases by 1",
            "is_final_cluster": "bool\nWhether this is the terminal cluster in the hierarchical leiden process or not"
        },
        "Section_id": "HierarchicalCluster"
    },
    {
        "Field List > Methods > __new__": {
            "Parameters": {
                "node": "Any",
                "cluster": "int",
                "parent_cluster": "int | None",
                "level": "int",
                "is_final_cluster": "bool"
            }
        },
        "Section_id": "HierarchicalCluster"
    },
    {
        "Section_id": "HierarchicalClusters",
        "Description": "HierarchicalClusters is a subclass of Python's list class with two helper methods for retrieving dictionary views of the first and final level of hierarchical clustering in dictionary form. The rest of the HierarchicalCluster entries in this list can be seen as a transition state log of our graspologic.partition.hierarchical_leiden() process as it continuously tries to break down communities over a certain size, with the two helper methods on this list providing you the starting point community map and ending point community map."
    },
    {
        "Field List > Methods > __init__": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > __new__": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > append": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > clear": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > copy": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > count": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > extend": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > index": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > insert": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > pop": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > remove": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > reverse": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > sort": {},
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > first_level_hierarchical_clustering": {
            "Returns": {
                "Dict[Any, int]": "The initial leiden algorithm clustering results as a dictionary of node id to community id."
            }
        },
        "Section_id": "HierarchicalClusters"
    },
    {
        "Field List > Methods > final_level_hierarchical_clustering": {
            "Returns": {
                "Dict[Any, int]": "The last leiden algorithm clustering results as a dictionary of node id to community id."
            }
        },
        "Section_id": "HierarchicalClusters"
    },
    {
        "Section_id": "hierarchical_leiden",
        "Description": "Leiden is a global network partitioning algorithm. Given a graph, it will iterate through the network node by node, and test for an improvement in our quality maximization function by speculatively joining partitions of each neighboring node.\n\nThis process continues until no moves are made that increases the partitioning quality.\n\nUnlike the function graspologic.partition.leiden(), this function does not stop after maximization has been achieved. On some large graphs, it's useful to identify particularly large communities whose membership counts exceed max_cluster_size and induce a subnetwork solely out of that community. This subnetwork is then treated as a wholly separate entity, leiden is run over it, and the new, smaller communities are then mapped into the original community map space.\n\nThe results also differ substantially; the returned List[HierarchicalCluster] is more of a log of state at each level. All HierarchicalClusters at level 0 should be considered to be the results of running graspologic.partition.leiden(). Every community whose membership is greater than max_cluster_size will then also have entries where level == 1, and so on until no communities are greater in population than max_cluster_size OR we are unable to break them down any further.\n\nOnce a node's membership registration in a community cannot be changed any further, it is marked with the flag graspologic.partition.HierarchicalCluster.is_final_cluster = True."
    },
    {
        "Field List > Parameters": {
            "graph": "GraphRepresentation\nA graph representation, whether a weighted edge list referencing an undirected graph, an undirected networkx graph, or an undirected adjacency matrix in either numpy.ndarray or scipy.sparse.csr_array form. Please see the Notes section regarding node ids used.",
            "max_cluster_size": "int\nDefault is 1000. Any partition or cluster with membership >= max_cluster_size will be isolated into a subnetwork. This subnetwork will be used for a new leiden global partition mapping, which will then be remapped back into the global space after completion. Once all clusters with membership >= max_cluster_size have been completed, the level increases and the partition scheme is scanned again for any new clusters with membership >= max_cluster_size and the process continues until every cluster's membership is < max_cluster_size or if they cannot be broken into more than one new community.",
            "starting_communities": "Optional[Dict[Any, int]]\nDefault is None. An optional community mapping dictionary that contains a node id mapping to the community it belongs to. Please see the Notes section regarding node ids used.\n\nIf no community map is provided, the default behavior is to create a node community identity map, where every node is in their own community.",
            "extra_forced_iterations": "int\nDefault is 0. Leiden will run until a maximum quality score has been found for the node clustering and no nodes are moved to a new cluster in another iteration. As there is an element of randomness to the Leiden algorithm, it is sometimes useful to set extra_forced_iterations to a number larger than 0 where the entire process is forced to attempt further refinement.",
            "resolution": "Union[int, float]\nDefault is 1.0. Higher resolution values lead to more communities and lower resolution values leads to fewer communities. Must be greater than 0.",
            "randomness": "Union[int, float]\nDefault is 0.001. The larger the randomness value, the more exploration of the partition space is possible. This is a major difference from the Louvain algorithm, which is purely greedy in the partition exploration.",
            "use_modularity": "bool\nDefault is True. If False, will use a Constant Potts Model (CPM).",
            "random_seed": "Optional int\nDefault is None. Can provide an optional seed to the PRNG used in Leiden for deterministic output.",
            "weight_attribute": "str\nDefault is weight. Only used when creating a weighed edge list of tuples when the source graph is a networkx graph. This attribute corresponds to the edge data dict key.",
            "is_weighted": "Optional[bool]\nDefault is None. Only used when creating a weighted edge list of tuples when the source graph is an adjacency matrix. The graspologic.utils.is_unweighted() function will scan these matrices and attempt to determine whether it is weighted or not. This flag can short circuit this test and the values in the adjacency matrix will be treated as weights.",
            "weight_default": "Union[int, float]\nDefault is 1.0. If the graph is a networkx graph and the graph does not have a fully weighted sequence of edges, this default will be used. If the adjacency matrix is found or specified to be unweighted, this weight_default will be used for every edge.",
            "check_directed": "bool\nDefault is True. If the graph is an adjacency matrix, we will attempt to ascertain whether it is directed or undirected. As our leiden implementation is only known to work with an undirected graph, this function will raise an error if it is found to be a directed graph. If you know it is undirected and wish to avoid this scan, you can set this value to False and only the lower triangle of the adjacency matrix will be used to generate the weighted edge list."
        },
        "Section_id": "hierarchical_leiden"
    },
    {
        "Field List > Returns": {
            "HierarchicalClusters": "The results of running hierarchical leiden over the provided graph, a list of HierarchicalClusters identifying the state of every node and cluster at each level. Isolate nodes in the input graph"
        },
        "Section_id": "hierarchical_leiden"
    },
    {
        "Rubric": {
            "Note": "No two different nodes are allowed to encode to the same str representation, e.g. node_a id of \"1\" and node_b id of 1 are different object types but str(node_a) == str(node_b). This collision will result in a ValueError\nThis function is implemented in the graspologic-native Python module, a module written in Rust for Python.",
            "References": [
                "Traag, V.A.; Waltman, L.; Van, Eck N.J. \"From Louvain to Leiden: guaranteeing well-connected communities\",Scientific Reports, Vol. 9, 2019",
                "https://github.com/microsoft/graspologic-native"
            ]
        },
        "Section_id": "hierarchical_leiden"
    },
    {
        "Section_id": "check_argument_types",
        "Description": "Raises a TypeError if the provided value is not one of the required_types"
    },
    {
        "Field List > Parameters": {
            "value": "Any\nThe argument to test for valid type",
            "required_types": "Union[type, Tuple[type, ...]]\nA type or a n-ary tuple of types to test for validity",
            "message": "str\nThe message to use as the body of the TypeError"
        },
        "Section_id": "check_argument_types"
    },
    {
        "Field List > Returns": {
            "None": "None"
        },
        "Section_id": "check_argument_types"
    },
    {
        "Section_id": "check_optional_argument_types",
        "Description": "Raises a TypeError if the provided value is not one of the required_types, unless it is None. A None value is treated as a valid type."
    },
    {
        "Field List > Parameters": {
            "value": "Any\nThe argument to test for valid type",
            "required_types": "Union[type, Tuple[type, ...]]\nA type or a n-ary tuple of types to test for validity",
            "message": "str\nThe message to use as the body of the TypeError"
        },
        "Section_id": "check_optional_argument_types"
    },
    {
        "Field List > Returns": {
            "None": "None"
        },
        "Section_id": "check_optional_argument_types"
    },
    {
        "Section_id": "check_argument",
        "Description": "Raises a ValueError if the provided check is false"
    },
    {
        "Field List > Parameters": {
            "check": "bool",
            "message": "str"
        },
        "Section_id": "check_argument"
    },
    {
        "Field List > Returns": {
            "None": "None"
        },
        "Section_id": "check_argument"
    },
    {
        "Section_id": "is_real_weighted",
        "Description": "Checks every edge in graph to ascertain whether it has:\n\na weight_attribute key in the data dictionary for the edge\nif that weight_attribute value is a subclass of numbers.Real\n\nIf any edge fails this test, it returns False, else True"
    },
    {
        "Field List > Parameters": {
            "graph": "Union[nx.Graph, nx.DiGraph]\nThe networkx graph to test",
            "weight_attribute": "str\nThe edge dictionary data attribute that holds the weight. Default is weight."
        },
        "Section_id": "is_real_weighted"
    },
    {
        "Field List > Returns": {
            "bool": "True if every edge has a numeric weight_attribute weight, False if any edge fails this test"
        },
        "Section_id": "is_real_weighted"
    },
    {
        "Section_id": "GraphBuilder",
        "Description": "GraphBuilder is a simple builder for networkx Graphs. To use less memory, it automatically maps all node ids of any hashable type to int.\n\nIn other words, if you can use it as a key in a dictionary, it will work."
    },
    {
        "Field List > Parameters": {
            "directed": "bool\nUsed to create either a networkx.Graph or networkx.DiGraph object."
        },
        "Section_id": "GraphBuilder"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "directed": "bool"
            }
        },
        "Section_id": "GraphBuilder"
    },
    {
        "Field List > Methods > add_edge": {
            "Description": "Adds a weighted edge between the provided source and target. The source and target id are converted to a unique int.",
            "Parameters": {
                "source": "Any\nsource node id",
                "target": "Any\ntarget node id",
                "weight": "Union[int, float] (default=1.0)\nThe weight for the edge. If none is provided, the weight is defaulted to 1.",
                "sum_weight": "bool(default=True)\nIf an edge between the source and target already exist, should we sum the edge weights or overwrite the edge weight with the provided weight value.",
                "attributes": "kwargs\nThe attributes kwargs are presumed to be attributes that should be added to the edge dictionary for source and target."
            },
            "Returns": {
                "None": "None"
            }
        },
        "Section_id": "GraphBuilder"
    },
    {
        "Field List > Methods > build": {
            "Returns": {
                "Tuple[Graph | DiGraph, Dict[Any, int], List[Any]]": "The returned tuple is either an undirected or directed graph, depending on the constructor argument directed. The second value in the tuple is a dictionary of original node ids to their assigned integer ids. The third and final value in the tuple is a List of original node ids, where the index corresponds to the assigned integer and the value is the corresponding original ID."
            }
        },
        "Section_id": "GraphBuilder"
    },
    {
        "Section_id": "embeddings",
        "Description": "Embeddings is an iterable, indexed interface over the parallel numpy arrays that are generated by our embedding functions."
    },
    {
        "Field List > Parameters": {
            "labels": "np.ndarray\nThe node labels that are positionally correlated with the embeddings. The dtype of labels is any object stored in a networkx Graph object, though type uniformity will be required",
            "embeddings": "np.ndarray\nThe embedded values generated by the embedding technique."
        },
        "Section_id": "embeddings"
    },
    {
        "Field List > Methods > __init__": {
            "Parameters": {
                "labels": "np.ndarry\nThe node labels that are positionally correlated with the embeddings. The dtype of labels is any object stored in a networkx Graph object, though type uniformity will be required",
                "embeddings": "np.ndarray\nThe embedded values generated by the embedding technique."
            },
            "Returns": {
                "beartype.roar.BeartypeCallHintParamViolation": "if the types are invalid",
                "ValueError": "if the row count of labels does not equal the row count of embeddings"
            },
            "example": [
                "import numpy as np\nraw_embeddings = np.array([[1,2,3,4,5], [6,4,2,0,8]])\nlabels = np.array([\"monotonic\", \"not_monotonic\"])\nembeddings_obj = Embeddings(labels, raw_embeddings)\nembeddings_obj[0]\n('monotonic', array([1, 2, 3, 4, 5]))\nlen(embeddings_obj)\n2\nlist(iter(embeddings_obj))\n[('monotonic', array([1, 2, 3, 4, 5])), ('not_monotonic', array([6, 4, 2, 0, 8]))]\nembeddings_lookup = embeddings_obj.as_dict()\nembeddings_lookup[\"not_monotonic\"]\narray([6, 4, 2, 0, 8])"
            ]
        },
        "Section_id": "embeddings"
    },
    {
        "Field List > Methods > labels": {
            "Returns": {
                "ndarray": "ndarray"
            }
        },
        "Section_id": "embeddings"
    },
    {
        "Field List > Methods > embeddings": {
            "Returns": {
                "ndarray": "ndarray"
            }
        },
        "Section_id": "embeddings"
    },
    {
        "Field List > Methods > as_dict": {
            "Returns": {
                "EmbeddingsView": "EmbeddingsView"
            }
        },
        "Section_id": "embeddings"
    },
    {
        "Rubric": {
            "example": [
                "import numpy as np\nraw_embeddings = np.array([[1,2,3,4,5], [6,4,2,0,8]])\nlabels = np.array([\"monotonic\", \"not_monotonic\"])\nembeddings_obj = Embeddings(labels, raw_embeddings)\nembeddings_obj[0]\n('monotonic', array([1, 2, 3, 4, 5]))\nlen(embeddings_obj)\n2\nlist(iter(embeddings_obj))\n[('monotonic', array([1, 2, 3, 4, 5])), ('not_monotonic', array([6, 4, 2, 0, 8]))]\nembeddings_lookup = embeddings_obj.as_dict()\nembeddings_lookup[\"not_monotonic\"]\narray([6, 4, 2, 0, 8])"
            ]
        },
        "Section_id": "embeddings"
    },
    {
        "Section_id": "adjacency_spectral_embedding",
        "Description": "Given a directed or undirected networkx graph (not multigraph), generate an Embeddings object.\n\nAdjacency spectral embeddings are extremely egocentric, implying that results are slanted toward the core-periphery of each node. This is in contrast to Laplacian spectral embeddings, which look further into the latent space when it captures change.\n\nAdjacency Spectral Embedding Tutorial\n\nGraphs will always have their diagonal augmented. In other words, a self-loop will be created for each node with a weight corresponding to the weighted degree.\n\nLastly, all weights will be rescaled based on their relative rank in the graph, which is beneficial in minimizing anomalous results if some edge weights are extremely atypical of the rest of the graph."
    },
    {
        "Field List > Parameters": {
            "graph": "Union[nx.Graph, nx.OrderedGraph, nx.DiGraph, nx.OrderedDiGraph]\nAn undirected or directed graph. The graph must:\n\nbe fully numerically weighted (every edge must have a real, numeric weight or else it will be treated as an unweighted graph)\nbe a basic graph (meaning it should not be a multigraph; if you have a multigraph you must first decide how you want to handle the weights of the edges between two nodes, whether summed, averaged, last-wins, maximum-weight-only, etc)",
            "dimensions": "int\nDimensions to use for the svd solver. For undirected graphs, if elbow_cut==None, you will receive an embedding that has nodes rows and dimensions columns. For directed graphs, if elbow_cut==None, you will receive an embedding that has nodes rows and 2*dimensions columns. If elbow_cut is specified to be not None, we will cut the embedding at elbow_cut elbow, but the provided dimensions will be used in the creation of the SVD.",
            "elbow_cut": "Optional[int]\nUsing a process described by Zhu & Ghodsi in their paper \"Automatic dimensionality selection from the scree plot via the use of profile likelihood\", truncate the dimensionality of the return on the elbow_cut-th elbow. By default this value is None but can be used to reduce the dimensionality of the returned tensors.",
            "svd_solver_algorithm": "str\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n\n'full'\nComputes full svd using scipy.linalg.svd() Does not support graph input of type scipy.sparse.csr_array\n\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "svd_solver_iterations": "int\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "svd_seed": "Optional[int]\nUsed to seed the PRNG used in the randomized svd solver algorithm.",
            "weight_attribute": "str\nThe edge dictionary key that contains the weight of the edge."
        },
        "Section_id": "adjacency_spectral_embedding"
    },
    {
        "Field List > Returns": {
            "Embeddings": "Embeddings"
        },
        "Section_id": "adjacency_spectral_embedding"
    },
    {
        "Rubric": {
            "Note": "The singular value decomposition:\n\nis used to find an orthonormal basis for a matrix, which in our case is the adjacency matrix of the graph. These basis vectors (in the matrices U or V) are ordered according to the amount of variance they explain in the original matrix. By selecting a subset of these basis vectors (through our choice of dimensionality reduction) we can find a lower dimensional space in which to represent the graph.",
            "References": [
                "Sussman, D.L., Tang, M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012",
                "Levin, K., Roosta-Khorasani, F., Mahoney, M. W., & Priebe, C. E. (2018). Out-of-sample extension of graph adjacency spectral embedding. PMLR: Proceedings of Machine Learning Research, 80, 2975-2984.",
                "Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2), pp.918-930."
            ]
        },
        "Section_id": "adjacency_spectral_embedding"
    },
    {
        "Section_id": "laplacian_spectral_embedding",
        "Description": "Given a directed or undirected networkx graph (not multigraph), generate an Embeddings object.\n\nThe laplacian spectral embedding process is similar to the adjacency spectral embedding process, with the key differentiator being that the LSE process looks further into the latent space when it captures changes, whereas the ASE process is egocentric and focused on immediate differentiators in a node's periphery.\n\nAll weights will be rescaled based on their relative rank in the graph, which is beneficial in minimizing anomalous results if some edge weights are extremely atypical of the rest of the graph."
    },
    {
        "Field List > Parameters": {
            "graph": "Union[nx.Graph, nx.OrderedGraph, nx.DiGraph, nx.OrderedDiGraph]\nAn undirected or directed graph. The graph must:\n\nbe fully numerically weighted (every edge must have a real, numeric weight or else it will be treated as an unweighted graph)\nbe a basic graph (meaning it should not be a multigraph; if you have a multigraph you must first decide how you want to handle the weights of the edges between two nodes, whether summed, averaged, last-wins, maximum-weight-only, etc)",
            "form": "str\nSpecifies the type of Laplacian normalization to use. Allowed values are: { \"DAD\", \"I-DAD\", \"R-DAD\" }. See to_laplacian() for more details regarding form.",
            "dimensions": "int\nDimensions to use for the svd solver. For undirected graphs, if elbow_cut==None, you will receive an embedding that has nodes rows and dimensions columns. For directed graphs, if elbow_cut==None, you will receive an embedding that has nodes rows and 2*dimensions columns. If elbow_cut is specified to be not None, we will cut the embedding at elbow_cut elbow, but the provided dimensions will be used in the creation of the SVD.",
            "elbow_cut": "Optional[int]\nUsing a process described by Zhu & Ghodsi in their paper \"Automatic dimensionality selection from the scree plot via the use of profile likelihood\", truncate the dimensionality of the return on the elbow_cut-th elbow. By default this value is None but can be used to reduce the dimensionality of the returned tensors.",
            "svd_solver_algorithm": "str\nallowed values: {'randomized', 'full', 'truncated'}\n\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n\n'full'\nComputes full svd using scipy.linalg.svd() Does not support graph input of type scipy.sparse.csr_array\n\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "svd_solver_iterations": "int\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "svd_seed": "Optional[int]\nUsed to seed the PRNG used in the randomized svd solver algorithm.",
            "weight_attribute": "str\nThe edge dictionary key that contains the weight of the edge.",
            "regularizer": "Optional[numbers.Real]\nOnly used when form=\"R-DAD\". Must be None or nonnegative. Constant to be added to the diagonal of degree matrix. If None, average node degree is added. If int or float, must be >= 0."
        },
        "Section_id": "laplacian_spectral_embedding"
    },
    {
        "Field List > Returns": {
            "Embeddings": "Embeddings"
        },
        "Section_id": "laplacian_spectral_embedding"
    },
    {
        "Rubric": {
            "Note": "The singular value decomposition:\n\nis used to find an orthonormal basis for a matrix, which in our case is the Laplacian matrix of the graph. These basis vectors (in the matrices U or V) are ordered according to the amount of variance they explain in the original matrix. By selecting a subset of these basis vectors (through our choice of dimensionality reduction) we can find a lower dimensional space in which to represent the graph.",
            "References": [
                "Sussman, D.L., Tang M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012.",
                "Von Luxburg, Ulrike. \"A tutorial on spectral clustering,\" Statistics and computing, Vol. 17(4), 2007.",
                "Rohe, Karl, Sourav Chatterjee, and Bin Yu. \"Spectral clustering and the high-dimensional stochastic blockmodel,\" The Annals of Statistics, Vol. 39(4), pp. 1878-1915, 2011.",
                "Zhu, M. and Ghodsi, A. \"Automatic dimensionality selection from the scree plot via the use of profile likelihood,\" Computational Statistics & Data Analysis, Vol. 51(2), pp. 918-930, 2006."
            ]
        },
        "Section_id": "laplacian_spectral_embedding"
    },
    {
        "Section_id": "omnibus_embedding_pairwise",
        "Description": "Generates a pairwise omnibus embedding for each pair of graphs in a list of graphs using the adjacency matrix. If given graphs A, B, and C, the embeddings will be computed for A, B and B, C.\n\nIf the node labels differ between each pair of graphs, then those nodes will only be found in the resulting embedding if they exist in the largest connected component of the union of all edges across all graphs in the time series.\n\nGraphs will always have their diagonal augmented. In other words, a self-loop will be created for each node with a weight corresponding to the weighted degree.\n\nLastly, all weights will be rescaled based on their relative rank in the graph, which is beneficial in minimizing anomalous results if some edge weights are extremely atypical of the rest of the graph."
    },
    {
        "Field List > Parameters": {
            "graphs": "List[Union[nx.Graph, nx.OrderedGraph, nx.DiGraph, nx.OrderedDiGraph]]\nA list of undirected or directed graphs. The graphs must:\n\nbe fully numerically weighted (every edge must have a real, numeric weight or else it will be treated as an unweighted graph)\nbe a basic graph (meaning it should not be a multigraph; if you have a multigraph you must first decide how you want to handle the weights of the edges between two nodes, whether summed, averaged, last-wins, maximum-weight-only, etc)",
            "dimensions": "int\nDimensions to use for the svd solver. For undirected graphs, if elbow_cut==None, you will receive an embedding that has nodes rows and dimensions columns. For directed graphs, if elbow_cut==None, you will receive an embedding that has nodes rows and 2*dimensions columns. If elbow_cut is specified to be not None, we will cut the embedding at elbow_cut elbow, but the provided dimensions will be used in the creation of the SVD.",
            "elbow_cut": "Optional[int]\nUsing a process described by Zhu & Ghodsi in their paper \"Automatic dimensionality selection from the scree plot via the use of profile likelihood\", truncate the dimensionality of the return on the elbow_cut-th elbow. By default this value is None but can be used to reduce the dimensionality of the returned tensors.",
            "svd_solver_algorithm": "str\nallowed values: {'randomized', 'full', 'truncated'}\n\nSVD solver to use:\n\n'randomized'\nComputes randomized svd using sklearn.utils.extmath.randomized_svd()\n\n'full'\nComputes full svd using scipy.linalg.svd() Does not support graph input of type scipy.sparse.csr_array\n\n'truncated'\nComputes truncated svd using scipy.sparse.linalg.svds()",
            "svd_solver_iterations": "int\nNumber of iterations for randomized SVD solver. Not used by 'full' or 'truncated'. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.",
            "svd_seed": "Optional[int]\nUsed to seed the PRNG used in the randomized svd solver algorithm.",
            "weight_attribute": "str\nThe edge dictionary key that contains the weight of the edge.",
            "use_laplacian": "bool\nDetermine whether to use the Laplacian matrix of each graph in order to calculate the omnibus embedding using the Laplacian spectral embedding technique."
        },
        "Section_id": "omnibus_embedding_pairwise"
    },
    {
        "Field List > Returns": {
            "List[Tuple[Embeddings, Embeddings]]": "List of Tuple[Embeddings, Embeddings]"
        },
        "Section_id": "omnibus_embedding_pairwise"
    },
    {
        "Rubric": {
            "References": [
                "Levin, K., Athreya, A., Tang, M., Lyzinski, V., & Priebe, C. E. \"A central limit theorem for an omnibus embedding of multiple random dot product graphs,\" Data Mining Workshops (ICDMW), 2017 IEEE International Conference on, 2017",
                "Sussman, D.L., Tang, M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012",
                "Levin, K., Roosta-Khorasani, F., Mahoney, M. W., & Priebe, C. E. \"Out-of-sample extension of graph adjacency spectral embedding,\" PMLR: Proceedings of Machine Learning Research, 80, 2975-2984.",
                "Zhu, M. and Ghodsi, A. \"Automatic dimensionality selection from the scree plot via the use of profile likelihood,\" Computational Statistics & Data Analysis, 51(2), pp.918-930."
            ]
        },
        "Section_id": "omnibus_embedding_pairwise"
    },
    {
        "Section_id": "heatmap",
        "Description": "Plots a graph as a color-encoded matrix.\n\nNodes can be grouped by providing inner_hier_labels or both inner_hier_labels and outer_hier_labels. Nodes can also be sorted by the degree from largest to smallest degree nodes. The nodes will be sorted within each group if labels are also provided."
    },
    {
        "Field List > Parameters": {
            "X": "nx.Graph or np.ndarray object\nGraph or numpy matrix to plot",
            "transform": "None, or string {'log', 'log10', 'zero-boost', 'simple-all', 'simple-nonzero'}\n'log'\nPlots the natural log of all nonzero numbers\n\n'log10'\nPlots the base 10 log of all nonzero numbers\n\n'zero-boost'\nPass to ranks method. preserves the edge weight for all 0s, but ranks the other edges as if the ranks of all 0 edges has been assigned.\n\n'simple-all'\nPass to ranks method. Assigns ranks to all non-zero edges, settling ties using the average. Ranks are then scaled by  where n is the number of nodes\n\n'simple-nonzero'\nPass to ranks method. Same as simple-all, but ranks are scaled by ",
            "figsize": "Tuple[int, int]\nWidth, height in inches.",
            "title": "str\nTitle of plot.",
            "context": "str\nThe name of a preconfigured set.",
            "font_scale": "int\nSeparate scaling factor to independently scale the size of the font elements.",
            "xticklabels": "bool\nIf list-like, plot these alternate labels as the ticklabels.",
            "yticklabels": "bool\nIf list-like, plot these alternate labels as the ticklabels.",
            "cmap": "str\nValid matplotlib color map.",
            "vmin": "float\nValues to anchor the colormap, otherwise they are inferred from the data and other keyword arguments.",
            "vmax": "float\nValues to anchor the colormap, otherwise they are inferred from the data and other keyword arguments.",
            "center": "int\nThe value at which to center the colormap",
            "cbar": "bool\nWhether to draw a colorbar.",
            "inner_hier_labels": "ndarray | List[Any] | None\nCategorical labeling of the nodes. If not None, will group the nodes according to these labels and plot the labels on the marginal",
            "outer_hier_labels": "ndarray | List[Any] | None\nCategorical labeling of the nodes, ignored without inner_hier_labels If not None, will plot these labels as the second level of a hierarchy on the marginals",
            "hier_label_fontsize": "int\nSize (in points) of the text labels for the inner_hier_labels and outer_hier_labels.",
            "ax": "Axes | None\nAxes in which to draw the plot, otherwise will generate its own axes",
            "title_pad": "float\nCustom padding to use for the distance of the title from the heatmap. Autoscales if None",
            "sort_nodes": "bool\nWhether or not to sort the nodes of the graph by the sum of edge weights (degree for an unweighted graph). If inner_hier_labels is passed and sort_nodes is True, will sort nodes this way within block.",
            "kwargs": "Any\nadditional plotting arguments passed to Seaborn's heatmap"
        },
        "Section_id": "heatmap"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes"
        },
        "Section_id": "heatmap"
    },
    {
        "Section_id": "gridplot",
        "Description": "Plots multiple graphs on top of each other with dots as edges.\n\nThis function is useful for visualizing multiple graphs simultaneously. The size of the dots correspond to the edge weights of the graphs, and colors represent input graphs."
    },
    {
        "Field List > Parameters": {
            "X": "list of nx.Graph or np.ndarray object\nList of nx.Graph or numpy arrays to plot",
            "labels": "list of str\nList of strings, which are labels for each element in X. len(X) == len(labels).",
            "transform": "None, or string {'log', 'log10', 'zero-boost', 'simple-all', 'simple-nonzero'}\n'log'\nPlots the natural log of all nonzero numbers\n\n'log10'\nPlots the base 10 log of all nonzero numbers\n\n'zero-boost'\nPass to ranks method. preserves the edge weight for all 0s, but ranks the other edges as if the ranks of all 0 edges has been assigned.\n\n'simple-all'\nPass to ranks method. Assigns ranks to all non-zero edges, settling ties using the average. Ranks are then scaled by  where n is the number of nodes\n\n'simple-nonzero'\nPass to ranks method. Same as simple-all, but ranks are scaled by ",
            "height": "int\nHeight of figure in inches.",
            "title": "str\nTitle of plot.",
            "context": "str\nThe name of a preconfigured set.",
            "font_scale": "float\nSeparate scaling factor to independently scale the size of the font elements.",
            "palette": "str\nSet of colors for mapping the hue variable. If a dict, keys should be values in the hue variable. For acceptable string arguments, see the palette options at Choosing Colormaps in Matplotlib",
            "alpha": "float [0, 1]\nAlpha value of plotted gridplot points",
            "sizes": "Tuple[int, int]\nMin and max size to plot edge weights",
            "legend_name": "str\nName to plot above the legend",
            "inner_hier_labels": "ndarray | List[Any] | None\nCategorical labeling of the nodes. If not None, will group the nodes according to these labels and plot the labels on the marginal",
            "outer_hier_labels": "ndarray | List[Any] | None\nCategorical labeling of the nodes, ignored without inner_hier_labels If not None, will plot these labels as the second level of a hierarchy on the marginals",
            "hier_label_fontsize": "int\nSize (in points) of the text labels for the inner_hier_labels and outer_hier_labels.",
            "title_pad": "int, float or None\nCustom padding to use for the distance of the title from the heatmap. Autoscales if None",
            "sort_nodes": "bool\nWhether or not to sort the nodes of the graph by the sum of edge weights (degree for an unweighted graph). If inner_hier_labels is passed and sort_nodes is True, will sort nodes this way within block."
        },
        "Section_id": "gridplot"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes"
        },
        "Section_id": "gridplot"
    },
    {
        "Section_id": "pairplot",
        "Description": "Plot pairwise relationships in a dataset.\n\nBy default, this function will create a grid of axes such that each dimension in data will by shared in the y-axis across a single row and in the x-axis across a single column.\n\nThe off-diagonal axes show the pairwise relationships displayed as scatterplot. The diagonal axes show the univariate distribution of the data for that dimension displayed as either a histogram or kernel density estimates (KDEs)."
    },
    {
        "Field List > Parameters": {
            "X": "array-like, shape (n_samples, n_features)\nInput data.",
            "labels": "array-like or list, shape (n_samples), optional\nLabels that correspond to each sample in X.",
            "col_names": "array-like or list, shape (n_features), optional\nNames or labels for each feature in X. If not provided, the default will be Dimension 1, Dimension 2, etc.",
            "title": "str, optional\nTitle of plot.",
            "legend_name": "str, optional\nTitle of the legend.",
            "variables": "list of variable names, optional\nVariables to plot based on col_names, otherwise use every column with a numeric datatype.",
            "height": "int, optional, default: 10\nHeight of figure in inches.",
            "context": "None, or one of {paper, notebook, talk (default), poster}\nThe name of a preconfigured set.",
            "font_scale": "float, optional, default: 1\nSeparate scaling factor to independently scale the size of the font elements.",
            "palette": "str, dict, optional, default: 'Set1'\nSet of colors for mapping the hue variable. If a dict, keys should be values in the hue variable. For acceptable string arguments, see the palette options at Choosing Colormaps in Matplotlib.",
            "alpha": "float, optional, default: 0.7\nOpacity value of plotter markers between 0 and 1",
            "size": "float or int, optional, default: 50\nSize of plotted markers.",
            "marker": "string, optional, default: '.'\nMatplotlib marker specifier, see the marker options at Matplotlib style marker specification"
        },
        "Section_id": "pairplot"
    },
    {
        "Field List > Returns": {
            "PairGrid": "PairGrid"
        },
        "Section_id": "pairplot"
    },
    {
        "Section_id": "pairplot_with_gmm",
        "Description": "Plot pairwise relationships in a dataset, also showing a clustering predicted by a Gaussian mixture model.\n\nBy default, this function will create a grid of axes such that each dimension in data will by shared in the y-axis across a single row and in the x-axis across a single column.\n\nThe off-diagonal axes show the pairwise relationships displayed as scatterplot. The diagonal axes show the univariate distribution of the data for that dimension displayed as either a histogram or kernel density estimates (KDEs)."
    },
    {
        "Field List > Parameters": {
            "X": "array-like, shape (n_samples, n_features)\nInput data.",
            "gmm": "GaussianMixture object\nA fit sklearn.mixture.GaussianMixture object. Gaussian mixture models (GMMs) are probabilistic models for representing data based on normally distributed subpopulations, GMM clusters each data point into a corresponding subpopulation.",
            "labels": "array-like or list, shape (n_samples), optional\nLabels that correspond to each sample in X. If labels are not passed in then labels are predicted by gmm.",
            "label_palette": "str or dict, optional, default: 'Set1'\nPalette used to color points if labels are passed in.",
            "cluster_palette": "str or dict, optional, default: 'Set1'\nPalette used to color GMM ellipses (and points if no labels are passed).",
            "title": "string, default: \"\"\nTitle of the plot.",
            "legend_name": "string, default: None\nName to put above the legend. If None, will be \"Cluster\" if no custom labels are passed, and \"\" otherwise.",
            "context": "None, or one of {talk (default), paper, notebook, poster}\nSeaborn plotting context",
            "font_scale": "float, optional, default: 1\nSeparate scaling factor to independently scale the size of the font elements.",
            "alpha": "float, optional, default: 0.7\nOpacity value of plotter markers between 0 and 1",
            "figsize": "tuple\nThe size of the 2d subplots configuration",
            "histplot_kws": "dict, default: {}\nKeyword arguments passed down to seaborn.histplot()"
        },
        "Section_id": "pairplot_with_gmm"
    },
    {
        "Field List > Returns": {
            "fit": {
                "Figure": "matplotlib Figure",
                "Axes": "np.ndarray\nArray of matplotlib Axes"
            }
        },
        "Section_id": "pairplot_with_gmm"
    },
    {
        "Section_id": "degreeplot",
        "Description": "Plots the distribution of node degrees for the input graph. Allows for sets of node labels, will plot a distribution for each node category."
    },
    {
        "Field List > Parameters": {
            "X": "ndarray\ninput graph",
            "labels": "ndarray | List[Any] | None",
            "direction": "str",
            "title": "str",
            "context": "str",
            "font_scale": "float",
            "figsize": "Tuple[int, int]",
            "palette": "str"
        },
        "Section_id": "degreeplot"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes"
        },
        "Section_id": "degreeplot"
    },
    {
        "Section_id": "edgeplot",
        "Description": "Plots the distribution of edge weights for the input graph. Allows for sets of node labels, will plot edge weight distribution for each node category."
    },
    {
        "Field List > Parameters": {
            "X": "ndarray",
            "labels": "ndarray | List[Any] | None",
            "nonzero": "bool",
            "title": "str",
            "context": "str",
            "font_scale": "float",
            "figsize": "Tuple[int, int]",
            "palette": "str"
        },
        "Section_id": "edgeplot"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes"
        },
        "Section_id": "edgeplot"
    },
    {
        "Section_id": "screeplot",
        "Description": "Plots the distribution of singular values for a matrix, either showing the raw distribution or an empirical CDF (depending on cumulative)"
    },
    {
        "Field List > Parameters": {
            "X": "ndarray",
            "title": "str",
            "context": "str",
            "font_scale": "float",
            "figsize": "Tuple[int, int]",
            "ax": "Axes | None",
            "cumulative": "bool",
            "show_first": "int | None",
            "show_elbow": "bool | int | None"
        },
        "Section_id": "screeplot"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes"
        },
        "Section_id": "screeplot"
    },
    {
        "Rubric": {
            "References": [
                "Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2), pp.918-930."
            ]
        },
        "Section_id": "screeplot"
    },
    {
        "Section_id": "adjplot",
        "Description": "Sorts and plots a square matrix in various ways, and with optional information added to the margin of the matrix plot.This function is a wrapper around matrixplot which assumes that the plotted matrix is square, and that the rows and columns represent the same items and have the same metadata (e.g. the adjacency matrix of a graph)."
    },
    {
        "Field List > Parameters": {
            "data": "ndarray\nMatrix to plot, must be square. Sparse matrix input is only accepted if plot_type == 'scattermap'.",
            "ax": "matplotlib axes object (default=None)\nAxes in which to draw the plot. If no axis is passed, one will be created.",
            "meta": "pd.DataFrame or None, (default=None)\nMetadata of the matrix.\nmeta is pd.DataFrame All sorting keywords should only be str or list of str. They should contain references to columns in meta.\nmeta is None\nAll sorting keywords should only array-like with the same length as the corresponding axis of data.",
            "plot_type": "str in {\"heatmap\", \"scattermap\"} (default=\"heatmap\")\n\"heatmap\" will draw the matrix using seaborn.heatmap(), \"scattermap\" will draw each nonzero element of the matrix using seaborn.scatterplot(). \"scattermap\" is recommended for larger sparse matrices.",
            "group": "str, list of str, or array-like, (default=None)\nAttribute(s) by which to group rows/columns of data. If multiple groups are specified, rows/columns will be sorted hierarchically (first by the first group), then within that group by a possible second group, etc.). Behaves similarly to pandas.DataFrame.sort_values().",
            "group_order": "str, list of str, or array-like, (default=\"size\")\nAttribute(s) by which to sort the groups if provided by group. Groups are sorted by the mean of this attribute in ascending order. \"size\" is a special keyword which will sort groups by the number of elements per group.",
            "item_order": "str, list of str, or array-like (default=None)\nAttribute(s) by which to sort the individual rows/columns within each group.",
            "color": "str, list of str, or array-like (default=None)\nAttribute(s) to use for drawing colors on the borders of the matrix plot.",
            "highlight": "str, list of str, or array-like (default=None)\nAttribute(s) in meta by which to draw highlighted separators between specific groups, can be useful for emphasizing a particular region of the plot. Styling of the highlighted separators can be specified via spinestyle_kws.",
            "palette": "str, dict, list of str or dict (default=\"tab10\")\nColormap(s) of the color axes if specified by color.",
            "ticks": "bool, optional (default=True)\nWhether the plot has labels for the groups specified by group.",
            "tick_pad": "int or float (default=None)\nCustom padding to use for the distance between tick axes",
            "color_pad": "int or float (default=None)\nCustom padding to use for the distance between color axes",
            "border": "bool (default=True)\nWhether the plot should have a border.",
            "center": "int (default=0)\nThe value at which to center the colormap when plotting divergent data (only used when plot_type=\"heatmap\").",
            "cmap": "str (default=\"RdBu_r\")\nColormap of the heatmap (only used when plot_type=\"heatmap\").",
            "sizes": "tuple (default=(5, 10))\nMin and max sizes of dots (only used when plot_type=\"scattermap\").",
            "square": "bool (default=False)\nWhether the plot should be square.",
            "gridline_kws": "dict (default=None)\nPlotting arguments for the separators.",
            "spinestyle_kws": "dict (default=None)\nPlotting arguments for the spine border.",
            "highlight_kws": "dict (default=None)\nPlotting arguments for the highlighted separators.",
            "**kwargs": "Additional plotting arguments passed down to the plotting function which will draw the matrix data, see plot_type for more information."
        },
        "Section_id": "adjplot"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes\nAxes in which to draw the plot, by default None",
            "divider": "AxesLocator\nDivider used to add new axes to the plot"
        },
        "Section_id": "adjplot"
    },
    {
        "Section_id": "matrixplot",
        "Description": "Sorts and plots a matrix in various ways, and with optional information added to the margin of the matrix plot."
    },
    {
        "Field List > Parameters": {
            "data": "np.ndarray or scipy.sparse.csr_array with ndim=2\nMatrix to plot. Sparse matrix input is only accepted if plot_type == 'scattermap'.",
            "ax": "matplotlib axes object (default=None)\nAxes in which to draw the plot. If no axis is passed, one will be created.",
            "plot_type": "str in {\"heatmap\", \"scattermap\"} (default=\"heatmap\")\n\"heatmap\" will draw the matrix using seaborn.heatmap(), \"scattermap\" will draw each nonzero element of the matrix using seaborn.scatterplot(). \"scattermap\" is recommended for larger sparse matrices.",
            "{row,col}_meta": "pd.DataFrame or None, (default=None)\nMetadata of the matrix.\n{row,col}_meta is pd.DataFrame All sorting keywords should only be str or list of str. They should contain references to columns in meta.\n{row,col}_meta is None All sorting keywords should only array-like with the same length as the corresponding axis of data.",
            "{row,col}_group": "str, list of str, or array-like, (default=None)\nAttribute(s) by which to group rows/columns of data. If multiple groups are specified, rows/columns will be sorted hierarchically (first by the first group), then within that group by a possible second group, etc.). Behaves similarly to pandas.DataFrame.sort_values().",
            "{row,col}_group_order": "str, list of str, or array-like, (default=\"size\")\nAttribute(s) by which to sort the groups if provided by {row,col}_group. Groups are sorted by the mean of this attribute in ascending order. \"size\" is a special keyword which will sort groups by the number of elements per group.",
            "{row,col}_item_order": "str, list of str, or array-like (default=None)\nAttribute(s) by which to sort the individual rows/columns within each group.",
            "{row,col}_color": "str, list of str, or array-like (default=None)\nAttribute(s) to use for drawing colors on the borders of the matrix plot.",
            "{row,col}_highlight": "str, list of str, or array-like (default=None)\nAttribute(s) in meta by which to draw highlighted separators between specific groups, can be useful for emphasizing a particular region of the plot. Styling of the highlighted separators can be specified via spinestyle_kws.",
            "{row,col}_palette": "str, dict, list of str or dict (default=\"tab10\")\nColormap(s) of the color axes if specified by {row,col}_color.",
            "{row,col}_ticks": "bool, optional (default=True)\nWhether the plot has labels for the groups specified by {row,col}_group.",
            "{row,col}_tick_pad": "int or float (default=None)\nCustom padding to use for the distance between tick axes",
            "{row,col}_color_pad": "int or float (default=None)\nCustom padding to use for the distance between color axes",
            "border": "bool (default=True)\nWhether the plot should have a border.",
            "center": "int (default=0)\nThe value at which to center the colormap when plotting divergent data (only used when plot_type=\"heatmap\").",
            "cmap": "str (default=\"RdBu_r\")\nColormap of the heatmap (only used when plot_type=\"heatmap\").",
            "sizes": "tuple (default=(5, 10))\nMin and max sizes of dots (only used when plot_type=\"scattermap\").",
            "square": "bool (default=False)\nWhether the plot should be square.",
            "gridline_kws": "dict (default=None)\nPlotting arguments for the separators.",
            "spinestyle_kws": "dict (default=None)\nPlotting arguments for the spine border.",
            "highlight_kws": "dict (default=None)\nPlotting arguments for the highlighted separators.",
            "**kwargs": "Additional plotting arguments passed down to the plotting function which will draw the matrix data, see plot_type for more information."
        },
        "Section_id": "matrixplot"
    },
    {
        "Field List > Returns": {
            "Axes": "Axes\nAxes in which to draw the plot, by default None",
            "divider": "AxesLocator\nDivider used to add new axes to the plot"
        },
        "Section_id": "matrixplot"
    },
    {
        "Section_id": "er_np",
        "Description": "Samples a Erdos Renyi (n, p) graph with specified edge probability.\n\nErdos Renyi (n, p) graph is a simple graph with n vertices and a probability p of edges being connected."
    },
    {
        "Field List > Parameters": {
            "n": "int\nNumber of vertices",
            "p": "float\nProbability of an edge existing between two vertices, between 0 and 1.",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal.",
            "wt": "object, optional (default=1)\nWeight function for each of the edges, taking only a size argument. This weight function will be randomly assigned for selected edges. If 1, graph produced is binary.",
            "wtargs": "dictionary, optional (default=None)\nOptional arguments for parameters that can be passed to weight function wt.",
            "dc": "function or array-like, shape (n_vertices)\ndc is used to generate a degree-corrected Erdos Renyi Model in which each node in the graph has a parameter to specify its expected degree relative to other nodes.\n\nfunction:\nshould generate a non-negative number to be used as a degree correction to create a heterogenous degree distribution. A weight will be generated for each vertex, normalized so that the sum of weights is 1.\n\narray-like of scalars, shape (n_vertices):\nThe weights should sum to 1; otherwise, they will be normalized and a warning will be thrown. The scalar associated with each vertex is the node's relative expected degree.",
            "dc_kws": "dictionary\nIgnored if dc is none or array of scalar. If dc is a function, dc_kws corresponds to its named arguments. If not specified, in either case all functions will assume their default parameters."
        },
        "Section_id": "er_np"
    },
    {
        "Field List > Returns": {
            "A": "ndarray,shape (n, n)\nSampled adjacency matrix"
        },
        "Section_id": "er_np"
    },
    {
        "Rubric": {
            "example": [
                "np.random.seed(1)\nn=4\np=0.25",
                "To sample a binary Erdos Renyi (n, p) graph:\n\ner_np(n, p)\narray([[0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [1., 1., 0., 0.],\n       [0., 0., 0., 0.]])\n",
                "To sample a weighted Erdos Renyi (n, p) graph with Uniform(0, 1) distribution:\n \nwt = np.random.uniform\nwtargs = dict(low=0, high=1)\ner_np(n, p, wt=wt, wtargs=wtargs)\narray([[0.        , 0.        , 0.95788953, 0.53316528],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.95788953, 0.        , 0.        , 0.31551563],\n       [0.53316528, 0.        , 0.31551563, 0.        ]])"
            ]
        },
        "Section_id": "er_np"
    },
    {
        "Section_id": "er_nm",
        "Description": "Samples an Erdos Renyi (n, m) graph with specified number of edges.\n\nErdos Renyi (n, m) graph is a simple graph with n vertices and exactly m number of total edges."
    },
    {
        "Field List > Parameters": {
            "n": "int\n,Number of vertices",
            "m": "int\nNumber of edges, a value between 1 and .",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal.",
            "wt": "object, optional (default=1)\nWeight function for each of the edges, taking only a size argument. This weight function will be randomly assigned for selected edges. If 1, graph produced is binary.",
            "wtargs": "dictionary, optional (default=None)\nOptional arguments for parameters that can be passed to weight function wt."
        },
        "Section_id": "er_nm"
    },
    {
        "Field List > Returns": {
            "A": "ndarray,shape (n, n)\nSampled adjacency matrix"
        },
        "Section_id": "er_nm"
    },
    {
        "Rubric": {
            "example": [
                "np.random.seed(1)\nn = 4\nm = 4",
                "To sample a binary Erdos Renyi (n, m) graph:\n\ner_nm(n, m)\narray([[0., 1., 1., 1.],\n       [1., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [1., 1., 0., 0.]])",
                "To sample a weighted Erdos Renyi (n, m) graph with Uniform(0, 1) distribution:\n\nwt = np.random.uniform\nwtargs = dict(low=0, high=1)\ner_nm(n, m, wt=wt, wtargs=wtargs)\narray([[0.        , 0.66974604, 0.        , 0.38791074],\n       [0.66974604, 0.        , 0.        , 0.39658073],\n       [0.        , 0.        , 0.        , 0.93553907],\n       [0.38791074, 0.39658073, 0.93553907, 0.        ])"
            ]
        },
        "Section_id": "er_nm"
    },
    {
        "Section_id": "sbm",
        "Description": "Samples a graph from the stochastic block model (SBM).\n\nSBM produces a graph with specified communities, in which each community can have different sizes and edge probabilities."
    },
    {
        "Field List > Parameters": {
            "n": "list of int, shape (n_communities)\nNumber of vertices in each community. Communities are assigned n[0], n[1], ...",
            "p": "array-like, shape (n_communities, n_communities)\nProbability of an edge between each of the communities, where p[i, j] indicates the probability of a connection between edges in communities [i, j]. 0 < p[i, j] < 1 for all i, j.",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal.",
            "wt": "object or array-like, shape (n_communities, n_communities)\nif wt is an object, a weight function to use globally over the sbm for assigning weights. 1 indicates to produce a binary graph. If wt is an array-like, a weight function for each of the edge communities. wt[i, j] corresponds to the weight function between communities i and j. If the entry is a function, should accept an argument for size. An entry of wt[i, j] = 1 will produce a binary subgraph over the i, j community.",
            "wtargs": "dictionary or array-like, shape (n_communities, n_communities)\nif wt is an object, wtargs corresponds to the trailing arguments to pass to the weight function. If Wt is an array-like, wtargs[i, j] corresponds to trailing arguments to pass to wt[i, j].",
            "dc": "function or array-like, shape (n_vertices) or (n_communities), optional\ndc is used to generate a degree-corrected stochastic block model [1] in which each node in the graph has a parameter to specify its expected degree relative to other nodes within its community.\n\nfunction:\nshould generate a non-negative number to be used as a degree correction to create a heterogenous degree distribution. A weight will be generated for each vertex, normalized so that the sum of weights in each block is 1.\n\narray-like of functions, shape (n_communities):\nEach function will generate the degree distribution for its respective community.\n\narray-like of scalars, shape (n_vertices):\nThe weights in each block should sum to 1; otherwise, they will be normalized and a warning will be thrown. The scalar associated with each vertex is the node's relative expected degree within its community.",
            "dc_kws": "dictionary or array-like, shape (n_communities), optional\nIgnored if dc is none or array of scalar. If dc is a function, dc_kws corresponds to its named arguments. If dc is an array-like of functions, dc_kws should be an array-like, shape (n_communities), of dictionary. Each dictionary is the named arguments for the corresponding function for that community. If not specified, in either case all functions will assume their default parameters.",
            "return_labels": "boolean, optional (default=False)\nIf False, only output is adjacency matrix. Otherwise, an additional output will be an array with length equal to the number of vertices in the graph, where each entry in the array labels which block a vertex in the graph is in."
        },
        "Section_id": "sbm"
    },
    {
        "Field List > Returns": {
            "A": "ndarray,shape (sum(n), sum(n))\nSampled adjacency matrix",
            "labels": "ndarray,shape (sum(n))\nLabel vector"
        },
        "Section_id": "sbm"
    },
    {
        "Rubric": {
            "References": [
                "Tai Qin and Karl Rohe. \"Regularized spectral clustering under the Degree-Corrected Stochastic Blockmodel,\" Advances in Neural Information Processing Systems 26, 2013"
            ],
            "example": [
                "np.random.seed(1)\nn = [3, 3]\np = [[0.5, 0.1], [0.1, 0.5]]",
                "To sample a binary 2-block SBM graph:\n\nsbm(n, p)\narray([[0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 1.],\n       [1., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0., 0.]])",
                "To sample a weighted 2-block SBM graph with Poisson(2) distribution:\n\nwt = np.random.poisson"
            ]
        },
        "Section_id": "sbm"
    },
    {
        "Section_id": "rdpg",
        "Description": "Samples a random graph based on the latent positions in X (and optionally in Y)\n\nIf only X is given, the P matrix is calculated as . If X, Y is given, then . These operations correspond to the dot products between a set of latent positions, so each row in X or Y represents the latent positions in for a single vertex in the random graph Note that this function may also rescale or clip the resulting P matrix to get probabilities between 0 and 1, or remove loops. A binary random graph is then sampled from the P matrix described by X (and possibly Y)."
    },
    {
        "Field List > Parameters": {
            "X": "np.ndarray, shape (n_vertices, n_dimensions)\nlatent position from which to generate a P matrix if Y is given, interpreted as the left latent position",
            "Y": "np.ndarray, shape (n_vertices, n_dimensions) or None, optional\nright latent position from which to generate a P matrix",
            "rescale": "boolean, optional (default=False)\nwhen rescale is True, will subtract the minimum value in P (if it is below 0) and divide by the maximum (if it is above 1) to ensure that P has entries between 0 and 1. If False, elements of P outside of [0, 1] will be clipped",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Diagonal elements in P matrix are removed prior to rescaling (see above) which may affect behavior. Otherwise, edges are sampled in the diagonal.",
            "wt": "object, optional (default=1)\nWeight function for each of the edges, taking only a size argument. This weight function will be randomly assigned for selected edges. If 1, graph produced is binary.",
            "wtargs": "dictionary, optional (default=None)\nOptional arguments for parameters that can be passed to weight function wt."
        },
        "Section_id": "rdpg"
    },
    {
        "Field List > Returns": {
            "A": "ndarray (n_vertices, n_vertices)\nA matrix representing the probabilities of connections between vertices in a random graph based on their latent positions"
        },
        "Section_id": "rdpg"
    },
    {
        "Rubric": {
            "References": [
                "Sussman, D.L., Tang, M., Fishkind, D.E., Priebe, C.E. \"A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs,\" Journal of the American Statistical Association, Vol. 107(499), 2012"
            ],
            "example": [
                "np.random.seed(1)\n",
                "Generate random latent positions using 2-dimensional Dirichlet distribution.\n\nX = np.random.dirichlet([1, 1], size=5)\n",
                "Sample a binary RDPG using sampled latent positions.\n\nrdpg(X, loops=False)\narray([[0., 1., 0., 0., 1.],\n       [1., 0., 0., 1., 1.],\n       [0., 0., 0., 1., 1.],\n       [0., 1., 1., 0., 0.],\n       [1., 1., 1., 0., 0.]])\n",
                "Sample a weighted RDPG with Poisson(2) weight distribution\n\nwt = np.random.poisson\nwtargs = dict(lam=2)\nrdpg(X, loops=False, wt=wt, wtargs=wtargs)\narray([[0., 4., 0., 2., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 2.],\n       [1., 0., 0., 0., 1.],\n       [0., 2., 2., 0., 0.]])"
            ]
        },
        "Section_id": "rdpg"
    },
    {
        "Section_id": "er_corr",
        "Description": "Generate a pair of correlated graphs with specified edge probability\n\nBoth G1 and G2 are binary matrices."
    },
    {
        "Field List > Parameters": {
            "n": "int\nNumber of vertices",
            "p": "float\nProbability of an edge existing between two vertices, between 0 and 1.",
            "r": "float\nThe value of the correlation between the same vertices in two graphs.",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal."
        },
        "Section_id": "er_corr"
    },
    {
        "Field List > Returns": {
            "G1": "ndarray (n_vertices, n_vertices)\nAdjacency matrix the same size as P representing a random graph.",
            "G2": "ndarray (n_vertices, n_vertices)\nAdjacency matrix the same size as P representing a random graph."
        },
        "Section_id": "er_corr"
    },
    {
        "Rubric": {
            "example": [
                "np.random.seed(2)\np = 0.5\nr = 0.3\nn = 5",
                "To sample a correlated ER graph pair based on n, p and r:\n\ner_corr(n, p, r, directed=False, loops=False)\n(array([[0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0.],\n        [1., 0., 0., 1., 1.],\n        [0., 1., 1., 0., 1.],\n        [0., 0., 1., 1., 0.]]), array([[0., 1., 1., 1., 0.],\n        [1., 0., 0., 1., 0.],\n        [1., 0., 0., 1., 1.],\n        [1., 1., 1., 0., 1.],\n        [0., 0., 1., 1., 0.]]))"
            ]
        },
        "Section_id": "er_corr"
    },
    {
        "Section_id": "sbm_corr",
        "Description": "Generate a pair of correlated graphs with specified edge probability\n\nBoth G1 and G2 are binary matrices."
    },
    {
        "Field List > Parameters": {
            "n": "ndarray | List[int]\nNumber of vertices in each community. Communities are assigned n[0], n[1], ...",
            "p": "ndarray\nProbability of an edge between each of the communities, where p[i, j] indicates the probability of a connection between edges in communities [i, j]. 0 < p[i, j] < 1 for all i, j.",
            "r": "float\nProbability of the correlation between the same vertices in two graphs.",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal."
        },
        "Section_id": "sbm_corr"
    },
    {
        "Field List > Returns": {
            "G1": "ndarray (n_vertices, n_vertices)\nAdjacency matrix the same size as P representing a random graph.",
            "G2": "ndarray (n_vertices, n_vertices)\nAdjacency matrix the same size as P representing a random graph."
        },
        "Section_id": "sbm_corr"
    },
    {
        "Rubric": {
            "example": [
                "np.random.seed(3)\nn = [3, 3]\np = [[0.5, 0.1], [0.1, 0.5]]\nr = 0.3",
                "To sample a correlated SBM graph pair based on n, p and r:\n\nsbm_corr(n, p, r, directed=False, loops=False)\n(array([[0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0.]]), array([[0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 1., 1., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 1.],\n        [0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0.]]))"
            ]
        },
        "Section_id": "sbm_corr"
    },
    {
        "Section_id": "rdpg_corr",
        "Description": "Samples a random graph pair based on the latent positions in X (and optionally in Y)\n\nIf only X is given, the P matrix is calculated as . If X, Y is given, then . These operations correspond to the dot products between a set of latent positions, so each row in X or Y represents the latent positions in for a single vertex in the random graph. Note that this function may also rescale or clip the resulting P matrix to get probabilities between 0 and 1, or remove loops. A binary random graph is then sampled from the P matrix described by X (and possibly Y)."
    },
    {
        "Field List > Parameters": {
            "X": "np.ndarray, shape (n_vertices, n_dimensions)\nlatent position from which to generate a P matrix if Y is given, interpreted as the left latent position",
            "Y": "np.ndarray, shape (n_vertices, n_dimensions) or None, optional\nright latent position from which to generate a P matrix",
            "r": "float\nThe value of the correlation between the same vertices in two graphs.",
            "rescale": "boolean, optional (default=False)\nwhen rescale is True, will subtract the minimum value in P (if it is below 0) and divide by the maximum (if it is above 1) to ensure that P has entries between 0 and 1. If False, elements of P outside of [0, 1] will be clipped.",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=True)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal."
        },
        "Section_id": "rdpg_corr"
    },
    {
        "Field List > Returns": {
            "G1": "ndarray (n_vertices, n_vertices)\nA matrix representing the probabilities of connections between vertices in a random graph based on their latent positions",
            "G2": "ndarray (n_vertices, n_vertices)\nA matrix representing the probabilities of connections between vertices in a random graph based on their latent positions"
        },
        "Section_id": "rdpg_corr"
    },
    {
        "Rubric": {
            "References": [
                "Vince Lyzinski, Donniell E Fishkind profile imageDonniell E. Fishkind, Carey E Priebe. \"Seeded graph matching for correlated Erd\u00f6s-R\u00e9nyi graphs\". The Journal of Machine Learning Research, January 2014"
            ],
            "example": [
                "np.random.seed(1234)\nX = np.random.dirichlet([1, 1], size=5)\nY = None",
                "Generate random latent positions using 2-dimensional Dirichlet distribution. Then sample a correlated RDPG graph pair:\n\nrdpg_corr(X, Y, 0.3, rescale=False, directed=False, loops=False)\n(array([[0., 1., 0., 1., 0.],\n        [1., 0., 0., 1., 1.],\n        [0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]), array([[0., 1., 0., 1., 0.],\n        [1., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0.]]))"
            ]
        },
        "Section_id": "rdpg_corr"
    },
    {
        "Section_id": "mmsbm",
        "Description": "Samples a graph from Mixed Membership Stochastic Block Model (MMSBM).\n\nMMSBM produces a graph given the specified block connectivity matrix B, which indicates the probability of connection between nodes based upon their community membership. Each node is assigned a membership vector drawn from Dirichlet distribution with parameter . The entries of this vector indicate the probabilities for that node of pertaining to each community when interacting with another node. Each node's membership is determined according to those probabilities. Finally, interactions are sampled according to the assigned memberships."
    },
    {
        "Field List > Parameters": {
            "n": "int\nNumber of vertices of the graph.",
            "p": "array-like, shape (n_communities, n_communities)\nProbability of an edge between each of the communities, where p[i, j] indicates the probability of a connection between edges in communities . 0 < p[i, j] < 1 for all .",
            "alpha": "array-like, shape (n_communities,)\nParameter alpha of the Dirichlet distribution used to sample the mixed-membership vectors for each node. alpha[i] > 0 for all .",
            "rng": "numpy.random.Generator, optional (default = None)\nnumpy.random.Generator object to sample from distributions. If None, the random number generator is the Generator object constructed by np.random.default_rng().",
            "directed": "boolean, optional (default=False)\nIf False, output adjacency matrix will be symmetric. Otherwise, output adjacency matrix will be asymmetric.",
            "loops": "boolean, optional (default=False)\nIf False, no edges will be sampled in the diagonal. Otherwise, edges are sampled in the diagonal.",
            "return_labels": "boolean, optional (default=False)\nIf False, the only output is the adjacency matrix. If True, output is a tuple. The first element of the tuple is the adjacency matrix. The second element is a matrix in which the entry indicates the membership assigned to node i when interacting with node j. Community 1 is labeled with a 0, community 2 with 1, etc. -1 indicates that no community was assigned for that interaction."
        },
        "Section_id": "mmsbm"
    },
    {
        "Field List > Returns": {
            "A": "ndarray,shape (n, n)\nSampled adjacency matrix",
            "labels": "ndarray,shape (n, n), optional\nArray containing the membership assigned to each node when interacting with another node."
        },
        "Section_id": "mmsbm"
    },
    {
        "Rubric": {
            "References": [
                "Airoldi, Edoardo, et al. \"Mixed Membership Stochastic Blockmodels.\" Journal of Machine Learning Research, vol. 9, 2008, pp. 1981\u20132014."
            ],
            "example": [
                "rng = np.random.default_rng(1)\nnp.random.seed(1)\nn = 6\np = [[0.5, 0], [0, 1]]",
                "To sample a binary MMSBM in which very likely all nodes pertain to community two:\n\nalpha = [0.05, 1000]\nmmsbm(n, p, alpha, rng = rng)\narray([[0., 1., 1., 1., 1., 1.],\n       [1., 0., 1., 1., 1., 1.],\n       [1., 1., 0., 1., 1., 1.],\n       [1., 1., 1., 0., 1., 1.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.]])",
                "To sample a binary MMSBM similar to 2-block SBM with connectivity matrix B:\n\nrng = np.random.default_rng(1)\nnp.random.seed(1)\nalpha = [0.05, 0.05]\nmmsbm(n, p, alpha, rng = rng)\narray([[0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 1.],\n       [0., 0., 0., 1., 0., 1.],\n       [0., 0., 0., 1., 1., 0.]])"
            ]
        },
        "Section_id": "mmsbm"
    },
    {
        "Section_id": "SignalSubgraph",
        "Description": "Estimate the signal-subgraph of a set of labeled graph samples.\n\nThe incoherent estimator finds the signal-subgraph, constrained by the number of edges. The coherent estimator finds the signal-subgraph, constrained by the number of edges and by the number of vertices that the edges in the signal-subgraph may be incident to."
    },
    {
        "Field List > Parameters": {
            "graphs": "array-like, shape (n_vertices, n_vertices, s_samples)\nA series of labeled (n_vertices, n_vertices) unweighted graph samples. If undirected, the upper or lower triangle matrices should be used.",
            "labels": "vector, length (s_samples)\nA vector of class labels. There must be a maximum of two classes."
        },
        "Section_id": "SignalSubgraph"
    },
    {
        "Field List > Attributes": {
            "contmat_": "array-like, shape (n_vertices, n_vertices, 2, 2)\nAn array that stores the 2-by-2 contingency matrix for each point in the graph samples.",
            "sigsub_": "tuple, shape (2, n_edges)\nA tuple of a row index array and column index array, where n_edges is the size of the signal-subgraph determined by constraints.",
            "mask_": "array-like, shape (n_vertices, n_vertices)\nAn array of boolean values. Entries are true for edges that are in the signal subgraph."
        },
        "Section_id": "SignalSubgraph"
    },
    {
        "Field List > Methods > fit": {
            "Description": "Fit the signal-subgraph estimator according to the constraints given.",
            "Parameters": {
                "graphs": "array-like, shape (n_vertices, n_vertices, s_samples)\nA series of labeled (n_vertices, n_vertices) unweighted graph samples. If undirected, the upper or lower triangle matrices should be used.",
                "labels": "vector, length (s_samples)\nA vector of class labels. There must be a maximum of two classes.",
                "constraints": "int or vector\nThe constraints that will be imposed onto the estimated signal-subgraph.\n\nIf constraints is an int, constraints is the number of edges in the signal-subgraph. If constraints is a vector, the first element of constraints is the number of edges in the signal-subgraph, and the second element of constraints is the number of vertices that the signal-subgraph must be incident to."
            },
            "Returns": {
                "self": "returns an instance of self"
            }
        },
        "Section_id": "SignalSubgraph"
    },
    {
        "Field List > Methods > fit_transform": {
            "Description": "A function to return the indices of the signal-subgraph. If return_mask is True, also returns a mask for the signal-subgraph.",
            "Parameters": {
                "graphs": "array-like, shape (n_vertices, n_vertices, s_samples)\nA series of labeled (n_vertices, n_vertices) unweighted graph samples. If undirected, the upper or lower triangle matrices should be used.",
                "labels": "vector, length (s_samples)\nA vector of class labels. There must be a maximum of two classes.",
                "constraints": "int or vector\nThe constraints that will be imposed onto the estimated signal-subgraph.\n\nIf constraints is an int, constraints is the number of edges in the signal-subgraph. If constraints is a vector, the first element of constraints is the number of edges in the signal-subgraph, and the second element of constraints is the number of vertices that the signal-subgraph must be incident to."
            },
            "Returns": {
                "sigsub": "tuple\nContains an array of row indices and an array of column indices."
            }
        },
        "Section_id": "SignalSubgraph"
    },
    {
        "Rubric": {
            "References": [
                "J.T.Vogelstein, W. R. Gray, R. J. Vogelstein, and C. E. Priebe, \"Graph Classification using Signal-Subgraphs: Applications in Statistical Connectomics,\" arXiv:1108.1427v2 [stat.AP], 2012."
            ]
        },
        "Section_id": "SignalSubgraph"
    },
    {
        "Section_id": "pass_to_ranks",
        "Description": "Rescales edge weights of an adjacency matrix based on their relative rank in the graph."
    },
    {
        "Field List > Parameters": {
            "graph": "array_like or networkx.Graph\nAdjacency matrix",
            "method": "{'simple-nonzero' (default), 'simple-all', 'zero-boost'} string, optional\n'simple-nonzero'\nassigns ranks to all non-zero edges, settling ties using the average. Ranks are then scaled by \n\n'simple-all'\nassigns ranks to all non-zero edges, settling ties using the average. Ranks are then scaled by \n\n where n is the number of nodes\n\n'zero-boost'\npreserves the edge weight for all 0s, but ranks the other edges as if the ranks of all 0 edges has been assigned. If there are 10 0-valued edges, the lowest non-zero edge gets weight 11 / (number of possible edges). Ties settled by the average of the weight that those edges would have received. Number of possible edges is determined by the type of graph (loopless or looped, directed or undirected)."
        },
        "Section_id": "pass_to_ranks"
    },
    {
        "Field List > Returns": {
            "graph": "numpy.ndarray, shape(n_vertices, n_vertices)\nAdjacency matrix of graph after being passed to ranks"
        },
        "Section_id": "pass_to_ranks"
    },
    {
        "Section_id": "to_laplacian",
        "Description": "A function to convert graph adjacency matrix to graph Laplacian.\n\nCurrently supports I-DAD, DAD, and R-DAD Laplacians, where D is the diagonal matrix of degrees of each node, I is the identity matrix, and A is the adjacency matrix.\n\nR-DAD is regularized Laplacian: where ."
    },
    {
        "Field List > Parameters": {
            "graph": "object\nEither array-like, (n_vertices, n_vertices) numpy array, scipy.sparse.csr_array, or an object of type networkx.Graph.",
            "form": "{'I-DAD', 'DAD' (default), 'R-DAD'}, string, optional\n'I-DAD'\nComputes \n\n'DAD'\nComputes \n\n'R-DAD'\nComputes \n where \n and likewise for \n",
            "regularizer": "int, float or None, optional (default=None)\nConstant to add to the degree vector(s). If None, average node degree is added. If int or float, must be >= 0. Only used when form is 'R-DAD'."
        },
        "Section_id": "to_laplacian"
    },
    {
        "Field List > Returns": {
            "L": "numpy.ndarray\n2D (n_vertices, n_vertices) array representing graph Laplacian of specified form"
        },
        "Section_id": "to_laplacian"
    },
    {
        "Rubric": {
            "References": [
                "Qin, Tai, and Karl Rohe. \"Regularized spectral clustering under the degree-corrected stochastic blockmodel.\" In Advances in Neural Information Processing Systems, pp. 3120-3128. 2013",
                "Rohe, Karl, Tai Qin, and Bin Yu. \"Co-clustering directed graphs to discover asymmetries and directional communities.\" Proceedings of the National Academy of Sciences 113.45 (2016): 12679-12684."
            ],
            "example": [
                "a = np.array([\n   [0, 1, 1],\n   [1, 0, 0],\n   [1, 0, 0]])\nto_laplacian(a, \"DAD\")\narray([[0.        , 0.70710678, 0.70710678],\n       [0.70710678, 0.        , 0.        ],\n       [0.70710678, 0.        , 0.        ]])"
            ]
        },
        "Section_id": "to_laplacian"
    },
    {
        "Section_id": "augment_diagonal",
        "Description": "Replaces the diagonal of an adjacency matrix with d/(nverts-1) where d is the degree vector for an unweighted graph and the sum of magnitude of edge weights for each node for a weighted graph. For a directed graph the in/out d is averaged."
    },
    {
        "Field List > Parameters": {
            "graph": "nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph, np.ndarray\nInput graph in any of the above specified formats. If np.ndarray, interpreted as an adjacency matrix",
            "weight": "float/int\nscalar value to multiply the new diagonal vector by"
        },
        "Section_id": "augment_diagonal"
    },
    {
        "Field List > Returns": {
            "graph": "np.array or csr_array\nAdjacency matrix with average degrees added to the diagonal."
        },
        "Section_id": "augment_diagonal"
    },
    {
        "Rubric": {
            "example": [
                "a = np.array([\n   [0, 1, 1],\n   [1, 0, 0],\n   [1, 0, 0]])\naugment_diagonal(a)\narray([[1. , 1. , 1. ],\n       [1. , 0.5, 0. ],\n       [1. , 0. , 0.5]])"
            ]
        },
        "Section_id": "augment_diagonal"
    },
    {
        "Section_id": "symmetrize",
        "Description": "A function for forcing symmetry upon a graph."
    },
    {
        "Field List > Parameters": {
            "graph": "object\nEither array-like, (n_vertices, n_vertices) numpy matrix or csr_array",
            "method": "{'avg' (default), 'triu', 'tril',}, optional\nAn option indicating which half of the edges to retain when symmetrizing.\n\n'avg'\nRetain the average weight between the upper and lower right triangle, of the adjacency matrix.\n\n'triu'\nRetain the upper right triangle.\n\n'tril'\nRetain the lower left triangle."
        },
        "Section_id": "symmetrize"
    },
    {
        "Field List > Returns": {
            "graph": "array-like, shape (n_vertices, n_vertices)\nGraph with asymmetries removed."
        },
        "Section_id": "symmetrize"
    },
    {
        "Rubric": {
            "example": [
                "a = np.array([\n   [0, 1, 1],\n   [0, 0, 1],\n   [0, 0, 1]])\nsymmetrize(a, method=\"triu\")\narray([[0, 1, 1],\n       [1, 0, 1],\n       [1, 1, 1]])"
            ]
        },
        "Section_id": "symmetrize"
    },
    {
        "Section_id": "remove_loops",
        "Description": "A function to remove loops from a graph."
    },
    {
        "Field List > Parameters": {
            "graph": "object\nEither array-like, (n_vertices, n_vertices) numpy matrix, or an object of type networkx.Graph."
        },
        "Section_id": "remove_loops"
    },
    {
        "Field List > Returns": {
            "graph": "array-like, shape(n_vertices, n_vertices)\nthe graph with self-loops (edges between the same node) removed."
        },
        "Section_id": "remove_loops"
    },
    {
        "Section_id": "is_fully_connected",
        "Description": "Checks whether the input graph is fully connected in the undirected case or weakly connected in the directed case.\n\nConnected means one can get from any vertex to vertex by traversing the graph. For a directed graph, weakly connected means that the graph is connected after it is converted to an unweighted graph (ignore the direction of each edge)"
    },
    {
        "Field List > Parameters": {
            "graph": "nx.Graph, nx.MultiDiGraph, nx.MultiGraph\nscipy.sparse.csr_array, np.ndarray Input graph in any of the above specified formats. If np.ndarray, interpreted as an adjacency matrix"
        },
        "Section_id": "is_fully_connected"
    },
    {
        "Field List > Returns": {
            "boolean": "True if the entire input graph is connected"
        },
        "Section_id": "is_fully_connected"
    },
    {
        "Rubric": {
            "References": [
                "http://mathworld.wolfram.com/ConnectedGraph.html",
                "http://mathworld.wolfram.com/WeaklyConnectedDigraph.html"
            ],
            "example": [
                "a = np.array([\n   [0, 1, 0],\n   [1, 0, 0],\n   [0, 0, 0]])\nis_fully_connected(a)\nFalse"
            ]
        },
        "Section_id": "is_fully_connected"
    },
    {
        "Section_id": "largest_connected_component",
        "Description": "Finds the largest connected component for the input graph.\n\nThe largest connected component is the fully connected subgraph which has the most nodes."
    },
    {
        "Field List > Parameters": {
            "graph": "nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph, np.ndarray, scipy.sparse.csr_array\nInput graph in any of the above specified formats. If np.ndarray or scipy.sparse.csr_array interpreted as an adjacency matrix.",
            "return_inds": "boolean, default: False\nWhether to return a np.ndarray containing the indices/nodes in the original adjacency matrix that were kept and are now in the returned graph."
        },
        "Section_id": "largest_connected_component"
    },
    {
        "Field List > Returns": {
            "graph": "nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph, np.ndarray, scipy.sparse.csr_array\nNew graph of the largest connected component, returned in the input format.",
            "inds": "(optional)\nIndices/nodes from the original adjacency matrix that were kept after taking the largest connected component."
        },
        "Section_id": "largest_connected_component"
    },
    {
        "Rubric": {
            "Note": "For networks input in scipy.sparse.csr_array format, explicit zeros are removed prior to finding the largest connected component, thus they are not treated as edges. This differs from the convention in scipy.sparse.csgraph.connected_components()."
        },
        "Section_id": "largest_connected_component"
    },
    {
        "Section_id": "multigraph_lcc_union",
        "Description": "Finds the union of all multiple graphs, then compute the largest connected component."
    },
    {
        "Field List > Parameters": {
            "graphs": "list or np.ndarray\nList of array-like, (n_vertices, n_vertices), or list of np.ndarray nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph.",
            "return_inds": "boolean, default: False\nWhether to return a np.ndarray containing the indices in the original adjacency matrix that were kept and are now in the returned graph. Ignored when input is networkx object"
        },
        "Section_id": "multigraph_lcc_union"
    },
    {
        "Field List > Returns": {
            "outlist": "or np.ndarray\nIf input was a list"
        },
        "Section_id": "multigraph_lcc_union"
    },
    {
        "Section_id": "multigraph_lcc_intersection",
        "Description": "Finds the intersection of multiple graphs's largest connected components.\n\nComputes the largest connected component for each graph that was input, and takes the intersection over all of these resulting graphs. Note that this does not guarantee finding the largest graph where every node is shared among all of the input graphs."
    },
    {
        "Field List > Parameters": {
            "graphs": "list or np.ndarray\nif list, each element must be an np.ndarray adjacency matrix",
            "return_inds": "boolean, default: False\nWhether to return a np.ndarray containing the indices in the original adjacency matrix that were kept and are now in the returned graph. Ignored when input is networkx object"
        },
        "Section_id": "multigraph_lcc_intersection"
    },
    {
        "Field List > Returns": {
            "graph": "list of(nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph) or np.ndarray\nNew graph of the largest connected component of the input parameter.",
            "inds": "(optional)\nIndices from the original adjacency matrix that were kept after taking the largest connected component"
        },
        "Section_id": "multigraph_lcc_intersection"
    },
    {
        "Section_id": "import_graph",
        "Description": "A function for reading a graph and returning a shared data type."
    },
    {
        "Field List > Parameters": {
            "graph": "GraphRepresentation\nEither array-like, shape (n_vertices, n_vertices) numpy array, a scipy.sparse.csr_array, or an object of type networkx.Graph.",
            "copy": "bool, (default=True)\nWhether to return a copied version of array. If False and input is np.array, the output returns the original input."
        },
        "Section_id": "import_graph"
    },
    {
        "Field List > Returns": {
            "out": "array-like, shape (n_vertices, n_vertices)\nA graph."
        },
        "Section_id": "import_graph"
    },
    {
        "Section_id": "import_edgelist",
        "Description": "Function for reading a single or multiple edgelists.\n\nWhen importing multiple edgelists, the union of vertices from all graphs is computed so that each output graph have matched vertex set. The order of nodes are sorted by node values."
    },
    {
        "Field List > Parameters": {
            "path": "str, Path object, or iterable\nIf path is a directory, then the importing order will be sorted in alphabetical order.",
            "extension": "str, optional\nIf path is a directory, then the function will convert all files with matching extension.",
            "delimiter": "str or None, default=None, optional\nDelimiter of edgelist. If None, the delimiter is whitespace.",
            "nodetype": "int (default), float, str, Python type, optional\nConvert node data from strings to specified type.",
            "return_vertices": "bool, default=False, optional\nReturns the union of all individual edgelists."
        },
        "Section_id": "import_edgelist"
    },
    {
        "Field List > Returns": {
            "out": "list of array-like, or array-like, shape (n_vertices, n_vertices)\nIf path is a directory, a list of arrays is returned. If path is a file, an array is returned.",
            "vertices": "array-like, shape (n_vertices, )\nIf return_vertices` is True, then returns an array of all vertices that were included in the output graphs."
        },
        "Section_id": "import_edgelist"
    },
    {
        "Section_id": "remap_labels",
        "Description": "Remaps a categorical labeling (such as one predicted by a clustering algorithm) to match the labels used by another similar labeling.\n\nGiven two -length vectors describing a categorical labeling of  samples, this method reorders the labels of the second vector (y_pred) so that as many samples as possible from the two label vectors are in the same category."
    },
    {
        "Field List > Parameters": {
            "y_true": "array-like of shape (n_samples,)\nGround truth labels, or, labels to map to.",
            "y_pred": "array-like of shape (n_samples,)\nLabels to remap to match the categorical labeling of y_true. The categorical labeling of y_pred will be preserved exactly, but the labels used to denote the categories will be changed to best match the categories used in y_true.",
            "return_map": "bool, optional\nWhether to return a dictionary where the keys are the original category labels from y_pred and the values are the new category labels that they were mapped to."
        },
        "Section_id": "remap_labels"
    },
    {
        "Field List > Returns": {
            "remapped_y_pred": "np.ndarray of shape (n_samples,)\nSame categorical labeling as that of y_pred, but with the category labels permuted to best match those of y_true.",
            "label_map": "dict\nMapping from the original labels of y_pred to the new labels which best resemble those of y_true. Only returned if return_map was True."
        },
        "Section_id": "remap_labels"
    },
    {
        "Rubric": {
            "Note": "This method will work well when the label vectors describe a somewhat similar categorization of the data (as measured by metrics such as sklearn.metrics.adjusted_rand_score(), for example). When the categorizations are not similar, the remapping may not make sense (as such a remapping does not exist).",
            "example": [
                "y_true = np.array([0,0,1,1,2,2])\ny_pred = np.array([2,2,1,1,0,0])\nremap_labels(y_true, y_pred)\narray([0, 0, 1, 1, 2, 2])"
            ]
        },
        "Section_id": "remap_labels"
    }
]